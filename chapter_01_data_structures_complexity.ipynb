{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 1: Data Structures & Complexity Refresher\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this chapter, you will be able to:\n",
        "- Analyze time and space complexity of algorithms\n",
        "- Understand memory layout and cache effects in ML operations\n",
        "- Compare vectorized operations vs loops for performance\n",
        "- Implement memory-efficient algorithms for large-scale ML\n",
        "- Profile and optimize memory usage in PyTorch/TensorFlow\n",
        "\n",
        "## Prerequisites\n",
        "- Basic understanding of Python and NumPy\n",
        "- Familiarity with PyTorch tensors\n",
        "- Knowledge of basic data structures (arrays, lists, dictionaries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Complexity Analysis Refresher\n",
        "\n",
        "### Big O Notation\n",
        "Big O notation describes the worst-case time or space complexity of an algorithm as the input size grows.\n",
        "\n",
        "**Common Complexities:**\n",
        "- O(1) - Constant time (array access, hash table lookup)\n",
        "- O(log n) - Logarithmic (binary search, balanced tree operations)\n",
        "- O(n) - Linear (single pass through array)\n",
        "- O(n log n) - Linearithmic (efficient sorting algorithms)\n",
        "- O(n¬≤) - Quadratic (nested loops)\n",
        "- O(2‚Åø) - Exponential (recursive Fibonacci)\n",
        "\n",
        "### Space Complexity\n",
        "Space complexity measures the amount of memory an algorithm uses relative to input size:\n",
        "- **Auxiliary space**: Extra space used by the algorithm\n",
        "- **Total space**: Input space + auxiliary space\n",
        "\n",
        "### Amortized Analysis\n",
        "Some operations may occasionally be expensive but are cheap on average:\n",
        "- Dynamic array resizing: O(1) amortized insertion\n",
        "- Hash table with chaining: O(1) amortized lookup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Memory Layout and Cache Effects\n",
        "\n",
        "### Memory Hierarchy\n",
        "Modern computers have multiple levels of memory with different speeds and sizes:\n",
        "\n",
        "1. **CPU Registers** (fastest, smallest)\n",
        "2. **L1 Cache** (~32KB, ~1 cycle)\n",
        "3. **L2 Cache** (~256KB, ~10 cycles)\n",
        "4. **L3 Cache** (~8MB, ~40 cycles)\n",
        "5. **RAM** (~16GB, ~200 cycles)\n",
        "6. **Storage** (SSD/HDD, ~100,000+ cycles)\n",
        "\n",
        "### Cache Locality\n",
        "**Spatial Locality**: Accessing nearby memory locations\n",
        "**Temporal Locality**: Accessing the same memory location repeatedly\n",
        "\n",
        "### Row-Major vs Column-Major Ordering\n",
        "```python\n",
        "# Row-major (C-style): elements in same row are adjacent\n",
        "arr[i][j] and arr[i][j+1] are adjacent in memory\n",
        "\n",
        "# Column-major (Fortran-style): elements in same column are adjacent\n",
        "arr[i][j] and arr[i+1][j] are adjacent in memory\n",
        "```\n",
        "\n",
        "### Memory Access Patterns in ML\n",
        "- **Sequential access** is much faster than random access\n",
        "- **Vectorized operations** leverage SIMD instructions\n",
        "- **Memory bandwidth** often limits performance more than compute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Vectorization vs Loops\n",
        "\n",
        "### Why Vectorization is Faster\n",
        "1. **SIMD Instructions**: Single Instruction, Multiple Data\n",
        "2. **Reduced Python overhead**: Less interpreter calls\n",
        "3. **Better cache utilization**: Sequential memory access\n",
        "4. **Optimized C implementations**: NumPy/PyTorch use optimized BLAS\n",
        "\n",
        "### Example: Element-wise Operations\n",
        "```python\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Large array\n",
        "arr = np.random.randn(1000000)\n",
        "\n",
        "# Loop version (slow)\n",
        "def loop_sum(arr):\n",
        "    total = 0\n",
        "    for x in arr:\n",
        "        total += x\n",
        "    return total\n",
        "\n",
        "# Vectorized version (fast)\n",
        "def vectorized_sum(arr):\n",
        "    return np.sum(arr)\n",
        "\n",
        "# Vectorized is ~100x faster for large arrays\n",
        "```\n",
        "\n",
        "### When to Use Each Approach\n",
        "- **Use loops for**: Complex logic, small arrays, irregular patterns\n",
        "- **Use vectorization for**: Mathematical operations, large arrays, regular patterns\n",
        "- **Hybrid approach**: Vectorize outer loops, keep inner logic readable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Memory-Efficient Algorithms\n",
        "\n",
        "### In-Place Operations\n",
        "Modify data structures without creating copies:\n",
        "```python\n",
        "# In-place: O(1) extra space\n",
        "def reverse_inplace(arr):\n",
        "    left, right = 0, len(arr) - 1\n",
        "    while left < right:\n",
        "        arr[left], arr[right] = arr[right], arr[left]\n",
        "        left += 1\n",
        "        right -= 1\n",
        "\n",
        "# Not in-place: O(n) extra space\n",
        "def reverse_copy(arr):\n",
        "    return arr[::-1]\n",
        "```\n",
        "\n",
        "### Memory-Efficient Data Structures\n",
        "- **Sparse matrices**: Store only non-zero elements\n",
        "- **Compressed representations**: Use bit packing, run-length encoding\n",
        "- **Lazy evaluation**: Compute values on-demand\n",
        "- **Streaming algorithms**: Process data in chunks\n",
        "\n",
        "### Memory Profiling Tools\n",
        "```python\n",
        "import tracemalloc\n",
        "import psutil\n",
        "import torch\n",
        "\n",
        "# PyTorch memory tracking\n",
        "torch.cuda.memory_allocated()  # Current GPU memory\n",
        "torch.cuda.max_memory_allocated()  # Peak GPU memory\n",
        "\n",
        "# Python memory profiling\n",
        "tracemalloc.start()\n",
        "# ... your code ...\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 PyTorch/Tensor Operations\n",
        "\n",
        "### Tensor Memory Layout\n",
        "PyTorch tensors are stored in row-major (C-contiguous) order by default:\n",
        "```python\n",
        "import torch\n",
        "\n",
        "x = torch.randn(3, 4)\n",
        "print(x.is_contiguous())  # True\n",
        "print(x.stride())  # (4, 1) - elements in same row are adjacent\n",
        "```\n",
        "\n",
        "### Memory-Efficient Tensor Operations\n",
        "```python\n",
        "# Avoid creating intermediate tensors\n",
        "# Bad: Creates temporary tensors\n",
        "result = (x + y).sum().sqrt()\n",
        "\n",
        "# Good: Use in-place operations when possible\n",
        "result = x.add_(y).sum().sqrt_()\n",
        "\n",
        "# Or use functional operations\n",
        "result = torch.sqrt(torch.sum(torch.add(x, y)))\n",
        "```\n",
        "\n",
        "### Broadcasting and Memory\n",
        "Broadcasting can create large intermediate tensors:\n",
        "```python\n",
        "# This creates a (1000, 1000) tensor\n",
        "a = torch.randn(1000, 1)\n",
        "b = torch.randn(1, 1000)\n",
        "c = a + b  # Broadcasting creates large intermediate tensor\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Practice Questions\n",
        "\n",
        "Now let's apply these concepts with 5 progressive exercises. Each question builds on the previous concepts and increases in difficulty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1: Memory-Efficient Prefix Sum (Easy)\n",
        "\n",
        "**Problem**: Implement a memory-efficient prefix sum algorithm that computes the cumulative sum of an array in-place.\n",
        "\n",
        "**Requirements**:\n",
        "- Modify the input array in-place (O(1) extra space)\n",
        "- Time complexity: O(n)\n",
        "- Handle edge cases (empty array, single element)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "arr = [1, 2, 3, 4, 5]\n",
        "prefix_sum_inplace(arr)\n",
        "print(arr)  # [1, 3, 6, 10, 15]\n",
        "```\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prefix_sum_inplace(arr):\n",
        "    \"\"\"\n",
        "    Compute prefix sum in-place.\n",
        "    \n",
        "    Args:\n",
        "        arr: List of numbers to modify in-place\n",
        "        \n",
        "    Returns:\n",
        "        None (modifies input array)\n",
        "    \"\"\"\n",
        "    # TODO: Implement in-place prefix sum\n",
        "    pass\n",
        "\n",
        "# Comprehensive Test Suite\n",
        "def test_prefix_sum():\n",
        "    \"\"\"Comprehensive tests for prefix sum implementation.\"\"\"\n",
        "    print(\"Running comprehensive prefix sum tests...\")\n",
        "    \n",
        "    # Test case 1: Normal array\n",
        "    arr1 = [1, 2, 3, 4, 5]\n",
        "    original1 = arr1.copy()\n",
        "    prefix_sum_inplace(arr1)\n",
        "    expected1 = [1, 3, 6, 10, 15]\n",
        "    assert arr1 == expected1, f\"Test 1 failed: Expected {expected1}, got {arr1}\"\n",
        "    print(\"‚úì Test 1: Normal array passed\")\n",
        "    \n",
        "    # Test case 2: Single element\n",
        "    arr2 = [42]\n",
        "    original2 = arr2.copy()\n",
        "    prefix_sum_inplace(arr2)\n",
        "    expected2 = [42]\n",
        "    assert arr2 == expected2, f\"Test 2 failed: Expected {expected2}, got {arr2}\"\n",
        "    print(\"‚úì Test 2: Single element passed\")\n",
        "    \n",
        "    # Test case 3: Empty array\n",
        "    arr3 = []\n",
        "    original3 = arr3.copy()\n",
        "    prefix_sum_inplace(arr3)\n",
        "    expected3 = []\n",
        "    assert arr3 == expected3, f\"Test 3 failed: Expected {expected3}, got {arr3}\"\n",
        "    print(\"‚úì Test 3: Empty array passed\")\n",
        "    \n",
        "    # Test case 4: Negative numbers\n",
        "    arr4 = [-1, 2, -3, 4]\n",
        "    original4 = arr4.copy()\n",
        "    prefix_sum_inplace(arr4)\n",
        "    expected4 = [-1, 1, -2, 2]\n",
        "    assert arr4 == expected4, f\"Test 4 failed: Expected {expected4}, got {arr4}\"\n",
        "    print(\"‚úì Test 4: Negative numbers passed\")\n",
        "    \n",
        "    # Test case 5: All zeros\n",
        "    arr5 = [0, 0, 0, 0]\n",
        "    original5 = arr5.copy()\n",
        "    prefix_sum_inplace(arr5)\n",
        "    expected5 = [0, 0, 0, 0]\n",
        "    assert arr5 == expected5, f\"Test 5 failed: Expected {expected5}, got {arr5}\"\n",
        "    print(\"‚úì Test 5: All zeros passed\")\n",
        "    \n",
        "    # Test case 6: Large array\n",
        "    arr6 = list(range(1, 1001))  # [1, 2, 3, ..., 1000]\n",
        "    original6 = arr6.copy()\n",
        "    prefix_sum_inplace(arr6)\n",
        "    # Verify first few and last few elements\n",
        "    assert arr6[0] == 1, f\"Test 6 failed: First element should be 1, got {arr6[0]}\"\n",
        "    assert arr6[1] == 3, f\"Test 6 failed: Second element should be 3, got {arr6[1]}\"\n",
        "    assert arr6[2] == 6, f\"Test 6 failed: Third element should be 6, got {arr6[2]}\"\n",
        "    assert arr6[-1] == sum(range(1, 1001)), f\"Test 6 failed: Last element incorrect\"\n",
        "    print(\"‚úì Test 6: Large array passed\")\n",
        "    \n",
        "    # Test case 7: Mixed positive and negative\n",
        "    arr7 = [5, -3, 2, -1, 4]\n",
        "    original7 = arr7.copy()\n",
        "    prefix_sum_inplace(arr7)\n",
        "    expected7 = [5, 2, 4, 3, 7]\n",
        "    assert arr7 == expected7, f\"Test 7 failed: Expected {expected7}, got {arr7}\"\n",
        "    print(\"‚úì Test 7: Mixed positive/negative passed\")\n",
        "    \n",
        "    # Test case 8: Single negative number\n",
        "    arr8 = [-10]\n",
        "    original8 = arr8.copy()\n",
        "    prefix_sum_inplace(arr8)\n",
        "    expected8 = [-10]\n",
        "    assert arr8 == expected8, f\"Test 8 failed: Expected {expected8}, got {arr8}\"\n",
        "    print(\"‚úì Test 8: Single negative passed\")\n",
        "    \n",
        "    # Test case 9: Verify in-place modification\n",
        "    test_arr = [1, 2, 3]\n",
        "    original_ref = id(test_arr)\n",
        "    prefix_sum_inplace(test_arr)\n",
        "    assert id(test_arr) == original_ref, \"Test 9 failed: Array reference changed (not in-place)\"\n",
        "    print(\"‚úì Test 9: In-place modification verified\")\n",
        "    \n",
        "    # Test case 10: Edge case with very small numbers\n",
        "    arr10 = [0.1, 0.2, 0.3]\n",
        "    original10 = arr10.copy()\n",
        "    prefix_sum_inplace(arr10)\n",
        "    expected10 = [0.1, 0.3, 0.6]\n",
        "    assert all(abs(a - b) < 1e-10 for a, b in zip(arr10, expected10)), f\"Test 10 failed: Expected {expected10}, got {arr10}\"\n",
        "    print(\"‚úì Test 10: Small numbers passed\")\n",
        "    \n",
        "    print(\"\\nüéâ All 10 prefix sum tests passed!\")\n",
        "\n",
        "def test_prefix_sum_performance():\n",
        "    \"\"\"Performance test for prefix sum.\"\"\"\n",
        "    import time\n",
        "    \n",
        "    print(\"\\nRunning performance test...\")\n",
        "    \n",
        "    # Test with large array\n",
        "    sizes = [1000, 10000, 100000]\n",
        "    for size in sizes:\n",
        "        arr = list(range(1, size + 1))\n",
        "        \n",
        "        start_time = time.time()\n",
        "        prefix_sum_inplace(arr)\n",
        "        end_time = time.time()\n",
        "        \n",
        "        execution_time = end_time - start_time\n",
        "        print(f\"Size {size:,}: {execution_time:.6f} seconds\")\n",
        "        \n",
        "        # Verify correctness\n",
        "        expected_sum = sum(range(1, size + 1))\n",
        "        assert arr[-1] == expected_sum, f\"Performance test failed for size {size}\"\n",
        "    \n",
        "    print(\"‚úì Performance test passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_prefix_sum()\n",
        "    test_prefix_sum_performance()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2: Tensor Operation Benchmarking (Easy-Medium)\n",
        "\n",
        "**Problem**: Compare the performance of Python loops vs NumPy vectorized operations for element-wise multiplication on large tensors.\n",
        "\n",
        "**Requirements**:\n",
        "- Implement both loop-based and vectorized versions\n",
        "- Measure execution time for different array sizes\n",
        "- Analyze memory usage patterns\n",
        "- Create a performance comparison plot\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "# Compare these approaches:\n",
        "# 1. Python loop: for i in range(n): result[i] = a[i] * b[i]\n",
        "# 2. NumPy vectorized: result = a * b\n",
        "```\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import tracemalloc\n",
        "\n",
        "def elementwise_multiply_loop(a, b):\n",
        "    \"\"\"\n",
        "    Element-wise multiplication using Python loops.\n",
        "    \n",
        "    Args:\n",
        "        a, b: 1D numpy arrays of same length\n",
        "        \n",
        "    Returns:\n",
        "        numpy array: element-wise product\n",
        "    \"\"\"\n",
        "    # TODO: Implement using Python loops\n",
        "    pass\n",
        "\n",
        "def elementwise_multiply_vectorized(a, b):\n",
        "    \"\"\"\n",
        "    Element-wise multiplication using NumPy vectorization.\n",
        "    \n",
        "    Args:\n",
        "        a, b: 1D numpy arrays of same length\n",
        "        \n",
        "    Returns:\n",
        "        numpy array: element-wise product\n",
        "    \"\"\"\n",
        "    # TODO: Implement using NumPy vectorization\n",
        "    pass\n",
        "\n",
        "def benchmark_operations(sizes=[1000, 10000, 100000, 1000000]):\n",
        "    \"\"\"\n",
        "    Benchmark both approaches across different array sizes.\n",
        "    \n",
        "    Args:\n",
        "        sizes: List of array sizes to test\n",
        "        \n",
        "    Returns:\n",
        "        dict: Results with timing and memory usage\n",
        "    \"\"\"\n",
        "    # TODO: Implement benchmarking\n",
        "    pass\n",
        "\n",
        "def plot_performance(results):\n",
        "    \"\"\"\n",
        "    Create performance comparison plots.\n",
        "    \n",
        "    Args:\n",
        "        results: Dictionary with benchmark results\n",
        "    \"\"\"\n",
        "    # TODO: Create plots showing:\n",
        "    # 1. Execution time vs array size\n",
        "    # 2. Memory usage vs array size\n",
        "    # 3. Speedup ratio\n",
        "    pass\n",
        "\n",
        "# Test the implementations\n",
        "if __name__ == \"__main__\":\n",
        "    # Quick test\n",
        "    a = np.random.randn(1000)\n",
        "    b = np.random.randn(1000)\n",
        "    \n",
        "    # Test both implementations\n",
        "    result_loop = elementwise_multiply_loop(a, b)\n",
        "    result_vec = elementwise_multiply_vectorized(a, b)\n",
        "    \n",
        "    # Verify they produce the same result\n",
        "    assert np.allclose(result_loop, result_vec), \"Results don't match!\"\n",
        "    print(\"Basic test passed!\")\n",
        "    \n",
        "    # Run full benchmark\n",
        "    results = benchmark_operations()\n",
        "    plot_performance(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3: Cache-Aware Matrix Operations (Medium)\n",
        "\n",
        "**Problem**: Implement matrix multiplication with cache-aware optimization. Compare different memory access patterns and their impact on performance.\n",
        "\n",
        "**Requirements**:\n",
        "- Implement standard matrix multiplication\n",
        "- Implement cache-optimized version (tiling/blocking)\n",
        "- Measure performance difference\n",
        "- Analyze cache miss rates\n",
        "\n",
        "**Background**: Matrix multiplication C = A √ó B where A is m√ók, B is k√ón, C is m√ón.\n",
        "The standard algorithm has poor cache locality when matrices don't fit in cache.\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from numba import jit\n",
        "\n",
        "def matrix_multiply_naive(A, B):\n",
        "    \"\"\"\n",
        "    Standard matrix multiplication with poor cache locality.\n",
        "    \n",
        "    Args:\n",
        "        A: numpy array of shape (m, k)\n",
        "        B: numpy array of shape (k, n)\n",
        "        \n",
        "    Returns:\n",
        "        numpy array of shape (m, n)\n",
        "    \"\"\"\n",
        "    m, k = A.shape\n",
        "    k2, n = B.shape\n",
        "    assert k == k2, \"Inner dimensions must match\"\n",
        "    \n",
        "    # TODO: Implement standard matrix multiplication\n",
        "    pass\n",
        "\n",
        "def matrix_multiply_tiled(A, B, tile_size=64):\n",
        "    \"\"\"\n",
        "    Cache-optimized matrix multiplication using tiling.\n",
        "    \n",
        "    Args:\n",
        "        A: numpy array of shape (m, k)\n",
        "        B: numpy array of shape (k, n)\n",
        "        tile_size: Size of the tile for blocking\n",
        "        \n",
        "    Returns:\n",
        "        numpy array of shape (m, n)\n",
        "    \"\"\"\n",
        "    m, k = A.shape\n",
        "    k2, n = B.shape\n",
        "    assert k == k2, \"Inner dimensions must match\"\n",
        "    \n",
        "    # TODO: Implement tiled matrix multiplication\n",
        "    pass\n",
        "\n",
        "@jit(nopython=True)\n",
        "def matrix_multiply_numba(A, B):\n",
        "    \"\"\"\n",
        "    Numba-optimized matrix multiplication for comparison.\n",
        "    \n",
        "    Args:\n",
        "        A: numpy array of shape (m, k)\n",
        "        B: numpy array of shape (k, n)\n",
        "        \n",
        "    Returns:\n",
        "        numpy array of shape (m, n)\n",
        "    \"\"\"\n",
        "    # TODO: Implement with Numba JIT compilation\n",
        "    pass\n",
        "\n",
        "def benchmark_matrix_multiply(sizes=[64, 128, 256, 512, 1024]):\n",
        "    \"\"\"\n",
        "    Benchmark different matrix multiplication approaches.\n",
        "    \n",
        "    Args:\n",
        "        sizes: List of matrix sizes to test (square matrices)\n",
        "        \n",
        "    Returns:\n",
        "        dict: Performance results\n",
        "    \"\"\"\n",
        "    # TODO: Implement benchmarking\n",
        "    pass\n",
        "\n",
        "def verify_correctness():\n",
        "    \"\"\"Verify all implementations produce the same result.\"\"\"\n",
        "    # TODO: Test with small matrices to ensure correctness\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Verify correctness first\n",
        "    verify_correctness()\n",
        "    print(\"Correctness tests passed!\")\n",
        "    \n",
        "    # Run benchmarks\n",
        "    results = benchmark_matrix_multiply()\n",
        "    \n",
        "    # Print results\n",
        "    for size in results['sizes']:\n",
        "        print(f\"Size {size}x{size}:\")\n",
        "        print(f\"  Naive: {results['naive_times'][size]:.4f}s\")\n",
        "        print(f\"  Tiled: {results['tiled_times'][size]:.4f}s\")\n",
        "        print(f\"  Numba: {results['numba_times'][size]:.4f}s\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4: Custom Sparse Tensor Operations (Medium-Hard)\n",
        "\n",
        "**Problem**: Implement a memory-efficient sparse tensor class with basic operations like addition, multiplication, and reshaping.\n",
        "\n",
        "**Requirements**:\n",
        "- Use COO (Coordinate) format for sparse representation\n",
        "- Implement tensor addition and element-wise multiplication\n",
        "- Support reshaping operations\n",
        "- Memory usage should be O(nnz) where nnz is number of non-zeros\n",
        "- Compare with dense tensor operations\n",
        "\n",
        "**Background**: Sparse tensors store only non-zero values, making them memory-efficient for data with many zeros.\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "class SparseTensor:\n",
        "    \"\"\"\n",
        "    Memory-efficient sparse tensor using COO (Coordinate) format.\n",
        "    \n",
        "    Stores only non-zero values with their coordinates.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, shape: Tuple[int, ...], values: np.ndarray = None, \n",
        "                 indices: np.ndarray = None):\n",
        "        \"\"\"\n",
        "        Initialize sparse tensor.\n",
        "        \n",
        "        Args:\n",
        "            shape: Tensor dimensions\n",
        "            values: Non-zero values (1D array)\n",
        "            indices: Coordinates of non-zero values (2D array, shape: [nnz, ndim])\n",
        "        \"\"\"\n",
        "        self.shape = shape\n",
        "        self.ndim = len(shape)\n",
        "        \n",
        "        if values is None:\n",
        "            self.values = np.array([], dtype=np.float32)\n",
        "            self.indices = np.array([], dtype=np.int32).reshape(0, self.ndim)\n",
        "        else:\n",
        "            self.values = values.astype(np.float32)\n",
        "            self.indices = indices.astype(np.int32)\n",
        "            self._validate()\n",
        "    \n",
        "    def _validate(self):\n",
        "        \"\"\"Validate tensor data.\"\"\"\n",
        "        # TODO: Add validation checks\n",
        "        pass\n",
        "    \n",
        "    def to_dense(self) -> np.ndarray:\n",
        "        \"\"\"Convert to dense tensor.\"\"\"\n",
        "        # TODO: Implement conversion to dense\n",
        "        pass\n",
        "    \n",
        "    @classmethod\n",
        "    def from_dense(cls, dense_tensor: np.ndarray) -> 'SparseTensor':\n",
        "        \"\"\"Create sparse tensor from dense tensor.\"\"\"\n",
        "        # TODO: Implement conversion from dense\n",
        "        pass\n",
        "    \n",
        "    def add(self, other: 'SparseTensor') -> 'SparseTensor':\n",
        "        \"\"\"Add two sparse tensors.\"\"\"\n",
        "        # TODO: Implement sparse tensor addition\n",
        "        pass\n",
        "    \n",
        "    def multiply_elementwise(self, other: 'SparseTensor') -> 'SparseTensor':\n",
        "        \"\"\"Element-wise multiplication of two sparse tensors.\"\"\"\n",
        "        # TODO: Implement element-wise multiplication\n",
        "        pass\n",
        "    \n",
        "    def reshape(self, new_shape: Tuple[int, ...]) -> 'SparseTensor':\n",
        "        \"\"\"Reshape the tensor.\"\"\"\n",
        "        # TODO: Implement reshaping\n",
        "        pass\n",
        "    \n",
        "    def memory_usage(self) -> int:\n",
        "        \"\"\"Calculate memory usage in bytes.\"\"\"\n",
        "        # TODO: Calculate actual memory usage\n",
        "        pass\n",
        "    \n",
        "    def sparsity(self) -> float:\n",
        "        \"\"\"Calculate sparsity ratio (fraction of zeros).\"\"\"\n",
        "        # TODO: Calculate sparsity\n",
        "        pass\n",
        "\n",
        "def create_sparse_tensor(shape: Tuple[int, ...], sparsity: float = 0.9) -> SparseTensor:\n",
        "    \"\"\"\n",
        "    Create a random sparse tensor with given sparsity.\n",
        "    \n",
        "    Args:\n",
        "        shape: Tensor dimensions\n",
        "        sparsity: Fraction of elements that should be zero\n",
        "        \n",
        "    Returns:\n",
        "        SparseTensor: Random sparse tensor\n",
        "    \"\"\"\n",
        "    # TODO: Implement random sparse tensor creation\n",
        "    pass\n",
        "\n",
        "def benchmark_sparse_vs_dense(shape: Tuple[int, ...], sparsity: float = 0.9):\n",
        "    \"\"\"\n",
        "    Benchmark sparse vs dense operations.\n",
        "    \n",
        "    Args:\n",
        "        shape: Tensor dimensions\n",
        "        sparsity: Sparsity level\n",
        "        \n",
        "    Returns:\n",
        "        dict: Performance comparison\n",
        "    \"\"\"\n",
        "    # TODO: Implement benchmarking\n",
        "    pass\n",
        "\n",
        "# Test cases\n",
        "def test_sparse_tensor():\n",
        "    \"\"\"Test sparse tensor functionality.\"\"\"\n",
        "    # TODO: Add comprehensive tests\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_sparse_tensor()\n",
        "    print(\"All tests passed!\")\n",
        "    \n",
        "    # Benchmark example\n",
        "    results = benchmark_sparse_vs_dense((100, 100), sparsity=0.95)\n",
        "    print(f\"Sparse memory: {results['sparse_memory']} bytes\")\n",
        "    print(f\"Dense memory: {results['dense_memory']} bytes\")\n",
        "    print(f\"Memory savings: {results['memory_savings']:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5: Memory Profiling and Optimization (Hard)\n",
        "\n",
        "**Problem**: Implement a comprehensive memory profiler for PyTorch operations and optimize a memory-intensive neural network forward pass.\n",
        "\n",
        "**Requirements**:\n",
        "- Create a memory profiler that tracks GPU/CPU memory usage\n",
        "- Profile a multi-layer neural network forward pass\n",
        "- Identify memory bottlenecks and optimize them\n",
        "- Implement gradient checkpointing to reduce memory usage\n",
        "- Compare memory usage before and after optimization\n",
        "\n",
        "**Background**: Large neural networks can consume massive amounts of memory. Understanding and optimizing memory usage is crucial for training large models.\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import psutil\n",
        "import gc\n",
        "from contextlib import contextmanager\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "class MemoryProfiler:\n",
        "    \"\"\"\n",
        "    Memory profiler for PyTorch operations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.memory_log = []\n",
        "        self.peak_memory = 0\n",
        "        \n",
        "    def get_memory_usage(self) -> Dict[str, float]:\n",
        "        \"\"\"Get current memory usage in MB.\"\"\"\n",
        "        # TODO: Implement memory usage tracking\n",
        "        pass\n",
        "    \n",
        "    def log_memory(self, operation_name: str):\n",
        "        \"\"\"Log memory usage for an operation.\"\"\"\n",
        "        # TODO: Implement memory logging\n",
        "        pass\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset profiler state.\"\"\"\n",
        "        # TODO: Reset profiler\n",
        "        pass\n",
        "    \n",
        "    def get_peak_memory(self) -> float:\n",
        "        \"\"\"Get peak memory usage in MB.\"\"\"\n",
        "        # TODO: Return peak memory\n",
        "        pass\n",
        "\n",
        "class LargeNeuralNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Memory-intensive neural network for profiling.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size: int = 1000, hidden_sizes: List[int] = [2000, 2000, 2000], \n",
        "                 output_size: int = 100, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # TODO: Implement large neural network\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass without optimization.\"\"\"\n",
        "        # TODO: Implement forward pass\n",
        "        pass\n",
        "    \n",
        "    def forward_with_checkpointing(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass with gradient checkpointing.\"\"\"\n",
        "        # TODO: Implement checkpointed forward pass\n",
        "        pass\n",
        "\n",
        "def profile_network_memory(model: nn.Module, input_tensor: torch.Tensor, \n",
        "                          use_checkpointing: bool = False) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Profile memory usage of network forward pass.\n",
        "    \n",
        "    Args:\n",
        "        model: Neural network model\n",
        "        input_tensor: Input tensor\n",
        "        use_checkpointing: Whether to use gradient checkpointing\n",
        "        \n",
        "    Returns:\n",
        "        dict: Memory usage statistics\n",
        "    \"\"\"\n",
        "    # TODO: Implement memory profiling\n",
        "    pass\n",
        "\n",
        "def optimize_memory_usage(model: nn.Module, input_tensor: torch.Tensor) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Apply memory optimizations and measure improvement.\n",
        "    \n",
        "    Args:\n",
        "        model: Neural network model\n",
        "        input_tensor: Input tensor\n",
        "        \n",
        "    Returns:\n",
        "        dict: Optimization results\n",
        "    \"\"\"\n",
        "    # TODO: Implement memory optimizations\n",
        "    pass\n",
        "\n",
        "def compare_memory_strategies():\n",
        "    \"\"\"Compare different memory optimization strategies.\"\"\"\n",
        "    # TODO: Implement comparison\n",
        "    pass\n",
        "\n",
        "# Test the implementation\n",
        "if __name__ == \"__main__\":\n",
        "    # Create model and test data\n",
        "    model = LargeNeuralNetwork()\n",
        "    input_tensor = torch.randn(32, 1000)  # batch_size=32\n",
        "    \n",
        "    print(\"Profiling memory usage...\")\n",
        "    \n",
        "    # Profile without optimization\n",
        "    results_baseline = profile_network_memory(model, input_tensor, use_checkpointing=False)\n",
        "    print(f\"Baseline memory usage: {results_baseline['peak_memory']:.2f} MB\")\n",
        "    \n",
        "    # Profile with optimization\n",
        "    results_optimized = profile_network_memory(model, input_tensor, use_checkpointing=True)\n",
        "    print(f\"Optimized memory usage: {results_optimized['peak_memory']:.2f} MB\")\n",
        "    \n",
        "    # Calculate improvement\n",
        "    improvement = (results_baseline['peak_memory'] - results_optimized['peak_memory']) / results_baseline['peak_memory']\n",
        "    print(f\"Memory reduction: {improvement:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üí° Hints\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 1: Memory-Efficient Prefix Sum</summary>\n",
        "\n",
        "**Hint**: For in-place prefix sum, you need to iterate through the array once and update each element to be the sum of all previous elements plus itself. Start from index 1 (since index 0 doesn't need to change) and for each position i, set `arr[i] = arr[i-1] + arr[i]`.\n",
        "\n",
        "**Key insight**: Each element becomes the sum of all elements from the beginning up to and including itself.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 2: Tensor Operation Benchmarking</summary>\n",
        "\n",
        "**Hint**: For the loop version, use a simple for loop with indexing. For the vectorized version, use NumPy's built-in multiplication operator. Use `time.time()` to measure execution time and `tracemalloc` to track memory usage. Create arrays of different sizes and measure both time and memory for each approach.\n",
        "\n",
        "**Key insight**: Vectorized operations should be significantly faster due to SIMD instructions and reduced Python overhead.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 3: Cache-Aware Matrix Operations</summary>\n",
        "\n",
        "**Hint**: For tiling, divide the matrices into smaller blocks (tiles) and process them separately. This improves cache locality by keeping frequently accessed data in cache. The tile size should be chosen based on cache size (typically 64x64 or 128x128). For Numba, use the `@jit(nopython=True)` decorator to compile the function.\n",
        "\n",
        "**Key insight**: Tiling reduces cache misses by ensuring that when you access a tile, all its data fits in cache.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 4: Custom Sparse Tensor Operations</summary>\n",
        "\n",
        "**Hint**: In COO format, store non-zero values and their coordinates separately. For addition, you need to merge the coordinate lists and sum values at the same coordinates. For element-wise multiplication, only multiply values that exist in both tensors. For reshaping, convert 1D coordinates to multi-dimensional coordinates.\n",
        "\n",
        "**Key insight**: Sparse operations should only work on non-zero elements, making them much more memory-efficient for sparse data.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 5: Memory Profiling and Optimization</summary>\n",
        "\n",
        "**Hint**: Use `torch.cuda.memory_allocated()` and `torch.cuda.max_memory_allocated()` for GPU memory tracking. For gradient checkpointing, use `torch.utils.checkpoint.checkpoint()` to trade compute for memory. Implement the network with multiple large layers and measure memory usage at each step.\n",
        "\n",
        "**Key insight**: Gradient checkpointing saves memory by recomputing activations during backward pass instead of storing them during forward pass.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
