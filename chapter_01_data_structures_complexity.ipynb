{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Data Structures & Complexity for ML Engineers\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jmamath/interview_prep/blob/main/chapter_01_data_structures_complexity.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As a machine learning engineer at Google DeepMind, Iâ€™ve seen firsthand how the performance of a model can make or break a project. A model that takes days to train or milliseconds too long to generate a prediction can be the difference between a groundbreaking innovation and a failed experiment. And often, the key to unlocking that performance lies not in the high-level model architecture, but in the low-level details of how we handle data.\n",
    "\n",
    "This chapter is about those details. Weâ€™re going to take a step back from the glamour of neural network design and get our hands dirty with the fundamentals of data structures and algorithmic complexity. Why? Because in the world of large-scale machine learning, a seemingly small choiceâ€”like how you iterate through a tensor or whether you update an array in-placeâ€”can have a massive impact on performance and memory usage.\n",
    "\n",
    "By the end of this chapter, youâ€™ll have a deeper appreciation for the importance of data structures and complexity analysis in machine learning, and youâ€™ll be equipped with the practical skills to write more efficient and scalable ML code. Let's get started!\n",
    "\n",
    "## Learning Objectives\n",
    "- Analyze algorithm complexity (Big O time and space)\n",
    "- Understand memory layout, cache effects, and vectorization\n",
    "- Implement memory-efficient algorithms\n",
    "- Profile and optimize memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Memory-Efficient Prefix Sum (Easy)\n",
    "\n",
    "### Contextual Introduction\n",
    "In many ML applications, especially in sequence modeling and data analysis, we need to compute running totals or cumulative sums. For example, when implementing custom attention mechanisms or calculating running statistics in a data stream, prefix sums are a fundamental operation. A naive implementation might create a new array to store the cumulative sums, but this can be memory-intensive for large sequences. By performing the operation in-place, we can save memory and improve performance, which is crucial when working with large datasets and models.\n",
    "\n",
    "### Key Concepts\n",
    "- **In-place operations**: Modifying the input data directly without creating a copy.\n",
    "- **Space Complexity**: The amount of memory an algorithm needs. An in-place algorithm has O(1) auxiliary space complexity.\n",
    "- **Time Complexity**: The amount of time an algorithm takes to run. A single pass through an array is O(n).\n",
    "\n",
    "### Problem Statement\n",
    "Implement a memory-efficient prefix sum algorithm that computes the cumulative sum of an array in-place.\n",
    "\n",
    "**Requirements**:\n",
    "- Modify the input array in-place (O(1) extra space).\n",
    "- Time complexity should be O(n).\n",
    "- Handle edge cases like an empty array or a single-element array.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "arr = [1, 2, 3, 4, 5]\n",
    "prefix_sum_inplace(arr)\n",
    "print(arr)  # Expected output: [1, 3, 6, 10, 15]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_sum_inplace(arr):\n",
    "    \"\"\"Computes the prefix sum of an array in-place.\"\"\"\n",
    "    for i in range(1, len(arr)):\n",
    "        arr[i] += arr[i-1]\n",
    "\n",
    "def test_prefix_sum_inplace():\n",
    "    \"\"\"Tests the in-place prefix sum implementation.\"\"\"\n",
    "    # Test case 1: Basic functionality\n",
    "    arr1 = [1, 2, 3, 4, 5]\n",
    "    prefix_sum_inplace(arr1)\n",
    "    assert arr1 == [1, 3, 6, 10, 15], f\"Test 1 Failed: {arr1}\"\n",
    "\n",
    "    # Test case 2: Empty array\n",
    "    arr2 = []\n",
    "    prefix_sum_inplace(arr2)\n",
    "    assert arr2 == [], f\"Test 2 Failed: {arr2}\"\n",
    "\n",
    "    # Test case 3: Single element\n",
    "    arr3 = [10]\n",
    "    prefix_sum_inplace(arr3)\n",
    "    assert arr3 == [10], f\"Test 3 Failed: {arr3}\"\n",
    "\n",
    "    # Test case 4: Array with negative numbers\n",
    "    arr4 = [1, -2, 3, -4, 5]\n",
    "    prefix_sum_inplace(arr4)\n",
    "    assert arr4 == [1, -1, 2, -2, 3], f\"Test 4 Failed: {arr4}\"\n",
    "\n",
    "    print(\"ðŸŽ‰ All prefix sum tests passed!\")\n",
    "\n",
    "test_prefix_sum_inplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 1</summary>\n",
    "\n",
    "**Hint**: Iterate through the array starting from the second element. For each element at index `i`, update it by adding the value of the element at index `i-1`. This way, each element becomes the sum of itself and all previous elements.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Tensor Operation Benchmarking (Easy-Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "In deep learning, we work with large multi-dimensional arrays called tensors. Operations on these tensors, like element-wise multiplication, are the building blocks of neural networks. A naive way to implement these operations is to use Python loops, but this is incredibly slow. High-performance ML libraries like NumPy and PyTorch use vectorized operations, which are implemented in C or CUDA and can perform operations on entire arrays at once. Understanding the performance difference between loops and vectorization is fundamental for writing efficient ML code.\n",
    "\n",
    "### Key Concepts\n",
    "- **Vectorization**: Performing operations on entire arrays at once, rather than element by element.\n",
    "- **SIMD (Single Instruction, Multiple Data)**: A hardware feature that allows a single instruction to be applied to multiple data points simultaneously. Vectorized operations leverage SIMD.\n",
    "- **Benchmarking**: Measuring the performance of code, typically in terms of execution time and memory usage.\n",
    "\n",
    "### Problem Statement\n",
    "Compare the performance of Python loops vs. NumPy vectorized operations for element-wise multiplication on large tensors. You will implement both versions, measure their execution time and memory usage for different array sizes, and create a plot to visualize the performance difference.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement both loop-based and vectorized versions of element-wise multiplication.\n",
    "- Measure and plot execution time and memory usage for different array sizes.\n",
    "- Analyze the speedup of the vectorized version over the loop version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def elementwise_multiply_loop(a, b):\n",
    "    result = np.zeros_like(a)\n",
    "    for i in range(len(a)):\n",
    "        result[i] = a[i] * b[i]\n",
    "    return result\n",
    "\n",
    "def elementwise_multiply_vectorized(a, b):\n",
    "    return a * b\n",
    "\n",
    "def benchmark_operations():\n",
    "    sizes = [10**i for i in range(1, 7)]\n",
    "    loop_times = []\n",
    "    vectorized_times = []\n",
    "\n",
    "    for size in sizes:\n",
    "        a = np.random.rand(size)\n",
    "        b = np.random.rand(size)\n",
    "\n",
    "        start_time = time.time()\n",
    "        elementwise_multiply_loop(a, b)\n",
    "        loop_times.append(time.time() - start_time)\n",
    "\n",
    "        start_time = time.time()\n",
    "        elementwise_multiply_vectorized(a, b)\n",
    "        vectorized_times.append(time.time() - start_time)\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sizes, loop_times, 'o-', label='Loop')\n",
    "    plt.plot(sizes, vectorized_times, 'o-', label='Vectorized')\n",
    "    plt.title('Loop vs. Vectorized Performance')\n",
    "    plt.xlabel('Array Size')\n",
    "    plt.ylabel('Execution Time (seconds)')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "benchmark_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 2</summary>\n",
    "\n",
    "**Hint**: Use the `time` module to measure the execution time of each function. For the loop version, iterate from 0 to the length of the array. For the vectorized version, simply use the `*` operator on the two NumPy arrays. Use `matplotlib` to plot the results with a logarithmic scale for both axes to better visualize the performance difference across a wide range of array sizes.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Cache-Aware Matrix Operations (Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "Matrix multiplication is at the heart of deep learning, forming the basis of fully connected layers and convolutions. When matrices are large, the way we access their elements in memory can have a huge impact on performance. Modern CPUs have a memory hierarchy (caches) where accessing data that is already in a cache is much faster than fetching it from main memory. A naive matrix multiplication algorithm can have poor cache locality, leading to many cache misses and slow performance. By using a cache-aware technique like tiling (or blocking), we can significantly speed up the operation.\n",
    "\n",
    "### Key Concepts\n",
    "- **Cache Locality**: The principle that if a memory location is accessed, it's likely that nearby memory locations will be accessed soon (spatial locality) and the same location will be accessed again soon (temporal locality).\n",
    "- **Tiling/Blocking**: A technique to improve cache locality by breaking down a large matrix multiplication into smaller matrix multiplications on sub-matrices (tiles) that can fit into the cache.\n",
    "- **Numba**: A just-in-time (JIT) compiler for Python that can translate a subset of Python and NumPy code into fast machine code, often achieving C-like speeds.\n",
    "\n",
    "### Problem Statement\n",
    "Implement matrix multiplication using a standard naive approach and a cache-optimized tiled approach. Compare their performance against each other and against a Numba-optimized version.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement a standard (i, j, k) matrix multiplication.\n",
    "- Implement a tiled version of matrix multiplication.\n",
    "- Use Numba's `@jit` decorator to create a JIT-compiled version.\n",
    "- Benchmark the three versions for different matrix sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numba import jit\n",
    "\n",
    "def matrix_multiply_naive(A, B):\n",
    "    m, k = A.shape\n",
    "    k2, n = B.shape\n",
    "    assert k == k2, \"Inner dimensions must match\"\n",
    "    C = np.zeros((m, n))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            for l in range(k):\n",
    "                C[i, j] += A[i, l] * B[l, j]\n",
    "    return C\n",
    "\n",
    "def matrix_multiply_tiled(A, B, tile_size=16):\n",
    "    m, k = A.shape\n",
    "    k2, n = B.shape\n",
    "    assert k == k2, \"Inner dimensions must match\"\n",
    "    C = np.zeros((m, n))\n",
    "    for i0 in range(0, m, tile_size):\n",
    "        for j0 in range(0, n, tile_size):\n",
    "            for l0 in range(0, k, tile_size):\n",
    "                for i in range(i0, min(i0 + tile_size, m)):\n",
    "                    for j in range(j0, min(j0 + tile_size, n)):\n",
    "                        for l in range(l0, min(l0 + tile_size, k)):\n",
    "                            C[i, j] += A[i, l] * B[l, j]\n",
    "    return C\n",
    "\n",
    "@jit(nopython=True)\n",
    "def matrix_multiply_numba(A, B):\n",
    "    m, k = A.shape\n",
    "    k2, n = B.shape\n",
    "    C = np.zeros((m, n))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            for l in range(k):\n",
    "                C[i, j] += A[i, l] * B[l, j]\n",
    "    return C\n",
    "\n",
    "def benchmark_matrix_multiplication():\n",
    "    sizes = [64, 128, 256, 512]\n",
    "    results = {s: {} for s in sizes}\n",
    "\n",
    "    for size in sizes:\n",
    "        A = np.random.rand(size, size)\n",
    "        B = np.random.rand(size, size)\n",
    "\n",
    "        start_time = time.time()\n",
    "        matrix_multiply_naive(A, B)\n",
    "        results[size]['naive'] = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        matrix_multiply_tiled(A, B)\n",
    "        results[size]['tiled'] = time.time() - start_time\n",
    "\n",
    "        # Warm up Numba\n",
    "        matrix_multiply_numba(A, B)\n",
    "        start_time = time.time()\n",
    "        matrix_multiply_numba(A, B)\n",
    "        results[size]['numba'] = time.time() - start_time\n",
    "\n",
    "    # Print results\n",
    "    for size, timings in results.items():\n",
    "        print(f\"Matrix size: {size}x{size}\")\n",
    "        print(f\"  Naive: {timings['naive']:.4f}s\")\n",
    "        print(f\"  Tiled: {timings['tiled']:.4f}s\")\n",
    "        print(f\"  Numba: {timings['numba']:.4f}s\")\n",
    "\n",
    "benchmark_matrix_multiplication()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 3</summary>\n",
    "\n",
    "**Hint**: For the tiled version, you will need three nested loops to iterate over the tiles, and then three more nested loops to perform the multiplication within each tile. The key is that the inner loops will be operating on small sub-matrices that fit in the cache. For the Numba version, simply apply the `@jit(nopython=True)` decorator to your naive implementation.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Custom Sparse Tensor Operations (Medium-Hard)\n",
    "\n",
    "### Contextual Introduction\n",
    "In many ML applications, such as natural language processing (e.g., word embeddings) and recommendation systems, we deal with very high-dimensional but sparse data. For example, a user-item interaction matrix in a recommendation system might have millions of rows and columns, but each user has only interacted with a few items. Storing this as a dense matrix would be prohibitively expensive in terms of memory. Sparse tensors are data structures that only store the non-zero elements, making them much more memory-efficient.\n",
    "\n",
    "### Key Concepts\n",
    "- **Sparsity**: The fraction of zero elements in a tensor.\n",
    "- **COO (Coordinate) format**: A way to represent a sparse tensor by storing a list of (row, column, value) tuples.\n",
    "- **Memory Efficiency**: Sparse tensors have a memory usage of O(nnz), where nnz is the number of non-zero elements, as opposed to O(N*M) for a dense matrix.\n",
    "\n",
    "### Problem Statement\n",
    "Implement a memory-efficient sparse tensor class using the COO format. The class should support conversion to and from a dense NumPy array, as well as a `memory_usage` method to demonstrate its efficiency.\n",
    "\n",
    "**Requirements**:\n",
    "- Use COO format (indices and values) to store non-zero elements.\n",
    "- Implement `to_dense` and `from_dense` methods.\n",
    "- Implement a `memory_usage` method that returns the memory used by the sparse representation.\n",
    "- Compare the memory usage of the sparse tensor with its dense equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SparseTensor:\n",
    "    def __init__(self, shape, indices, values):\n",
    "        self.shape = shape\n",
    "        self.indices = indices\n",
    "        self.values = values\n",
    "\n",
    "    @classmethod\n",
    "    def from_dense(cls, dense_tensor):\n",
    "        indices = np.argwhere(dense_tensor != 0)\n",
    "        values = dense_tensor[indices[:, 0], indices[:, 1]]\n",
    "        return cls(dense_tensor.shape, indices, values)\n",
    "\n",
    "    def to_dense(self):\n",
    "        dense_tensor = np.zeros(self.shape)\n",
    "        for i, v in zip(self.indices, self.values):\n",
    "            dense_tensor[tuple(i)] = v\n",
    "        return dense_tensor\n",
    "\n",
    "    def memory_usage(self):\n",
    "        return self.indices.nbytes + self.values.nbytes\n",
    "\n",
    "def test_sparse_tensor():\n",
    "    # Create a large, sparse dense tensor\n",
    "    dense_tensor = np.zeros((1000, 1000))\n",
    "    dense_tensor[10, 20] = 5\n",
    "    dense_tensor[100, 200] = 10\n",
    "\n",
    "    # Convert to sparse tensor\n",
    "    sparse_tensor = SparseTensor.from_dense(dense_tensor)\n",
    "\n",
    "    # Check memory usage\n",
    "    dense_memory = dense_tensor.nbytes\n",
    "    sparse_memory = sparse_tensor.memory_usage()\n",
    "\n",
    "    print(f\"Dense tensor memory: {dense_memory / 1024**2:.4f} MB\")\n",
    "    print(f\"Sparse tensor memory: {sparse_memory / 1024**2:.4f} MB\")\n",
    "    print(f\"Memory savings: {1 - sparse_memory / dense_memory:.2%}\")\n",
    "\n",
    "    # Verify correctness\n",
    "    reconstructed_dense = sparse_tensor.to_dense()\n",
    "    assert np.allclose(dense_tensor, reconstructed_dense)\n",
    "    print(\"\\nðŸŽ‰ Sparse tensor implementation is correct and memory-efficient!\")\n",
    "\n",
    "test_sparse_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 4</summary>\n",
    "\n",
    "**Hint**: For `from_dense`, use `np.argwhere` to find the indices of non-zero elements. For `to_dense`, create a zero-filled NumPy array of the correct shape and then fill in the non-zero values using the stored indices and values. The memory usage is the sum of the bytes used by the indices array and the values array, which you can get from the `.nbytes` attribute of a NumPy array.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Memory Profiling and Optimization (Hard)\n",
    "\n",
    "### Contextual Introduction\n",
    "Training large neural networks, like the ones used for language modeling or image generation, is often limited by GPU memory. During the forward pass, the activations (outputs of each layer) are stored in memory to be used for gradient calculations in the backward pass. For very deep or wide networks, these activations can consume a huge amount of memory. Gradient checkpointing is a technique that trades compute for memory by not storing the activations for some layers and instead recomputing them during the backward pass. This can allow you to train much larger models than would otherwise fit in memory.\n",
    "\n",
    "### Key Concepts\n",
    "- **Memory Profiling**: The process of measuring and analyzing the memory usage of a program.\n",
    "- **Gradient Checkpointing**: A technique to reduce the memory footprint of a neural network by recomputing activations during the backward pass instead of storing them.\n",
    "- **Activation**: The output of a layer in a neural network.\n",
    "\n",
    "### Problem Statement\n",
    "Implement a simple memory profiler for PyTorch and use it to compare the memory usage of a neural network forward pass with and without gradient checkpointing.\n",
    "\n",
    "**Requirements**:\n",
    "- Create a `MemoryProfiler` class that can track peak GPU memory usage.\n",
    "- Implement a simple multi-layer neural network.\n",
    "- Implement a version of the forward pass that uses `torch.utils.checkpoint.checkpoint`.\n",
    "- Profile the memory usage of both forward passes and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class MemoryProfiler:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def profile(self, forward_pass_func, *args):\n",
    "        torch.cuda.reset_peak_memory_stats(self.device)\n",
    "        torch.cuda.synchronize()\n",
    "        _ = forward_pass_func(*args)\n",
    "        torch.cuda.synchronize()\n",
    "        peak_memory = torch.cuda.max_memory_allocated(self.device) / 1024**2\n",
    "        return peak_memory\n",
    "\n",
    "class LargeNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size=1024, hidden_size=4096, num_layers=8):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        return x\n",
    "\n",
    "    def forward_with_checkpointing(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = checkpoint(lambda y: torch.relu(layer(y)), x)\n",
    "        return x\n",
    "\n",
    "def test_memory_profiling():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, skipping memory profiling test.\")\n",
    "        return\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = LargeNeuralNetwork().to(device)\n",
    "    profiler = MemoryProfiler(model, device)\n",
    "\n",
    "    # Profile standard forward pass\n",
    "    input_tensor = torch.randn(16, 1024, device=device)\n",
    "    peak_mem_standard = profiler.profile(model.forward, input_tensor)\n",
    "    print(f\"Standard forward pass peak memory: {peak_mem_standard:.2f} MB\")\n",
    "\n",
    "    # Profile checkpointed forward pass\n",
    "    peak_mem_checkpointed = profiler.profile(model.forward_with_checkpointing, input_tensor)\n",
    "    print(f\"Checkpointed forward pass peak memory: {peak_mem_checkpointed:.2f} MB\")\n",
    "\n",
    "    # Compare results\n",
    "    print(f\"Memory savings with checkpointing: {1 - peak_mem_checkpointed / peak_mem_standard:.2%}\")\n",
    "    assert peak_mem_checkpointed < peak_mem_standard\n",
    "    print(\"\\nðŸŽ‰ Gradient checkpointing successfully reduced memory usage!\")\n",
    "\n",
    "test_memory_profiling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 5</summary>\n",
    "\n",
    "**Hint**: Use `torch.cuda.reset_peak_memory_stats()` before your forward pass and `torch.cuda.max_memory_allocated()` after to get the peak memory usage. For the checkpointed forward pass, wrap the application of each layer (or a sequence of layers) in the `torch.utils.checkpoint.checkpoint` function. This function takes a function to be run (e.g., a lambda that applies the layer) and the input to that function.\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
