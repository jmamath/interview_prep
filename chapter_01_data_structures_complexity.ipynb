{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 1: Data Structures & Complexity for ML Engineers\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jmamath/interview_prep/blob/main/chapter_01_data_structures_complexity.ipynb)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "As a machine learning engineer at Google DeepMind, I’ve seen firsthand how the performance of a model can make or break a project. A model that takes days to train or milliseconds too long to generate a prediction can be the difference between a groundbreaking innovation and a failed experiment. And often, the key to unlocking that performance lies not in the high-level model architecture, but in the low-level details of how we handle data.\n",
        "\n",
        "This chapter is about those details. We’re going to take a step back from the glamour of neural network design and get our hands dirty with the fundamentals of data structures and algorithmic complexity. Why? Because in the world of large-scale machine learning, a seemingly small choice—like how you iterate through a tensor or whether you update an array in-place—can have a massive impact on performance and memory usage.\n",
        "\n",
        "By the end of this chapter, you’ll have a deeper appreciation for the importance of data structures and complexity analysis in machine learning, and you’ll be equipped with the practical skills to write more efficient and scalable ML code. Let's get started!\n",
        "\n",
        "## Learning Objectives\n",
        "- Analyze algorithm complexity (Big O time and space)\n",
        "- Understand memory layout, cache effects, and vectorization\n",
        "- Implement memory-efficient algorithms\n",
        "- Profile and optimize memory usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 1: Memory-Efficient Prefix Sum (Easy)\n",
        "\n",
        "### Contextual Introduction\n",
        "In many ML applications, especially in sequence modeling and data analysis, we need to compute running totals or cumulative sums. For example, when implementing custom attention mechanisms or calculating running statistics in a data stream, prefix sums are a fundamental operation. A naive implementation might create a new array to store the cumulative sums, but this can be memory-intensive for large sequences. By performing the operation in-place, we can save memory and improve performance, which is crucial when working with large datasets and models.\n",
        "\n",
        "### Key Concepts\n",
        "- **In-place operations**: Modifying the input data directly without creating a copy.\n",
        "- **Space Complexity**: The amount of memory an algorithm needs. An in-place algorithm has O(1) auxiliary space complexity.\n",
        "- **Time Complexity**: The amount of time an algorithm takes to run. A single pass through an array is O(n).\n",
        "\n",
        "### Problem Statement\n",
        "Implement a memory-efficient prefix sum algorithm that computes the cumulative sum of an array in-place.\n",
        "\n",
        "**Requirements**:\n",
        "- Modify the input array in-place (O(1) extra space).\n",
        "- Time complexity should be O(n).\n",
        "- Handle edge cases like an empty array or a single-element array.\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "arr = [1, 2, 3, 4, 5]\n",
        "prefix_sum_inplace(arr)\n",
        "print(arr)  # Expected output: [1, 3, 6, 10, 15]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prefix_sum_inplace(arr):\n",
        "    \"\"\"\n",
        "    Computes the prefix sum of an array in-place.\n",
        "    \n",
        "    Args:\n",
        "        arr: A list of integers\n",
        "        \n",
        "    Returns:\n",
        "        None (modifies arr in-place)\n",
        "    \"\"\"\n",
        "    # TODO: Implement the prefix sum algorithm\n",
        "    # Start from index 1 and modify each element by adding the previous element\n",
        "    # Remember: You need to preserve the loop structure below\n",
        "    pass\n",
        "\n",
        "# Test your implementation\n",
        "def test_prefix_sum_inplace():\n",
        "    \"\"\"Tests the in-place prefix sum implementation.\"\"\"\n",
        "    # Test case 1: Basic functionality\n",
        "    arr1 = [1, 2, 3, 4, 5]\n",
        "    prefix_sum_inplace(arr1)\n",
        "    assert arr1 == [1, 3, 6, 10, 15], f\"Test 1 Failed: {arr1}\"\n",
        "\n",
        "    # Test case 2: Empty array\n",
        "    arr2 = []\n",
        "    prefix_sum_inplace(arr2)\n",
        "    assert arr2 == [], f\"Test 2 Failed: {arr2}\"\n",
        "\n",
        "    # Test case 3: Single element\n",
        "    arr3 = [10]\n",
        "    prefix_sum_inplace(arr3)\n",
        "    assert arr3 == [10], f\"Test 3 Failed: {arr3}\"\n",
        "\n",
        "    # Test case 4: Array with negative numbers\n",
        "    arr4 = [1, -2, 3, -4, 5]\n",
        "    prefix_sum_inplace(arr4)\n",
        "    assert arr4 == [1, -1, 2, -2, 3], f\"Test 4 Failed: {arr4}\"\n",
        "\n",
        "    print(\"✓ All prefix sum tests passed!\")\n",
        "\n",
        "# Run the tests\n",
        "test_prefix_sum_inplace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Click to reveal hint for Problem 1</summary>\n",
        "\n",
        "**Hint**: Iterate through the array starting from the second element. For each element at index `i`, update it by adding the value of the element at index `i-1`. This way, each element becomes the sum of itself and all previous elements.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 2: Tensor Operation Benchmarking (Easy-Medium)\n",
        "\n",
        "### Contextual Introduction\n",
        "In deep learning, we work with large multi-dimensional arrays called tensors. Operations on these tensors, like element-wise multiplication, are the building blocks of neural networks. A naive way to implement these operations is to use Python loops, but this is incredibly slow. High-performance ML libraries like NumPy and PyTorch use vectorized operations, which are implemented in C or CUDA and can perform operations on entire arrays at once. Understanding the performance difference between loops and vectorization is fundamental for writing efficient ML code.\n",
        "\n",
        "### Key Concepts\n",
        "- **Vectorization**: Performing operations on entire arrays at once, rather than element by element.\n",
        "- **SIMD (Single Instruction, Multiple Data)**: A hardware feature that allows a single instruction to be applied to multiple data points simultaneously. Vectorized operations leverage SIMD.\n",
        "- **Benchmarking**: Measuring the performance of code, typically in terms of execution time and memory usage.\n",
        "\n",
        "### Problem Statement\n",
        "Compare the performance of Python loops vs. NumPy vectorized operations for element-wise multiplication on large tensors. You will implement both versions, measure their execution time and memory usage for different array sizes, and create a plot to visualize the performance difference.\n",
        "\n",
        "**Requirements**:\n",
        "- Implement both loop-based and vectorized versions of element-wise multiplication.\n",
        "- Measure and plot execution time and memory usage for different array sizes.\n",
        "- Analyze the speedup of the vectorized version over the loop version.\n",
        "\n",
        "### Example: Loop vs. Vectorized Operations\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Here's the difference between loop and vectorized approaches:\n",
        "\n",
        "# LOOP VERSION (slow)\n",
        "a = np.array([1, 2, 3, 4])\n",
        "b = np.array([2, 2, 2, 2])\n",
        "result_loop = np.zeros_like(a)\n",
        "for i in range(len(a)):\n",
        "    result_loop[i] = a[i] * b[i]  # One operation at a time\n",
        "# Result: [2, 4, 6, 8]\n",
        "\n",
        "# VECTORIZED VERSION (fast)\n",
        "result_vectorized = a * b  # All operations at once\n",
        "# Result: [2, 4, 6, 8]\n",
        "\n",
        "# Same result, but vectorized is 100-1000x faster!\n",
        "```\n",
        "\n",
        "### Your Exercise\n",
        "Implement and benchmark both approaches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def elementwise_multiply_loop(a, b):\n",
        "    \"\"\"\n",
        "    Element-wise multiplication using Python loops.\n",
        "    \n",
        "    TODO: Implement this function\n",
        "    - Create a result array of zeros with the same shape as 'a'\n",
        "    - Iterate through indices and multiply element-by-element\n",
        "    - Return the result\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def elementwise_multiply_vectorized(a, b):\n",
        "    \"\"\"\n",
        "    Element-wise multiplication using NumPy vectorization.\n",
        "    \n",
        "    TODO: Implement this function\n",
        "    - Simply return a * b (let NumPy handle the vectorization)\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def benchmark_operations(sizes=[10**i for i in range(1, 6)]):\n",
        "    \"\"\"\n",
        "    Benchmark loop vs. vectorized operations.\n",
        "    \n",
        "    TODO: Implement this function\n",
        "    - For each size, create random arrays a and b\n",
        "    - Time elementwise_multiply_loop(a, b)\n",
        "    - Time elementwise_multiply_vectorized(a, b)\n",
        "    - Store the times in loop_times and vectorized_times lists\n",
        "    - Plot the results with matplotlib (logarithmic scales recommended)\n",
        "    \"\"\"\n",
        "    loop_times = []\n",
        "    vectorized_times = []\n",
        "    \n",
        "    for size in sizes:\n",
        "        # TODO: Create random arrays\n",
        "        # TODO: Time loop version\n",
        "        # TODO: Time vectorized version\n",
        "        pass\n",
        "    \n",
        "    # TODO: Create a plot comparing loop_times and vectorized_times\n",
        "    # Use log scale for both axes\n",
        "    pass\n",
        "\n",
        "# Test your implementation\n",
        "try:\n",
        "    benchmark_operations()\n",
        "    print(\"✓ Benchmarking complete!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Make sure both functions and the benchmark are implemented.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Click to reveal hint for Problem 2</summary>\n",
        "\n",
        "**Hint**: Use the `time` module to measure the execution time of each function. For the loop version, iterate from 0 to the length of the array. For the vectorized version, simply use the `*` operator on the two NumPy arrays. Use `matplotlib` to plot the results with a logarithmic scale for both axes to better visualize the performance difference across a wide range of array sizes.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 3: Cache-Aware Matrix Operations (Medium)\n",
        "\n",
        "### Contextual Introduction\n",
        "Matrix multiplication is at the heart of deep learning, forming the basis of fully connected layers and convolutions. When matrices are large, the way we access their elements in memory can have a huge impact on performance. Modern CPUs have a memory hierarchy (caches) where accessing data that is already in a cache is much faster than fetching it from main memory. A naive matrix multiplication algorithm can have poor cache locality, leading to many cache misses and slow performance. By using a cache-aware technique like tiling (or blocking), we can significantly speed up the operation.\n",
        "\n",
        "### Key Concepts\n",
        "- **Cache Locality**: The principle that if a memory location is accessed, it's likely that nearby memory locations will be accessed soon (spatial locality) and the same location will be accessed again soon (temporal locality).\n",
        "- **Tiling/Blocking**: A technique to improve cache locality by breaking down a large matrix multiplication into smaller matrix multiplications on sub-matrices (tiles) that can fit into the cache.\n",
        "- **Numba**: A just-in-time (JIT) compiler for Python that can translate a subset of Python and NumPy code into fast machine code, often achieving C-like speeds.\n",
        "\n",
        "### Problem Statement\n",
        "Implement matrix multiplication using a standard naive approach and a cache-optimized tiled approach. Compare their performance against each other and against a Numba-optimized version.\n",
        "\n",
        "**Requirements**:\n",
        "- Implement a standard (i, j, k) matrix multiplication.\n",
        "- Implement a tiled version of matrix multiplication.\n",
        "- Use Numba's `@jit` decorator to create a JIT-compiled version.\n",
        "- Benchmark the three versions for different matrix sizes.\n",
        "\n",
        "### Example: Naive vs. Tiled Matrix Multiplication\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Small example to understand the concept\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "# NAIVE APPROACH: Three nested loops (i, j, k)\n",
        "# Access pattern: A[i,:] and B[:,j] in each iteration\n",
        "# This causes many cache misses for large matrices\n",
        "def matrix_mult_naive_small(A, B):\n",
        "    m, k = A.shape\n",
        "    k2, n = B.shape\n",
        "    C = np.zeros((m, n))\n",
        "    for i in range(m):\n",
        "        for j in range(n):\n",
        "            for l in range(k):\n",
        "                C[i, j] += A[i, l] * B[l, j]\n",
        "    return C\n",
        "\n",
        "result_naive = matrix_mult_naive_small(A, B)\n",
        "# Result: [[19, 22], [43, 50]]\n",
        "\n",
        "# TILED APPROACH: Break into smaller blocks\n",
        "# Process 2x2 tiles that fit in cache\n",
        "# Same result, but better cache utilization for larger matrices\n",
        "def matrix_mult_tiled_small(A, B, tile_size=1):\n",
        "    m, k = A.shape\n",
        "    k2, n = B.shape\n",
        "    C = np.zeros((m, n))\n",
        "    \n",
        "    # Process in tile_size x tile_size blocks\n",
        "    for i in range(0, m, tile_size):\n",
        "        for j in range(0, n, tile_size):\n",
        "            for l in range(0, k, tile_size):\n",
        "                # Process the tile\n",
        "                for i2 in range(i, min(i + tile_size, m)):\n",
        "                    for j2 in range(j, min(j + tile_size, n)):\n",
        "                        for l2 in range(l, min(l + tile_size, k)):\n",
        "                            C[i2, j2] += A[i2, l2] * B[l2, j2]\n",
        "    return C\n",
        "\n",
        "result_tiled = matrix_mult_tiled_small(A, B, tile_size=2)\n",
        "# Result: [[19, 22], [43, 50]]\n",
        "\n",
        "print(f\"Naive result matches tiled: {np.allclose(result_naive, result_tiled)}\")\n",
        "# Output: True\n",
        "\n",
        "# For large matrices (512x512+), the tiled version can be 2-5x faster!\n",
        "```\n",
        "\n",
        "### Your Exercise\n",
        "Implement all three approaches and benchmark them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def matrix_multiply_naive(A, B):\n",
        "    \"\"\"\n",
        "    Standard matrix multiplication using nested loops.\n",
        "    \n",
        "    TODO: Implement this function\n",
        "    - Check that inner dimensions match\n",
        "    - Create a result matrix of zeros (m x n where A is m x k, B is k x n)\n",
        "    - Use three nested loops (i, j, l) to compute the multiplication\n",
        "    - Return the result\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def matrix_multiply_tiled(A, B, tile_size=64):\n",
        "    \"\"\"\n",
        "    Cache-optimized matrix multiplication using tiling.\n",
        "    \n",
        "    TODO: Implement this function (optional, more advanced)\n",
        "    - Break matrices into tile_size x tile_size blocks\n",
        "    - Compute matrix multiplication on blocks\n",
        "    - This improves cache locality\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def benchmark_matrix_multiply(sizes=[64, 128, 256, 512]):\n",
        "    \"\"\"\n",
        "    Benchmark different matrix multiplication approaches.\n",
        "    \n",
        "    TODO: Implement this function\n",
        "    - For each size in sizes:\n",
        "      - Create random square matrices A, B of size x size\n",
        "      - Time matrix_multiply_naive(A, B)\n",
        "      - Store the timing results\n",
        "    - Print or plot the results\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for size in sizes:\n",
        "        # TODO: Implement benchmarking\n",
        "        pass\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test your implementation\n",
        "try:\n",
        "    result = benchmark_matrix_multiply(sizes=[64, 128])\n",
        "    print(\"✓ Matrix multiplication benchmarking complete!\")\n",
        "    print(f\"Results: {result}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Make sure the functions are implemented.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Click to reveal hint for Problem 3</summary>\n",
        "\n",
        "**Hint**: For the tiled version, you will need three nested loops to iterate over the tiles, and then three more nested loops to perform the multiplication within each tile. The key is that the inner loops will be operating on small sub-matrices that fit in the cache. For the Numba version, simply apply the `@jit(nopython=True)` decorator to your naive implementation.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4: Custom Sparse Tensor Operations (Medium-Hard)\n",
        "\n",
        "### Contextual Introduction\n",
        "In many ML applications, such as natural language processing (e.g., word embeddings) and recommendation systems, we deal with very high-dimensional but sparse data. For example, a user-item interaction matrix in a recommendation system might have millions of rows and columns, but each user has only interacted with a few items. Storing this as a dense matrix would be prohibitively expensive in terms of memory. Sparse tensors are data structures that only store the non-zero elements, making them much more memory-efficient.\n",
        "\n",
        "### Key Concepts\n",
        "- **Sparsity**: The fraction of zero elements in a tensor.\n",
        "- **COO (Coordinate) format**: A way to represent a sparse tensor by storing a list of (row, column, value) tuples.\n",
        "- **Memory Efficiency**: Sparse tensors have a memory usage of O(nnz), where nnz is the number of non-zero elements, as opposed to O(N*M) for a dense matrix.\n",
        "\n",
        "### Problem Statement\n",
        "Implement a memory-efficient sparse tensor class using the COO format. The class should support conversion to and from a dense NumPy array, as well as a `memory_usage` method to demonstrate its efficiency.\n",
        "\n",
        "### Example: Dense vs Sparse Representation\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Imagine a user-item interaction matrix (e.g., Netflix recommendations)\n",
        "# 1 = user watched movie, 0 = user hasn't watched\n",
        "\n",
        "# DENSE representation (wasteful!)\n",
        "dense_matrix = np.array([\n",
        "    [1, 0, 0, 0, 2, 0, 0, 0],  # User 1: watched item 0 (score 1), item 4 (score 2)\n",
        "    [0, 0, 0, 3, 0, 0, 0, 0],  # User 2: watched item 3 (score 3)\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],  # User 3: watched nothing\n",
        "    [0, 4, 0, 0, 0, 0, 5, 0],  # User 4: watched items 1, 6 (scores 4, 5)\n",
        "])\n",
        "# This takes 4 * 8 = 32 elements, but only 5 are non-zero!\n",
        "# Memory waste for 10,000 users × 100,000 items: 1 billion elements!\n",
        "\n",
        "# SPARSE representation (efficient!)\n",
        "# Store only (row, column, value) tuples for non-zero elements\n",
        "sparse_data = {\n",
        "    'indices': [(0, 0, 1), (0, 4, 2), (1, 3, 3), (3, 1, 4), (3, 6, 5)],\n",
        "    'shape': (4, 8)\n",
        "}\n",
        "# Same information, only 5 tuples stored!\n",
        "# Memory scales with number of non-zeros, not total elements\n",
        "\n",
        "# Your task: Create a SparseTensor class that stores this efficiently\n",
        "# and can convert back to dense when needed\n",
        "\n",
        "# Example usage (what your code should do):\n",
        "# sparse = SparseTensor((4, 8))\n",
        "# sparse.from_dense(dense_matrix)  # Store efficiently\n",
        "# \n",
        "# dense_recovered = sparse.to_dense()  # Convert back\n",
        "# assert np.allclose(dense_recovered, dense_matrix)  # Verify correctness\n",
        "# \n",
        "# print(sparse.sparsity())  # Output: 0.84375 (87.5% zeros)\n",
        "# print(sparse.memory_usage())  # Show memory savings\n",
        "```\n",
        "\n",
        "### Your Exercise\n",
        "Implement the SparseTensor class:\n",
        "\n",
        "**Requirements**:\n",
        "- Use COO format (indices and values) to store non-zero elements.\n",
        "- Implement `to_dense` and `from_dense` methods.\n",
        "- Implement a `memory_usage` method that returns the memory used by the sparse representation.\n",
        "- Compare the memory usage of the sparse tensor with its dense equivalent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SparseTensor:\n",
        "    \"\"\"\n",
        "    A simple sparse tensor representation using a dictionary.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, shape, data_dict=None):\n",
        "        \"\"\"\n",
        "        Initialize sparse tensor.\n",
        "        \n",
        "        TODO: Initialize shape, indices, and values\n",
        "        - Store shape as a tuple\n",
        "        - Initialize empty storage for non-zero elements\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def to_dense(self):\n",
        "        \"\"\"\n",
        "        Convert sparse tensor to dense array.\n",
        "        \n",
        "        TODO: Implement this function\n",
        "        - Create a dense numpy array of zeros with self.shape\n",
        "        - Fill in the non-zero values at their correct positions\n",
        "        - Return the dense array\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def from_dense(self, dense_array, threshold=1e-10):\n",
        "        \"\"\"\n",
        "        Convert dense array to sparse tensor format.\n",
        "        \n",
        "        TODO: Implement this function\n",
        "        - Find all non-zero elements (use threshold for floating point)\n",
        "        - Store their indices and values\n",
        "        - Return self for chaining\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "def test_sparse_tensor():\n",
        "    \"\"\"Test sparse tensor implementation.\"\"\"\n",
        "    # Create a simple sparse tensor\n",
        "    shape = (5, 5)\n",
        "    sparse = SparseTensor(shape)\n",
        "    \n",
        "    # Create a dense array with some zeros\n",
        "    dense_array = np.array([\n",
        "        [1, 0, 0, 2, 0],\n",
        "        [0, 3, 0, 0, 0],\n",
        "        [0, 0, 4, 0, 5],\n",
        "        [0, 0, 0, 0, 0],\n",
        "        [6, 0, 0, 0, 7]\n",
        "    ], dtype=float)\n",
        "    \n",
        "    # TODO: Convert dense to sparse and back\n",
        "    # Convert to sparse, then to dense, and verify they match\n",
        "    \n",
        "    print(\"✓ Sparse tensor tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "try:\n",
        "    test_sparse_tensor()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Click to reveal hint for Problem 4</summary>\n",
        "\n",
        "**Hint**: For `from_dense`, use `np.argwhere` to find the indices of non-zero elements. For `to_dense`, create a zero-filled NumPy array of the correct shape and then fill in the non-zero values using the stored indices and values. The memory usage is the sum of the bytes used by the indices array and the values array, which you can get from the `.nbytes` attribute of a NumPy array.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 5: Memory Profiling and Optimization (Hard)\n",
        "\n",
        "### Contextual Introduction\n",
        "Training large neural networks, like the ones used for language modeling or image generation, is often limited by GPU memory. During the forward pass, the activations (outputs of each layer) are stored in memory to be used for gradient calculations in the backward pass. For very deep or wide networks, these activations can consume a huge amount of memory. Gradient checkpointing is a technique that trades compute for memory by not storing the activations for some layers and instead recomputing them during the backward pass. This can allow you to train much larger models than would otherwise fit in memory.\n",
        "\n",
        "### Key Concepts\n",
        "- **Memory Profiling**: The process of measuring and analyzing the memory usage of a program.\n",
        "- **Gradient Checkpointing**: A technique to reduce the memory footprint of a neural network by recomputing activations during the backward pass instead of storing them.\n",
        "- **Activation**: The output of a layer in a neural network.\n",
        "\n",
        "### Problem Statement\n",
        "Implement a simple memory profiler for PyTorch and use it to compare the memory usage of a neural network forward pass with and without gradient checkpointing.\n",
        "\n",
        "**Requirements**:\n",
        "- Create a `MemoryProfiler` class that can track peak GPU memory usage.\n",
        "- Implement a simple multi-layer neural network.\n",
        "- Implement a version of the forward pass that uses `torch.utils.checkpoint.checkpoint`.\n",
        "- Profile the memory usage of both forward passes\n",
        "\n",
        "### Example: Memory Profiling Concept\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Understanding memory usage in neural networks:\n",
        "\n",
        "# WITHOUT CHECKPOINTING:\n",
        "# Forward pass stores ALL intermediate activations\n",
        "# Layer 1 -> activation 1 (stored in memory)\n",
        "# Layer 2 -> activation 2 (stored in memory)\n",
        "# Layer 3 -> activation 3 (stored in memory)\n",
        "# ...\n",
        "# Layer 100 -> activation 100 (stored in memory)\n",
        "# Total memory: sum of all 100 activations\n",
        "\n",
        "# For a batch of 32 samples with hidden size 1000:\n",
        "# Each activation ≈ 32 * 1000 * 4 bytes = 128KB\n",
        "# 100 layers * 128KB = 12.8MB (this adds up!)\n",
        "\n",
        "# BACKWARD PASS uses all stored activations to compute gradients\n",
        "# gradients = compute_gradients(activation1, activation2, ...)\n",
        "\n",
        "# WITH CHECKPOINTING:\n",
        "# Forward pass stores ONLY checkpointed layers\n",
        "# Layer 1 -> activation 1 (stored)\n",
        "# Layer 2-10 -> activations NOT stored (will be recomputed)\n",
        "# Layer 11 -> activation 11 (stored)\n",
        "# ...\n",
        "# Layer 91-100 -> activations NOT stored (will be recomputed)\n",
        "\n",
        "# Backward pass recomputes activations as needed:\n",
        "# To compute gradients for layer 9:\n",
        "#   1. Recompute activation 9 (fast, only through layers 1-9)\n",
        "#   2. Use it to compute gradient\n",
        "#   3. Discard activation 9\n",
        "#   4. Move to next layer\n",
        "\n",
        "# Result: Memory reduced from 12.8MB to ~1.3MB!\n",
        "# Trade-off: Backward pass is slightly slower (recomputation overhead)\n",
        "\n",
        "# Your task:\n",
        "# 1. Create a MemoryProfiler to measure memory before/after operations\n",
        "# 2. Create a network and measure memory DURING training\n",
        "# 3. Show that checkpointing reduces peak memory usage\n",
        "```\n",
        "\n",
        "### Your Exercise\n",
        "Implement memory profiling and gradient checkpointing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tracemalloc\n",
        "\n",
        "class MemoryProfiler:\n",
        "    \"\"\"Simple memory profiler.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the profiler.\"\"\"\n",
        "        self.memory_snapshots = []\n",
        "    \n",
        "    def get_memory_usage(self):\n",
        "        \"\"\"\n",
        "        Get current memory usage in MB.\n",
        "        \n",
        "        TODO: Implement this function\n",
        "        - Use psutil or torch.cuda.memory_allocated() for GPU\n",
        "        - Return memory in MB\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def log_memory(self, label):\n",
        "        \"\"\"\n",
        "        Log current memory usage with a label.\n",
        "        \n",
        "        TODO: Implement this function\n",
        "        - Call get_memory_usage()\n",
        "        - Store the result with the label\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class SimpleNetwork(nn.Module):\n",
        "    \"\"\"Simple neural network for testing.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize the network.\n",
        "        \n",
        "        TODO: Implement this function\n",
        "        - Create linear layers: input_size -> hidden_size -> output_size\n",
        "        - Store them as self.fc1 and self.fc2\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        TODO: Implement this function\n",
        "        - Pass through fc1\n",
        "        - Apply ReLU activation\n",
        "        - Pass through fc2\n",
        "        - Return result\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "def profile_network_memory():\n",
        "    \"\"\"Profile memory usage of a neural network.\"\"\"\n",
        "    profiler = MemoryProfiler()\n",
        "    \n",
        "    # TODO: Implement profiling\n",
        "    # - Create a network\n",
        "    # - Create input batch\n",
        "    # - Log memory before forward pass\n",
        "    # - Run forward pass\n",
        "    # - Log memory after forward pass\n",
        "    # - Create output and backward\n",
        "    # - Log memory after backward\n",
        "    \n",
        "    print(\"✓ Memory profiling complete!\")\n",
        "\n",
        "# Run the profiler\n",
        "try:\n",
        "    profile_network_memory()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Click to reveal hint for Problem 5</summary>\n",
        "\n",
        "**Hint**: Use `torch.cuda.reset_peak_memory_stats()` before your forward pass and `torch.cuda.max_memory_allocated()` after to get the peak memory usage. For the checkpointed forward pass, wrap the application of each layer (or a sequence of layers) in the `torch.utils.checkpoint.checkpoint` function. This function takes a function to be run (e.g., a lambda that applies the layer) and the input to that function.\n",
        "\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
