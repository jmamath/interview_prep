{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Streaming & Online Algorithms\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jmamath/interview_prep/blob/main/chapter_03_streaming_online_algorithms.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the real world of machine learning, data rarely arrives all at once in a neat package. Whether you're analyzing Twitter streams, processing IoT sensor data, or building recommendation systems that adapt to user behavior in real-time, you need algorithms that can handle massive, unbounded data streams.\n",
    "\n",
    "This chapter introduces you to streaming and online algorithms—techniques that process data in a single pass without storing everything in memory. These algorithms are fundamental to modern ML systems, from real-time analytics pipelines to incremental model training.\n",
    "\n",
    "You'll learn how to estimate frequencies with minimal memory, count unique items without storing them all, compute statistics on-the-fly, sample from infinite streams, and train models that learn incrementally as new data arrives.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement space-efficient frequency estimation with Count-Min Sketch\n",
    "- Estimate cardinality with HyperLogLog\n",
    "- Compute mean and variance in one pass using Welford's algorithm\n",
    "- Sample uniformly from streams with Reservoir Sampling\n",
    "- Train models incrementally with Online Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Frequency Estimation with Count-Min Sketch (Easy-Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "Imagine you're analyzing search queries at Google. Billions of queries per day—you can't store them all. But you need to answer: \"How many times was 'machine learning' searched?\" Count-Min Sketch is a probabilistic data structure that estimates frequencies using constant memory, trading perfect accuracy for incredible space efficiency.\n",
    "\n",
    "### Key Concepts\n",
    "- **Probabilistic Data Structure**: Trades accuracy for space using randomness\n",
    "- **Hash Functions**: Multiple independent hashes provide redundancy\n",
    "- **Space-Time Tradeoff**: O(w*d) space, O(1) update and query time\n",
    "\n",
    "### Problem Statement\n",
    "Implement a Count-Min Sketch that estimates item frequencies in a data stream.\n",
    "\n",
    "**Requirements**:\n",
    "- Initialize with width and depth parameters\n",
    "- Implement update() to process items\n",
    "- Implement query() to estimate frequency\n",
    "- Handle hash collisions gracefully\n",
    "\n",
    "### Example: How Count-Min Sketch Works\n",
    "\n",
    "```python\n",
    "# Simplified example with width=4, depth=2\n",
    "# Imagine we have 2 hash functions and 4 buckets per row\n",
    "\n",
    "# Initial sketch (all zeros):\n",
    "# Row 0: [0, 0, 0, 0]\n",
    "# Row 1: [0, 0, 0, 0]\n",
    "\n",
    "# Add \"apple\" (hash1(\"apple\") % 4 = 2, hash2(\"apple\") % 4 = 1)\n",
    "# Row 0: [0, 0, 1, 0]  <- Incremented position 2\n",
    "# Row 1: [0, 1, 0, 0]  <- Incremented position 1\n",
    "\n",
    "# Add \"banana\" (hash1(\"banana\") % 4 = 1, hash2(\"banana\") % 4 = 3)\n",
    "# Row 0: [0, 1, 1, 0]  <- Incremented position 1\n",
    "# Row 1: [0, 1, 0, 1]  <- Incremented position 3\n",
    "\n",
    "# Add \"apple\" again\n",
    "# Row 0: [0, 1, 2, 0]  <- Incremented position 2 (now 2)\n",
    "# Row 1: [0, 2, 0, 1]  <- Incremented position 1 (now 2)\n",
    "\n",
    "# Query \"apple\": min(sketch[0,2], sketch[1,1]) = min(2, 2) = 2 ✓\n",
    "# Query \"banana\": min(sketch[0,1], sketch[1,3]) = min(1, 1) = 1 ✓\n",
    "\n",
    "# Why minimum? If there's a collision, one counter might be higher.\n",
    "# The minimum gives the best estimate!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "class CountMinSketch:\n",
    "    def __init__(self, width: int = 1000, depth: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize Count-Min Sketch.\n",
    "        \n",
    "        Args:\n",
    "            width: Number of columns (hash table size)\n",
    "            depth: Number of rows (hash functions)\n",
    "        \"\"\"\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.sketch = np.zeros((depth, width), dtype=int)\n",
    "    \n",
    "    def _hash(self, item: str, seed: int) -> int:\n",
    "        \"\"\"Hash an item to a column index.\"\"\"\n",
    "        h = hashlib.md5((str(item) + str(seed)).encode()).hexdigest()\n",
    "        return int(h, 16) % self.width\n",
    "    \n",
    "    def update(self, item: str, count: int = 1):\n",
    "        # TODO: For each row i from 0 to depth:\n",
    "        #   - Compute hash index using _hash(item, i)\n",
    "        #   - Increment sketch[i, index] by count\n",
    "        pass\n",
    "    \n",
    "    def query(self, item: str) -> int:\n",
    "        # TODO: Return the minimum count across all rows\n",
    "        # This gives the best estimate of the item's frequency\n",
    "        pass\n",
    "\n",
    "def test_count_min_sketch():\n",
    "    \"\"\"Test Count-Min Sketch implementation.\"\"\"\n",
    "    sketch = CountMinSketch(width=100, depth=5)\n",
    "    stream = ['apple', 'banana', 'apple', 'orange', 'apple', 'banana']\n",
    "    for item in stream:\n",
    "        sketch.update(item)\n",
    "    \n",
    "    # Test frequencies (using >= since it's an estimate)\n",
    "    assert sketch.query('apple') >= 3, \"Apple should appear at least 3 times\"\n",
    "    assert sketch.query('banana') >= 2, \"Banana should appear at least 2 times\"\n",
    "    assert sketch.query('orange') >= 1, \"Orange should appear at least 1 time\"\n",
    "    \n",
    "    print(\"All Count-Min Sketch tests passed!\")\n",
    "\n",
    "# Uncomment to test\n",
    "# test_count_min_sketch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 1</summary>\n",
    "\n",
    "**Hint**: For update(), loop through each row (0 to depth-1), hash the item with that row index as seed, and increment the cell. For query(), hash the item with each row index, get the count from each row, and return the minimum.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Cardinality Estimation with HyperLogLog (Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "Facebook needs to count unique daily active users across billions of events. Storing every user ID would require terabytes. HyperLogLog estimates cardinality (number of unique items) using only kilobytes of memory, with typical error rates under 2%. It's used everywhere from database query optimizers to real-time analytics dashboards.\n",
    "\n",
    "### Key Concepts\n",
    "- **Cardinality**: The number of unique elements in a set\n",
    "- **Probabilistic Counting**: Uses hash function properties to estimate cardinality\n",
    "- **Harmonic Mean**: Combines estimates from multiple buckets for accuracy\n",
    "\n",
    "### Problem Statement\n",
    "Implement HyperLogLog to estimate the number of unique items in a stream with minimal memory.\n",
    "\n",
    "**Requirements**:\n",
    "- Initialize with precision parameter (number of buckets)\n",
    "- Implement add() to process items\n",
    "- Implement count() to estimate cardinality\n",
    "- Achieve reasonable accuracy (within 10% error)\n",
    "\n",
    "### Example: HyperLogLog Intuition\n",
    "\n",
    "```python\n",
    "# Concept: If you flip a coin until you get heads,\n",
    "# the number of flips tells you about how many sequences you've seen\n",
    "\n",
    "# Example with precision=2 (4 buckets: 00, 01, 10, 11)\n",
    "# Hash \"user_1\" → binary: 10110101...\n",
    "# First 2 bits (10) = bucket 2\n",
    "# Remaining bits: count leading zeros + 1 = 3 (00110101... has 2 leading zeros)\n",
    "# Register[2] = max(Register[2], 3) = 3\n",
    "\n",
    "# Hash \"user_2\" → binary: 10010111...\n",
    "# First 2 bits (10) = bucket 2 (same bucket!)\n",
    "# Leading zeros + 1 = 2\n",
    "# Register[2] = max(3, 2) = 3 (keep maximum)\n",
    "\n",
    "# Hash \"user_3\" → binary: 01111001...\n",
    "# First 2 bits (01) = bucket 1\n",
    "# Leading zeros + 1 = 1\n",
    "# Register[1] = 1\n",
    "\n",
    "# Registers: [0, 1, 3, 0]\n",
    "\n",
    "# Estimate cardinality:\n",
    "# If we saw a run of 3 flips, we probably saw ~2^3 = 8 unique items\n",
    "# But we have 4 buckets, so we use harmonic mean to combine estimates\n",
    "# This gives a more accurate estimate\n",
    "\n",
    "# Actual formula: alpha * m^2 / sum(2^(-register[i]))\n",
    "# where m = number of buckets, alpha = bias correction\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "class HyperLogLog:\n",
    "    def __init__(self, precision: int = 14):\n",
    "        self.precision = precision\n",
    "        self.m = 1 << precision  # 2^precision buckets\n",
    "        self.registers = np.zeros(self.m, dtype=int)\n",
    "    \n",
    "    def _hash(self, item: str) -> int:\n",
    "        return int(hashlib.md5(str(item).encode()).hexdigest(), 16)\n",
    "    \n",
    "    def add(self, item: str):\n",
    "        # TODO: Hash the item, split into bucket index and remaining bits\n",
    "        # Count leading zeros in remaining bits + 1\n",
    "        # Update register[bucket] = max(register[bucket], leading_zeros)\n",
    "        pass\n",
    "    \n",
    "    def count(self) -> float:\n",
    "        # TODO: Compute harmonic mean of 2^registers\n",
    "        # Return estimated cardinality\n",
    "        pass\n",
    "\n",
    "def test_hyperloglog():\n",
    "    hll = HyperLogLog(precision=10)\n",
    "    \n",
    "    # Add unique items\n",
    "    for i in range(10000):\n",
    "        hll.add(f'user_{i}')\n",
    "    \n",
    "    estimate = hll.count()\n",
    "    error = abs(estimate - 10000) / 10000\n",
    "    \n",
    "    print(f'Actual: 10000, Estimated: {estimate:.0f}, Error: {error:.2%}')\n",
    "    assert error < 0.1, f'Error {error:.2%} exceeds 10%'\n",
    "    print('All HyperLogLog tests passed!')\n",
    "\n",
    "# Uncomment to test\n",
    "# test_hyperloglog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 2</summary>\n",
    "\n",
    "**Hint**: Hash the item to get a 64-bit integer. Use the first `precision` bits to determine the bucket index. Count the leading zeros in the remaining bits (plus 1). Update the bucket's register to the maximum value seen. To estimate cardinality, use the formula: `alpha * m^2 / sum(2^(-register))` where alpha is a bias correction constant.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Online Mean and Variance Computation (Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "Training neural networks requires computing statistics over mini-batches. Computing mean and variance naively can cause numerical instability. Welford's algorithm computes these statistics in one pass with numerical stability, used in BatchNorm layers and real-time monitoring systems.\n",
    "\n",
    "### Key Concepts\n",
    "- **Numerical Stability**: Avoiding catastrophic cancellation in floating-point arithmetic\n",
    "- **Online Algorithm**: Updates statistics incrementally\n",
    "- **Welford's Method**: Uses running mean to compute variance\n",
    "\n",
    "### Problem Statement\n",
    "Implement online mean and variance computation using Welford's algorithm.\n",
    "\n",
    "**Requirements**:\n",
    "- Maintain count, mean, and M2 (sum of squared differences)\n",
    "- Implement update() to add new values\n",
    "- Implement mean() and variance() methods\n",
    "- Handle numerical stability\n",
    "\n",
    "### Example: Welford's Algorithm Step-by-Step\n",
    "\n",
    "```python\n",
    "# Stream: [4, 7, 13, 16]\n",
    "# Let's compute mean and variance online\n",
    "\n",
    "# Initial state:\n",
    "# n=0, mean=0, M2=0\n",
    "\n",
    "# Add 4:\n",
    "# n = 1\n",
    "# delta = 4 - 0 = 4\n",
    "# mean = 0 + 4/1 = 4\n",
    "# delta2 = 4 - 4 = 0\n",
    "# M2 = 0 + 4*0 = 0\n",
    "\n",
    "# Add 7:\n",
    "# n = 2\n",
    "# delta = 7 - 4 = 3\n",
    "# mean = 4 + 3/2 = 5.5\n",
    "# delta2 = 7 - 5.5 = 1.5\n",
    "# M2 = 0 + 3*1.5 = 4.5\n",
    "\n",
    "# Add 13:\n",
    "# n = 3\n",
    "# delta = 13 - 5.5 = 7.5\n",
    "# mean = 5.5 + 7.5/3 = 8\n",
    "# delta2 = 13 - 8 = 5\n",
    "# M2 = 4.5 + 7.5*5 = 42\n",
    "\n",
    "# Add 16:\n",
    "# n = 4\n",
    "# delta = 16 - 8 = 8\n",
    "# mean = 8 + 8/4 = 10\n",
    "# delta2 = 16 - 10 = 6\n",
    "# M2 = 42 + 8*6 = 90\n",
    "\n",
    "# Final: mean = 10, variance = M2/(n-1) = 90/3 = 30 ✓\n",
    "\n",
    "# Why stable? We never compute (x - mean)^2 directly for large sums,\n",
    "# which can lose precision with floating-point arithmetic!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineStats:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self._mean = 0.0\n",
    "        self.M2 = 0.0\n",
    "    \n",
    "    def update(self, value: float):\n",
    "        # TODO: Implement Welford's algorithm\n",
    "        # Update count, mean, and M2\n",
    "        pass\n",
    "    \n",
    "    def mean(self) -> float:\n",
    "        return self._mean\n",
    "    \n",
    "    def variance(self) -> float:\n",
    "        # TODO: Return M2 / n for population variance\n",
    "        # Or M2 / (n-1) for sample variance\n",
    "        pass\n",
    "\n",
    "def test_online_stats():\n",
    "    stats = OnlineStats()\n",
    "    data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    \n",
    "    for x in data:\n",
    "        stats.update(x)\n",
    "    \n",
    "    expected_mean = 5.5\n",
    "    expected_var = 8.25  # Sample variance\n",
    "    \n",
    "    assert abs(stats.mean() - expected_mean) < 0.01\n",
    "    assert abs(stats.variance() - expected_var) < 0.01\n",
    "    print('All Online Stats tests passed!')\n",
    "\n",
    "# Uncomment to test\n",
    "# test_online_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 3</summary>\n",
    "\n",
    "**Hint**: Welford's algorithm: `delta = value - mean`, `mean += delta / n`, `delta2 = value - mean`, `M2 += delta * delta2`. Variance is `M2 / (n-1)` for sample variance.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Reservoir Sampling (Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "You're building a recommendation system that needs to sample from user activity streams. The stream is unbounded—you don't know when it ends. Reservoir sampling maintains a uniform random sample of k items from a stream of unknown length, ensuring each item has equal probability of being selected.\n",
    "\n",
    "### Key Concepts\n",
    "- **Uniform Random Sampling**: Each item has equal probability\n",
    "- **Single-Pass Algorithm**: Process stream only once\n",
    "- **Constant Memory**: Uses O(k) space regardless of stream length\n",
    "\n",
    "### Problem Statement\n",
    "Implement reservoir sampling to maintain k random samples from a stream.\n",
    "\n",
    "**Requirements**:\n",
    "- Initialize with sample size k\n",
    "- Implement add() to process stream items\n",
    "- Implement get_sample() to return current reservoir\n",
    "- Ensure uniform distribution\n",
    "\n",
    "### Example: Reservoir Sampling in Action\n",
    "\n",
    "```python\n",
    "# Goal: Keep k=3 random samples from a stream\n",
    "# Stream: [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# Step 1-3: Fill reservoir with first 3 items\n",
    "# Reservoir: [1, 2, 3], n=3\n",
    "\n",
    "# Step 4: Add item 4\n",
    "# n=4, generate random j in [0, 4) → say j=2\n",
    "# Since j < k (2 < 3), replace reservoir[2] = 4\n",
    "# Reservoir: [1, 2, 4], n=4\n",
    "\n",
    "# Step 5: Add item 5\n",
    "# n=5, generate random j in [0, 5) → say j=4\n",
    "# Since j >= k (4 >= 3), don't replace anything\n",
    "# Reservoir: [1, 2, 4], n=5\n",
    "\n",
    "# Step 6: Add item 6\n",
    "# n=6, generate random j in [0, 6) → say j=1\n",
    "# Since j < k (1 < 3), replace reservoir[1] = 6\n",
    "# Reservoir: [1, 6, 4], n=6\n",
    "\n",
    "# Step 7: Add item 7\n",
    "# n=7, generate random j in [0, 7) → say j=5\n",
    "# Since j >= k (5 >= 3), don't replace\n",
    "# Reservoir: [1, 6, 4], n=7\n",
    "\n",
    "# Step 8: Add item 8\n",
    "# n=8, generate random j in [0, 8) → say j=0\n",
    "# Since j < k (0 < 3), replace reservoir[0] = 8\n",
    "# Reservoir: [8, 6, 4], n=8\n",
    "\n",
    "# Each item had exactly 3/8 probability of being in final sample!\n",
    "# Item i had probability: k/i * (i/(i+1)) * ((i+1)/(i+2)) * ... * ((n-1)/n) = k/n\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Any\n",
    "\n",
    "class ReservoirSampler:\n",
    "    def __init__(self, k: int):\n",
    "        self.k = k\n",
    "        self.reservoir = []\n",
    "        self.n = 0\n",
    "    \n",
    "    def add(self, item: Any):\n",
    "        # TODO: Implement reservoir sampling algorithm\n",
    "        # If reservoir not full, add item\n",
    "        # Otherwise, randomly replace with probability k/n\n",
    "        pass\n",
    "    \n",
    "    def get_sample(self) -> List[Any]:\n",
    "        return self.reservoir.copy()\n",
    "\n",
    "def test_reservoir_sampler():\n",
    "    random.seed(42)\n",
    "    sampler = ReservoirSampler(k=10)\n",
    "    \n",
    "    # Add 1000 items\n",
    "    for i in range(1000):\n",
    "        sampler.add(i)\n",
    "    \n",
    "    sample = sampler.get_sample()\n",
    "    assert len(sample) == 10\n",
    "    assert all(0 <= x < 1000 for x in sample)\n",
    "    print('All Reservoir Sampler tests passed!')\n",
    "\n",
    "# Uncomment to test\n",
    "# test_reservoir_sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 4</summary>\n",
    "\n",
    "**Hint**: Fill reservoir with first k items. For item i > k, generate random number j in [0, i). If j < k, replace reservoir[j] with item i.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Online Gradient Descent (Hard)\n",
    "\n",
    "### Contextual Introduction\n",
    "Modern ML systems need to adapt to new data continuously. Online gradient descent updates model parameters as new samples arrive, enabling real-time learning. It's used in online advertising, recommendation systems, and any application where the data distribution shifts over time.\n",
    "\n",
    "### Key Concepts\n",
    "- **Stochastic Gradient Descent**: Updates using single samples or mini-batches\n",
    "- **Learning Rate Schedule**: Decreases learning rate over time for convergence\n",
    "- **Regret**: Measures performance vs. optimal offline algorithm\n",
    "\n",
    "### Problem Statement\n",
    "Implement online gradient descent for linear regression that learns incrementally.\n",
    "\n",
    "**Requirements**:\n",
    "- Initialize weights and learning rate\n",
    "- Implement partial_fit() for incremental updates\n",
    "- Implement predict() method\n",
    "- Use decreasing learning rate\n",
    "\n",
    "### Example: Online Learning Step-by-Step\n",
    "\n",
    "```python\n",
    "# Problem: Learn y = 2x1 + 3x2 from streaming data\n",
    "# Initial: weights = [0, 0], bias = 0, lr = 0.1\n",
    "\n",
    "# Sample 1: x = [1, 1], y_true = 5\n",
    "# Prediction: y_pred = [1, 1] @ [0, 0] + 0 = 0\n",
    "# Error: error = 0 - 5 = -5\n",
    "# Gradients: grad_w = [1, 1] * (-5) = [-5, -5]\n",
    "#            grad_b = -5\n",
    "# Update: weights = [0, 0] - 0.1 * [-5, -5] = [0.5, 0.5]\n",
    "#         bias = 0 - 0.1 * (-5) = 0.5\n",
    "# t = 1, next_lr = 0.1 / sqrt(2) ≈ 0.07\n",
    "\n",
    "# Sample 2: x = [2, 0], y_true = 4\n",
    "# Prediction: y_pred = [2, 0] @ [0.5, 0.5] + 0.5 = 1.5\n",
    "# Error: error = 1.5 - 4 = -2.5\n",
    "# Gradients: grad_w = [2, 0] * (-2.5) = [-5, 0]\n",
    "#            grad_b = -2.5\n",
    "# Update: weights = [0.5, 0.5] - 0.07 * [-5, 0] = [0.85, 0.5]\n",
    "#         bias = 0.5 - 0.07 * (-2.5) = 0.675\n",
    "# t = 2, next_lr = 0.1 / sqrt(3) ≈ 0.058\n",
    "\n",
    "# ... continue for more samples ...\n",
    "# Weights gradually converge to [2, 3]!\n",
    "\n",
    "# Why decreasing learning rate?\n",
    "# Early: Large steps to get close to optimum quickly\n",
    "# Later: Small steps to fine-tune and converge\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OnlineLinearRegression:\n",
    "    def __init__(self, n_features: int, learning_rate: float = 0.01):\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "        self.lr = learning_rate\n",
    "        self.t = 0\n",
    "    \n",
    "    def partial_fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        # TODO: Compute prediction and error\n",
    "        # Update weights using gradient descent\n",
    "        # Use learning rate schedule: lr / sqrt(t+1)\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # TODO: Return X @ weights + bias\n",
    "        pass\n",
    "\n",
    "def test_online_gradient_descent():\n",
    "    # Generate simple linear data\n",
    "    np.random.seed(42)\n",
    "    X_train = np.random.randn(1000, 5)\n",
    "    true_weights = np.array([1, 2, 3, 4, 5])\n",
    "    y_train = X_train @ true_weights + np.random.randn(1000) * 0.1\n",
    "    \n",
    "    model = OnlineLinearRegression(n_features=5)\n",
    "    \n",
    "    # Train online\n",
    "    for X, y in zip(X_train, y_train):\n",
    "        model.partial_fit(X.reshape(1, -1), np.array([y]))\n",
    "    \n",
    "    # Test prediction\n",
    "    X_test = np.random.randn(10, 5)\n",
    "    y_test = X_test @ true_weights\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(f'MSE: {mse:.4f}')\n",
    "    assert mse < 1.0, f'MSE {mse:.4f} too high'\n",
    "    print('All Online Gradient Descent tests passed!')\n",
    "\n",
    "# Uncomment to test\n",
    "# test_online_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 5</summary>\n",
    "\n",
    "**Hint**: For each sample, compute prediction `y_pred = X @ weights + bias`, error `error = y_pred - y`, gradient for weights `grad_w = X.T * error`, gradient for bias `grad_b = error`. Update: `weights -= lr * grad_w`, `bias -= lr * grad_b`. Use learning rate schedule `lr / sqrt(t+1)`.\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
