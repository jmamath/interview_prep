{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Classic DP/Graphs for ML Engineers\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jmamath/interview_prep/blob/main/chapter_02_dp_graphs_ml.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Have you ever wondered how Google Translate can take a sentence in one language and produce a coherent, grammatically correct translation in another? Or how a speech recognition system on your phone can accurately transcribe your spoken words into text, even in a noisy environment? These are not just feats of large-scale data processing; they are also triumphs of algorithmic ingenuity. At the heart of these technologies lie classic algorithms from computer science, adapted and scaled for the complexities of machine learning.\n",
    "\n",
    "In this chapter, we'll pull back the curtain on some of these fundamental algorithms. We'll see how dynamic programming and graph search, concepts you might have first encountered in a standard algorithms course, are the workhorses behind many of the ML-powered features you use every day. We'll move beyond the textbook definitions and dive into practical, hands-on exercises that show you how these algorithms are applied in the real world.\n",
    "\n",
    "By the end of this chapter, you'll not only have a deeper understanding of these classic algorithms, but you'll also have a practical toolkit for applying them to your own machine learning problems. So, let's get started!\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement dynamic programming solutions for ML-related problems\n",
    "- Design and implement beam search algorithms for sequence generation\n",
    "- Apply graph algorithms to model training and inference problems\n",
    "- Implement the Viterbi algorithm for sequence tagging\n",
    "- Use diverse beam search for better generation diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Simple Beam Search (Easy)\n",
    "\n",
    "### Contextual Introduction\n",
    "In machine translation or text summarization, we often need to generate a sequence of words. A simple approach, called greedy search, is to pick the most likely word at each step. However, this can lead to suboptimal results. For example, the best-scoring sentence might not start with the single best word. Beam search is a more effective alternative that keeps track of the `k` most promising sequences (the \"beam\") at each step, leading to better overall results.\n",
    "\n",
    "### Key Concepts\n",
    "- **Greedy Search**: Always choosing the locally optimal option at each step.\n",
    "- **Beam Search**: A graph search algorithm that explores a graph by expanding the most promising nodes in a limited set.\n",
    "- **Beam Width (k)**: The number of partial sequences (beams) to keep at each step.\n",
    "\n",
    "### Problem Statement\n",
    "Implement a basic beam search algorithm for sequence generation. You will be given a vocabulary and a simple scoring function. Your task is to generate the top `k` sequences of a given maximum length.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement beam search with a configurable beam width.\n",
    "- Support early stopping when an end-of-sequence token is reached.\n",
    "- Return the top `k` sequences and their scores.\n",
    "\n",
    "### Example: Understanding Beam Search with a Small Vocabulary\n",
    "\n",
    "```python\n",
    "# Let's see how beam search works step by step\n",
    "# Vocabulary: ['a', 'b', 'c', '<END>']\n",
    "# Beam width: 2 (keep top-2 sequences)\n",
    "# Max length: 2\n",
    "\n",
    "# STEP 0: Start with empty sequence\n",
    "# Beam: [(score=0.0, seq=[])]\n",
    "\n",
    "# STEP 1: Expand - try adding each word\n",
    "# Candidates: (score=-1, ['a']), (score=-1, ['b']), (score=-1, ['c']), (score=-1, ['<END>'])\n",
    "# After sorting by score and keeping top-2:\n",
    "# Beam: [(score=-1, ['a']), (score=-1, ['b'])]\n",
    "\n",
    "# STEP 2: Expand each sequence in beam\n",
    "# From ['a']: (score=-2, ['a','a']), (score=-2, ['a','b']), (score=-2, ['a','c']), ...\n",
    "# From ['b']: (score=-2, ['b','a']), (score=-2, ['b','b']), (score=-2, ['b','c']), ...\n",
    "# Top-2: [(score=-2, ['a','a']), (score=-2, ['a','b'])]\n",
    "\n",
    "# RESULT: [(['a', 'a'], -2), (['a', 'b'], -2)]\n",
    "\n",
    "# Why this is better than greedy:\n",
    "# - Greedy would pick ['a'] -> ['a','a'] only, missing other good options\n",
    "# - Beam search explores multiple paths and finds better combinations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import heapq\n",
    "\n",
    "class BeamSearch:\n",
    "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.end_token = end_token\n",
    "\n",
    "    def score_sequence(self, sequence: List[str]) -> float:\n",
    "        # In a real scenario, this would be a language model score.\n",
    "        # For this exercise, we use a simple length-based score.\n",
    "        return -len(sequence)\n",
    "\n",
    "    def search(self, beam_width: int, max_length: int) -> List[Tuple[float, List[str]]]:\n",
    "        # TODO: Implement the beam search algorithm.\n",
    "        # Remember to handle the beam, generate candidates, and manage completed sequences.\n",
    "        pass\n",
    "\n",
    "def test_beam_search():\n",
    "    vocabulary = ['a', 'b', 'c', '<END>']\n",
    "    # We are providing a correct implementation here for the sake of the test\n",
    "    class CorrectBeamSearch(BeamSearch):\n",
    "        def search(self, beam_width: int, max_length: int) -> List[Tuple[float, List[str]]]:\n",
    "            if beam_width == 0: return []\n",
    "            beam = [(0.0, [])]  # (score, sequence)\n",
    "            completed_sequences = []\n",
    "            for _ in range(max_length):\n",
    "                new_beam = []\n",
    "                for score, seq in beam:\n",
    "                    if not seq or seq[-1] == self.end_token:\n",
    "                        completed_sequences.append((score, seq))\n",
    "                        continue\n",
    "                    for token in self.vocabulary:\n",
    "                        new_seq = seq + [token]\n",
    "                        new_score = self.score_sequence(new_seq)\n",
    "                        heapq.heappush(new_beam, (new_score, new_seq))\n",
    "                beam.clear()\n",
    "                while new_beam and len(beam) < beam_width:\n",
    "                    score, seq = heapq.heappop(new_beam)\n",
    "                    beam.append((score, seq))\n",
    "            all_sequences = completed_sequences + beam\n",
    "            return sorted(all_sequences, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "    beam_search = CorrectBeamSearch(vocabulary)\n",
    "\n",
    "    # Test 1: Basic search\n",
    "    results = beam_search.search(beam_width=2, max_length=2)\n",
    "    assert len(results) == 2\n",
    "    assert results[0][1] == []\n",
    "\n",
    "    # Test 2: Zero beam width\n",
    "    results = beam_search.search(beam_width=0, max_length=2)\n",
    "    assert len(results) == 0\n",
    "\n",
    "    print(\"ðŸŽ‰ All beam search tests passed!\")\n",
    "\n",
    "test_beam_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 1</summary>\n",
    "\n",
    "**Hint**: Use a priority queue (like Python's `heapq`) to maintain the beam of the top `k` sequences at each step. The priority queue should store tuples of `(score, sequence)`. At each step of the generation, expand each sequence in the beam with all possible next tokens, score the new sequences, and use the priority queue to keep only the top `k`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Top-k Beam Search with Scores (Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "Standard beam search can sometimes produce sequences that are very similar to each other. To encourage more diversity, we can introduce techniques like length normalization and a diversity penalty. Length normalization prevents the search from favoring shorter sequences, while a diversity penalty discourages sequences that are too similar to already selected ones. These techniques are crucial for applications like creative text generation or offering multiple diverse translation options.\n",
    "\n",
    "### Key Concepts\n",
    "- **Length Normalization**: A technique to reduce the bias of beam search towards shorter sequences by dividing the score by the sequence length raised to some power.\n",
    "- **Diversity Penalty**: A penalty applied to the score of a sequence based on its similarity to other sequences in the beam, encouraging more diverse outputs.\n",
    "\n",
    "### Problem Statement\n",
    "Extend the simple beam search to include length normalization and a diversity penalty. You will implement a `TopKBeamSearch` class that generates more diverse and higher-quality sequences.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement length normalization in the scoring function.\n",
    "- Implement a diversity penalty based on n-gram overlap.\n",
    "- Combine these techniques in the search algorithm to produce diverse sequences.\n",
    "\n",
    "### Example: Length Normalization and Diversity Penalty\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "# WITHOUT length normalization: Shorter sequences get higher scores\n",
    "sequences = [\n",
    "    ['hello'],           # score = -1\n",
    "    ['hello', 'world']   # score = -2\n",
    "]\n",
    "# Result: First sequence wins! (score -1 > -2)\n",
    "\n",
    "# WITH length normalization (dividing by length^alpha):\n",
    "# alpha = 0.6 is commonly used\n",
    "sequences_with_norm = [\n",
    "    ['hello'],           # normalized = -1 / (1^0.6) = -1.0\n",
    "    ['hello', 'world']   # normalized = -2 / (2^0.6) = -1.32\n",
    "]\n",
    "# Result: First still wins, but the gap is smaller\n",
    "\n",
    "# DIVERSITY PENALTY - preventing similar sequences:\n",
    "# Sequence 1: ['a', 'b', 'c']  -> bigrams: {('a','b'), ('b','c')}\n",
    "# Sequence 2: ['a', 'b', 'd']  -> bigrams: {('a','b'), ('b','d')}\n",
    "# Overlap: 1 bigram ('a','b')\n",
    "# Diversity penalty = 1 * penalty_weight (e.g., 0.5)\n",
    "\n",
    "# In beam search with diversity:\n",
    "# - First sequence ['a', 'b', 'c'] is selected with score 1.0\n",
    "# - Second sequence ['a', 'b', 'd'] gets penalty for overlapping bigram\n",
    "# - New score = 0.8 - (1 * 0.5) = 0.3\n",
    "# - Different sequence like ['x', 'y', 'z'] with no overlap keeps full score 0.9\n",
    "# Result: More diverse outputs!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "class TopKBeamSearch(BeamSearch):\n",
    "    def score_sequence(self, sequence: List[str], length_penalty: float = 0.6) -> float:\n",
    "        # TODO: Implement length-normalized scoring\n",
    "        pass\n",
    "\n",
    "    def calculate_diversity_penalty(self, sequence: List[str], existing_sequences: List[List[str]], penalty_weight: float = 0.5) -> float:\n",
    "        # TODO: Implement diversity penalty calculation based on n-gram overlap\n",
    "        pass\n",
    "\n",
    "    def search(self, beam_width: int, max_length: int, diversity_penalty: float = 0.5) -> List[Tuple[float, List[str]]]:\n",
    "        # TODO: Implement the search method incorporating the new scoring and penalty functions\n",
    "        pass\n",
    "\n",
    "def test_top_k_beam_search():\n",
    "    vocabulary = ['a', 'b', 'c', 'd', '<END>']\n",
    "    # Correct implementation for testing purposes\n",
    "    class CorrectTopK(TopKBeamSearch):\n",
    "        def score_sequence(self, sequence: List[str], length_penalty: float = 0.6) -> float:\n",
    "            raw_score = -len(sequence) * 0.5\n",
    "            return raw_score / (len(sequence)**length_penalty) if sequence else 0\n",
    "        def calculate_diversity_penalty(self, sequence: List[str], existing_sequences: List[List[str]], penalty_weight: float = 0.5) -> float:\n",
    "            penalty = 0.0\n",
    "            for existing_seq in existing_sequences:\n",
    "                seq_bigrams = set(zip(sequence, sequence[1:]))\n",
    "                existing_bigrams = set(zip(existing_seq, existing_seq[1:]))\n",
    "                overlap = len(seq_bigrams.intersection(existing_bigrams))\n",
    "                penalty += overlap * penalty_weight\n",
    "            return penalty\n",
    "        def search(self, beam_width: int, max_length: int, diversity_penalty: float = 0.5) -> List[Tuple[float, List[str]]]:\n",
    "            beam = [(0.0, [])]\n",
    "            completed = []\n",
    "            for _ in range(max_length):\n",
    "                new_beam = []\n",
    "                for score, seq in beam:\n",
    "                    if not seq or seq[-1] == self.end_token: continue\n",
    "                    for token in self.vocabulary:\n",
    "                        new_seq = seq + [token]\n",
    "                        new_score = self.score_sequence(new_seq) - self.calculate_diversity_penalty(new_seq, [s for _, s in beam], diversity_penalty)\n",
    "                        heapq.heappush(new_beam, (new_score, new_seq))\n",
    "                beam.clear()\n",
    "                while new_beam and len(beam) < beam_width:\n",
    "                    score, seq = heapq.heappop(new_beam)\n",
    "                    if seq[-1] == self.end_token: completed.append((score, seq))\n",
    "                    else: beam.append((score, seq))\n",
    "            return sorted(completed + beam, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "    top_k_search = CorrectTopK(vocabulary)\n",
    "\n",
    "    # Test 1: With diversity penalty, we should get more diverse results\n",
    "    results_diverse = top_k_search.search(beam_width=3, max_length=3, diversity_penalty=1.0)\n",
    "    results_no_diversity = top_k_search.search(beam_width=3, max_length=3, diversity_penalty=0.0)\n",
    "\n",
    "    assert results_diverse[0][1] != results_no_diversity[0][1]\n",
    "\n",
    "    print(\"ðŸŽ‰ All top-k beam search tests passed!\")\n",
    "\n",
    "test_top_k_beam_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 2</summary>\n",
    "\n",
    "**Hint**: For length normalization, divide the sequence score by `len(sequence)**alpha`, where `alpha` is the length penalty factor. For the diversity penalty, you can calculate the n-gram overlap between a candidate sequence and the sequences already in the beam. Subtract a penalty proportional to this overlap from the candidate's score.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Viterbi Algorithm for Sequence Tagging (Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "Part-of-speech (POS) tagging is a classic NLP task where we assign a grammatical tag (like noun, verb, or adjective) to each word in a sentence. A simple approach of picking the most likely tag for each word independently can fail because it ignores the context (e.g., \"the fish\" is more likely than \"the and\"). The Viterbi algorithm, a dynamic programming method, solves this by finding the most likely sequence of tags given the sequence of words. It's used in Hidden Markov Models (HMMs) and is fundamental to many sequence labeling tasks.\n",
    "\n",
    "### Key Concepts\n",
    "- **Hidden Markov Model (HMM)**: A statistical model with hidden states (the tags) and observable outputs (the words).\n",
    "- **Transition Probability**: The probability of moving from one state to another, P(tag_i | tag_{i-1}).\n",
    "- **Emission Probability**: The probability of observing a word given a state, P(word_i | tag_i).\n",
    "- **Dynamic Programming**: The Viterbi algorithm uses a table to store the maximum probability of being in a certain state at a certain time, avoiding re-computation.\n",
    "\n",
    "### Problem Statement\n",
    "Implement the Viterbi algorithm for a simple POS tagger. You will be given the transition, emission, and initial probabilities of an HMM. Your task is to find the most likely sequence of POS tags for a given sentence.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement the Viterbi algorithm using dynamic programming.\n",
    "- Use logarithms to prevent underflow with small probabilities.\n",
    "- Handle unknown words with a simple smoothing technique.\n",
    "- Reconstruct the most likely path of tags.\n",
    "\n",
    "### Example: Manual Viterbi Computation\n",
    "\n",
    "```python\n",
    "# Let's trace through a simple example: sentence = ['the', 'cat', 'sat']\n",
    "# Tags: DET (determiner), NOUN (noun), VERB (verb)\n",
    "\n",
    "# Transition probabilities P(tag_i | tag_{i-1}):\n",
    "#           DET   NOUN  VERB\n",
    "#  DET:    [0.1   0.8   0.1]\n",
    "#  NOUN:   [0.3   0.1   0.6]\n",
    "#  VERB:   [0.2   0.7   0.1]\n",
    "\n",
    "# Emission probabilities P(word | tag):\n",
    "#         'the'  'cat'  'sat'\n",
    "#  DET:  [0.9    0.05   0.05]\n",
    "#  NOUN: [0.05   0.9    0.05]\n",
    "#  VERB: [0.05   0.05   0.9]\n",
    "\n",
    "# Initial probabilities: [0.8 (DET), 0.1 (NOUN), 0.1 (VERB)]\n",
    "\n",
    "# STEP 1: Process word 'the'\n",
    "# For each tag, compute: initial_prob * emission_prob['the']\n",
    "# DET:  0.8 * 0.9 = 0.72   <- best\n",
    "# NOUN: 0.1 * 0.05 = 0.005\n",
    "# VERB: 0.1 * 0.05 = 0.005\n",
    "# Viterbi table: [0.72, 0.005, 0.005]\n",
    "\n",
    "# STEP 2: Process word 'cat'\n",
    "# For NOUN tag: max(\n",
    "#     0.72 * 0.8 * 0.9,     <- from DET\n",
    "#     0.005 * 0.1 * 0.9,    <- from NOUN\n",
    "#     0.005 * 0.7 * 0.9     <- from VERB\n",
    "# ) = max(0.518, 0.0005, 0.0032) = 0.518  <- from DET\n",
    "# Similarly for other tags...\n",
    "\n",
    "# STEP 3: Process word 'sat'\n",
    "# Similar computation, tracking best path\n",
    "\n",
    "# FINAL RESULT: ['DET', 'NOUN', 'VERB']\n",
    "# Because this path has the highest joint probability\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class HMMTagger:\n",
    "    def __init__(self, tags: List[str], words: List[str], initial_prob: np.ndarray, transition_prob: np.ndarray, emission_prob: np.ndarray):\n",
    "        self.tags = tags\n",
    "        self.words = words\n",
    "        self.tag_to_idx = {tag: i for i, tag in enumerate(tags)}\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "        self.log_initial = np.log(initial_prob + 1e-10)\n",
    "        self.log_transition = np.log(transition_prob + 1e-10)\n",
    "        self.log_emission = np.log(emission_prob + 1e-10)\n",
    "\n",
    "    def viterbi(self, sentence: List[str]) -> Tuple[List[str], float]:\n",
    "        # TODO: Implement the Viterbi algorithm, including initialization, recursion, and backtracking.\n",
    "        pass\n",
    "\n",
    "def test_viterbi_algorithm():\n",
    "    tags = ['DET', 'NOUN', 'VERB']\n",
    "    words = ['the', 'cat', 'sat']\n",
    "    initial_prob = np.array([0.8, 0.1, 0.1])\n",
    "    transition_prob = np.array([[0.1, 0.8, 0.1], [0.3, 0.1, 0.6], [0.2, 0.7, 0.1]])\n",
    "    emission_prob = np.array([[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
    "\n",
    "    class CorrectHMM(HMMTagger):\n",
    "        def viterbi(self, sentence: List[str]) -> Tuple[List[str], float]:\n",
    "            T = len(sentence); N = len(self.tags)\n",
    "            if T == 0: return [], 0.0\n",
    "            viterbi_table = np.zeros((T, N)); backpointer = np.zeros((T, N), dtype=int)\n",
    "            word_idx = self.word_to_idx.get(sentence[0], -1)\n",
    "            emission = self.log_emission[:, word_idx] if word_idx != -1 else np.log(np.full(N, 1e-10))\n",
    "            viterbi_table[0, :] = self.log_initial + emission\n",
    "            for t in range(1, T):\n",
    "                word_idx = self.word_to_idx.get(sentence[t], -1)\n",
    "                emission = self.log_emission[:, word_idx] if word_idx != -1 else np.log(np.full(N, 1e-10))\n",
    "                for s in range(N):\n",
    "                    trans_probs = viterbi_table[t-1, :] + self.log_transition[:, s]\n",
    "                    viterbi_table[t, s] = np.max(trans_probs) + emission[s]\n",
    "                    backpointer[t, s] = np.argmax(trans_probs)\n",
    "            best_prob = np.max(viterbi_table[T-1, :]); last_state = np.argmax(viterbi_table[T-1, :])\n",
    "            path = [self.tags[last_state]]\n",
    "            for t in range(T - 1, 0, -1):\n",
    "                last_state = backpointer[t, last_state]\n",
    "                path.insert(0, self.tags[last_state])\n",
    "            return path, np.exp(best_prob)\n",
    "\n",
    "    tagger = CorrectHMM(tags, words, initial_prob, transition_prob, emission_prob)\n",
    "\n",
    "    # Test 1: A likely sequence\n",
    "    sentence1 = ['the', 'cat', 'sat']\n",
    "    path, prob = tagger.viterbi(sentence1)\n",
    "    assert path == ['DET', 'NOUN', 'VERB']\n",
    "\n",
    "    # Test 2: A sequence with an unknown word\n",
    "    sentence2 = ['the', 'dog', 'sat']\n",
    "    path, prob = tagger.viterbi(sentence2)\n",
    "    assert path == ['DET', 'NOUN', 'VERB']\n",
    "\n",
    "    print(\"ðŸŽ‰ All Viterbi algorithm tests passed!\")\n",
    "\n",
    "test_viterbi_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 3</summary>\n",
    "\n",
    "**Hint**: Create a dynamic programming table of size `(num_words, num_tags)`. Each cell `(i, j)` will store the maximum probability of a tag sequence of length `i` ending with tag `j`. Also, create a `backpointer` table to store the path. Iterate through the words and for each word, calculate the probabilities for each tag based on the previous word's tag probabilities and the transition probabilities. After filling the table, backtrack from the end to find the most likely path.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Constrained Beam Search (Medium-Hard)\n",
    "\n",
    "### Contextual Introduction\n",
    "In many real-world applications, we need to generate text that adheres to certain rules. For example, a chatbot must avoid generating toxic language, or a text summarization model might be required to include certain keywords. Constrained beam search extends the standard algorithm by pruning partial sequences that violate predefined constraints, ensuring that the final output meets the requirements.\n",
    "\n",
    "### Key Concepts\n",
    "- **Constraint Satisfaction**: The process of finding a solution that satisfies a set of constraints.\n",
    "- **Hard Constraints vs. Soft Constraints**: Hard constraints must be satisfied, while soft constraints are desirable but not mandatory.\n",
    "- **Pruning**: The process of eliminating partial solutions that cannot lead to a valid final solution.\n",
    "\n",
    "### Problem Statement\n",
    "Implement a constrained beam search algorithm that can handle two types of constraints: `MustContainConstraint` and `MustNotContainConstraint`. You will integrate these constraints into the beam search process to generate sequences that satisfy all given rules.\n",
    "\n",
    "**Requirements**:\n",
    "- Design an abstract `Constraint` class and specific implementations for `MustContainConstraint` and `MustNotContainConstraint`.\n",
    "- Integrate constraint checking into the beam search algorithm.\n",
    "- Prune the search space by discarding partial sequences that violate constraints.\n",
    "\n",
    "### Example: How Constraints Filter Beam Search\n",
    "\n",
    "```python\n",
    "# Vocabulary: ['a', 'b', 'c', 'toxic', '<END>']\n",
    "# Beam width: 2\n",
    "# Constraint: MustNotContainConstraint('toxic')\n",
    "\n",
    "# STEP 1: Generate candidates\n",
    "# Candidates: (score=-1, ['a']), (score=-1, ['b']), (score=-1, ['toxic']), (score=-1, ['c'])\n",
    "\n",
    "# STEP 2: Apply constraints\n",
    "# Check each candidate:\n",
    "# - ['a']: Contains 'toxic'? No -> KEEP\n",
    "# - ['b']: Contains 'toxic'? No -> KEEP\n",
    "# - ['toxic']: Contains 'toxic'? Yes -> PRUNE (remove)\n",
    "# - ['c']: Contains 'toxic'? No -> KEEP\n",
    "\n",
    "# STEP 3: Select top-2 after filtering\n",
    "# Beam: [(score=-1, ['a']), (score=-1, ['b'])]\n",
    "\n",
    "# STEP 4: Continue expanding only valid sequences\n",
    "# From ['a']: ['a','a'], ['a','b'], ['a','c'] (not ['a','toxic']!)\n",
    "# From ['b']: ['b','a'], ['b','b'], ['b','c'] (not ['b','toxic']!)\n",
    "\n",
    "# FINAL RESULT: Only sequences without 'toxic' are generated\n",
    "# This is much more efficient than generating toxic sequences and filtering later!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Set\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Constraint(ABC):\n",
    "    @abstractmethod\n",
    "    def check(self, sequence: List[str]) -> bool:\n",
    "        pass\n",
    "\n",
    "class MustContainConstraint(Constraint):\n",
    "    def __init__(self, required_token: str):\n",
    "        self.required_token = required_token\n",
    "\n",
    "    def check(self, sequence: List[str]) -> bool:\n",
    "        # TODO: Implement the check for must-contain constraint\n",
    "        pass\n",
    "\n",
    "class MustNotContainConstraint(Constraint):\n",
    "    def __init__(self, forbidden_token: str):\n",
    "        self.forbidden_token = forbidden_token\n",
    "\n",
    "    def check(self, sequence: List[str]) -> bool:\n",
    "        # TODO: Implement the check for must-not-contain constraint\n",
    "        pass\n",
    "\n",
    "class ConstrainedBeamSearch(BeamSearch):\n",
    "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
    "        super().__init__(vocabulary, end_token)\n",
    "        self.constraints = []\n",
    "\n",
    "    def add_constraint(self, constraint: Constraint):\n",
    "        self.constraints.append(constraint)\n",
    "\n",
    "    def search(self, beam_width: int, max_length: int) -> List[Tuple[float, List[str]]]:\n",
    "        # TODO: Implement the constrained beam search\n",
    "        pass\n",
    "\n",
    "def test_constrained_beam_search():\n",
    "    vocabulary = ['a', 'b', 'c', 'd', '<END>']\n",
    "    class CorrectConstrained(ConstrainedBeamSearch):\n",
    "        def search(self, beam_width: int, max_length: int) -> List[Tuple[float, List[str]]]:\n",
    "            beam = [(0.0, [])]\n",
    "            completed = []\n",
    "            for _ in range(max_length):\n",
    "                new_beam = []\n",
    "                for score, seq in beam:\n",
    "                    if not seq or seq[-1] == self.end_token: continue\n",
    "                    for token in self.vocabulary:\n",
    "                        new_seq = seq + [token]\n",
    "                        if all(c.check(new_seq) for c in self.constraints):\n",
    "                            new_score = self.score_sequence(new_seq)\n",
    "                            heapq.heappush(new_beam, (new_score, new_seq))\n",
    "                beam.clear()\n",
    "                while new_beam and len(beam) < beam_width:\n",
    "                    score, seq = heapq.heappop(new_beam)\n",
    "                    if seq[-1] == self.end_token: completed.append((score, seq))\n",
    "                    else: beam.append((score, seq))\n",
    "            return sorted(completed + beam, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "    \n",
    "    # Test 1: Must contain 'c'\n",
    "    searcher_must_contain = CorrectConstrained(vocabulary)\n",
    "    searcher_must_contain.add_constraint(MustContainConstraint('c'))\n",
    "    results = searcher_must_contain.search(beam_width=2, max_length=3)\n",
    "    for _, seq in results:\n",
    "        assert 'c' in seq\n",
    "\n",
    "    # Test 2: Must not contain 'b'\n",
    "    searcher_must_not_contain = CorrectConstrained(vocabulary)\n",
    "    searcher_must_not_contain.add_constraint(MustNotContainConstraint('b'))\n",
    "    results = searcher_must_not_contain.search(beam_width=2, max_length=3)\n",
    "    for _, seq in results:\n",
    "        assert 'b' not in seq\n",
    "\n",
    "    print(\"ðŸŽ‰ All constrained beam search tests passed!\")\n",
    "\n",
    "test_constrained_beam_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 4</summary>\n",
    "\n",
    "**Hint**: Create an abstract base class `Constraint` with a `check` method. Then, implement concrete constraint classes like `MustContainConstraint` and `MustNotContainConstraint`. In your beam search loop, after generating a new candidate sequence, iterate through your list of constraints and only add the sequence to the new beam if it satisfies all of them.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Diverse Beam Search with Groups (Hard)\n",
    "\n",
    "### Contextual Introduction\n",
    "While standard beam search is good at finding high-quality sequences, it often produces a set of very similar results. For creative applications like story generation or offering multiple translation choices, we need diversity. Diverse beam search addresses this by partitioning the beam into groups and encouraging each group to explore a different part of the search space. This ensures that the final set of sequences is both high-quality and diverse.\n",
    "\n",
    "### Key Concepts\n",
    "- **Sequence Similarity**: A metric to quantify how similar two sequences are (e.g., n-gram overlap, Jaccard similarity).\n",
    "- **Clustering/Grouping**: The process of partitioning a set of items into groups based on similarity.\n",
    "- **Quality-Diversity Trade-off**: The balance between generating high-scoring (quality) sequences and generating a wide variety of (diverse) sequences.\n",
    "\n",
    "### Problem Statement\n",
    "Implement a diverse beam search algorithm that groups similar sequences and selects the best from each group. This will involve calculating sequence similarity, grouping sequences, and modifying the beam search to maintain diversity across groups.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement a function to calculate sequence similarity (e.g., Jaccard similarity of bigrams).\n",
    "- In each step of the beam search, group the candidate sequences.\n",
    "- Select the best sequence from each group to form the new beam, ensuring diversity.\n",
    "\n",
    "### Example: Sequence Similarity and Grouping\n",
    "\n",
    "```python\n",
    "# Calculating Jaccard similarity between sequences using bigrams\n",
    "\n",
    "# Sequence 1: ['hello', 'world', 'end']\n",
    "# Bigrams: {('hello', 'world'), ('world', 'end')}\n",
    "\n",
    "# Sequence 2: ['hello', 'world', 'now']\n",
    "# Bigrams: {('hello', 'world'), ('world', 'now')}\n",
    "\n",
    "# Intersection: {('hello', 'world')}  (1 common bigram)\n",
    "# Union: {('hello', 'world'), ('world', 'end'), ('world', 'now')}  (3 total unique)\n",
    "# Jaccard similarity = 1 / 3 â‰ˆ 0.33  (quite similar)\n",
    "\n",
    "# Sequence 3: ['goodbye', 'friend', 'soon']\n",
    "# Bigrams: {('goodbye', 'friend'), ('friend', 'soon')}\n",
    "\n",
    "# Intersection with Seq1: {} (0 common bigrams)\n",
    "# Union with Seq1: {('hello', 'world'), ('world', 'end'), ('goodbye', 'friend'), ('friend', 'soon')}\n",
    "# Jaccard similarity = 0 / 4 = 0.0  (very different!)\n",
    "\n",
    "# GROUPING IN DIVERSE BEAM SEARCH:\n",
    "# Beam width: 4, Groups: 2\n",
    "# Candidates (sorted by score):\n",
    "# 1. (score=0.9, ['hello', 'world', 'end'])\n",
    "# 2. (score=0.88, ['hello', 'world', 'now'])      <- Similar to #1\n",
    "# 3. (score=0.85, ['goodbye', 'friend', 'soon'])  <- Different from #1\n",
    "# 4. (score=0.82, ['hi', 'there', 'friend'])      <- Different from #1 and #3\n",
    "\n",
    "# GROUP 1 (from sequence 1): [#1, #2]  <- Similar sequences\n",
    "# GROUP 2 (from sequence 3): [#3, #4]  <- Different sequences\n",
    "\n",
    "# SELECT BEST FROM EACH GROUP:\n",
    "# From GROUP 1: #1 (best score)\n",
    "# From GROUP 2: #3 (best score in this group)\n",
    "\n",
    "# RESULT: [['hello', 'world', 'end'], ['goodbye', 'friend', 'soon']]\n",
    "# More diverse than ['hello', 'world', 'end'], ['hello', 'world', 'now']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "class DiverseBeamSearch(BeamSearch):\n",
    "    def calculate_similarity(self, seq1: List[str], seq2: List[str]) -> float:\n",
    "        # TODO: Implement Jaccard similarity for bigrams\n",
    "        pass\n",
    "\n",
    "    def search(self, beam_width: int, max_length: int, num_groups: int, diversity_strength: float = 0.5) -> List[Tuple[float, List[str]]]:\n",
    "        # TODO: Implement diverse beam search using groups\n",
    "        pass\n",
    "\n",
    "def test_diverse_beam_search():\n",
    "    vocabulary = ['a', 'b', 'c', 'd', 'e', '<END>']\n",
    "    class CorrectDiverse(DiverseBeamSearch):\n",
    "        def calculate_similarity(self, seq1: List[str], seq2: List[str]) -> float:\n",
    "            set1 = set(zip(seq1, seq1[1:])); set2 = set(zip(seq2, seq2[1:]))\n",
    "            intersection = len(set1.intersection(set2)); union = len(set1.union(set2))\n",
    "            return intersection / union if union > 0 else 0\n",
    "        def search(self, beam_width: int, max_length: int, num_groups: int, diversity_strength: float = 0.5) -> List[Tuple[float, List[str]]]:\n",
    "            if num_groups > beam_width: num_groups = beam_width\n",
    "            beams = [[(0.0, [])] for _ in range(num_groups)]\n",
    "            completed = []\n",
    "            for _ in range(max_length):\n",
    "                all_candidates = []\n",
    "                for i in range(num_groups):\n",
    "                    for score, seq in beams[i]:\n",
    "                        if not seq or seq[-1] == self.end_token: continue\n",
    "                        for token in self.vocabulary:\n",
    "                            new_seq = seq + [token]\n",
    "                            new_score = self.score_sequence(new_seq) - (i * diversity_strength)\n",
    "                            all_candidates.append((new_score, new_seq))\n",
    "                beams = [[] for _ in range(num_groups)]\n",
    "                sorted_candidates = sorted(all_candidates, key=lambda x: x[0], reverse=True)\n",
    "                for score, seq in sorted_candidates:\n",
    "                    group_idx = hash(' '.join(seq[:1])) % num_groups\n",
    "                    if len(beams[group_idx]) < beam_width / num_groups:\n",
    "                        if seq[-1] == self.end_token: completed.append((score, seq))\n",
    "                        else: beams[group_idx].append((score, seq))\n",
    "            flat_beam = [item for sublist in beams for item in sublist]\n",
    "            return sorted(completed + flat_beam, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "    diverse_searcher = CorrectDiverse(vocabulary)\n",
    "\n",
    "    # Test that with diversity, we get different results\n",
    "    results_diverse = diverse_searcher.search(beam_width=4, max_length=3, num_groups=2, diversity_strength=0.8)\n",
    "    results_regular = diverse_searcher.search(beam_width=4, max_length=3, num_groups=1, diversity_strength=0.0)\n",
    "\n",
    "    assert results_diverse[0][1] != results_regular[0][1]\n",
    "\n",
    "    print(\"ðŸŽ‰ All diverse beam search tests passed!\")\n",
    "\n",
    "test_diverse_beam_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 5</summary>\n",
    "\n",
    "**Hint**: A simple approach to grouping is to use a penalty. In each step of the beam search, when you are scoring new candidate sequences, add a penalty to the score that is proportional to the sequence's similarity to the other sequences in its group. This will encourage the groups to diverge. For example, you can use `group_index * diversity_penalty` as a simple penalty.\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
