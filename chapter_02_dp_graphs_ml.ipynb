{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Classic DP/Graphs for ML Engineers\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jmamath/interview_prep/blob/main/chapter_02_dp_graphs_ml.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Have you ever wondered how Google Translate can take a sentence in one language and produce a coherent, grammatically correct translation in another? Or how a speech recognition system on your phone can accurately transcribe your spoken words into text, even in a noisy environment? These are not just feats of large-scale data processing; they are also triumphs of algorithmic ingenuity. At the heart of these technologies lie classic algorithms from computer science, adapted and scaled for the complexities of machine learning.\n",
    "\n",
    "In this chapter, we'll pull back the curtain on some of these fundamental algorithms. We'll see how dynamic programming and graph search, concepts you might have first encountered in a standard algorithms course, are the workhorses behind many of the ML-powered features you use every day. We'll move beyond the textbook definitions and dive into practical, hands-on exercises that show you how these algorithms are applied in the real world.\n",
    "\n",
    "By the end of this chapter, you'll not only have a deeper understanding of these classic algorithms, but you'll also have a practical toolkit for applying them to your own machine learning problems. So, let's get started!\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement dynamic programming solutions for ML-related problems\n",
    "- Design and implement beam search algorithms for sequence generation\n",
    "- Apply graph algorithms to model training and inference problems\n",
    "- Implement the Viterbi algorithm for sequence tagging\n",
    "- Use diverse beam search for better generation diversity\n",
    "\n",
    "## Chapter Progression\n",
    "\n",
    "This chapter is organized to build your understanding progressively:\n",
    "\n",
    "- **Foundation**: We start with an introduction to beam search, explaining the core intuition and mechanics that underlie many sequence generation tasks.\n",
    "- **Problems 1-2**: You'll implement standard beam search and learn about scoring techniques like length normalization and diversity penalties.\n",
    "- **Problem 3**: We shift to dynamic programming with the Viterbi algorithm, a classic method for structured prediction in sequence tagging.\n",
    "- **Problems 4-5**: Finally, you'll explore advanced beam search variants that handle constraints and maximize output diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search: Foundation and Intuition\n",
    "\n",
    "### What is Beam Search?\n",
    "\n",
    "Beam search is a heuristic search algorithm that explores a graph by expanding the most promising nodes in a limited set. Unlike exhaustive search algorithms that explore every possibility, beam search keeps only the top-k most promising partial solutions at each step. This \"beam\" of candidates is progressively extended until complete solutions are found.\n",
    "\n",
    "Think of beam search as a middle ground between two extremes:\n",
    "- **Greedy search**: Keep only the single best option at each step (fast but often suboptimal)\n",
    "- **Exhaustive search**: Keep all possibilities (optimal but computationally infeasible)\n",
    "\n",
    "### Why Beam Search?\n",
    "\n",
    "Consider translating \"I love dogs\" to French. A greedy approach might choose:\n",
    "1. Pick best first word: \"Je\" (score: 0.9)\n",
    "2. Pick best second word given \"Je\": \"suis\" (score: 0.8)\n",
    "3. Result: \"Je suis...\" (total score: 0.72)\n",
    "\n",
    "But the optimal translation might be:\n",
    "1. Start with \"J'\" (score: 0.7)  \n",
    "2. Add \"adore\" (score: 0.85)\n",
    "3. Add \"les chiens\" (score: 0.9)\n",
    "4. Result: \"J'adore les chiens\" (total score: 0.54) - better despite lower first word score!\n",
    "\n",
    "**The problem**: Greedy search commits too early to locally optimal choices. By keeping multiple candidates alive (the beam), beam search can discover globally better solutions.\n",
    "\n",
    "### Core Mechanics: A Step-by-Step Walkthrough\n",
    "\n",
    "Let's trace beam search with a tiny vocabulary and see exactly how it works:\n",
    "\n",
    "**Setup:**\n",
    "- Vocabulary: ['hello', 'hi', 'bye']\n",
    "- Beam width k=2 (keep top-2 sequences)\n",
    "- Max length: 2 words\n",
    "- Scoring: Longer sequences score lower (simulating log probabilities)\n",
    "\n",
    "**Step 0 - Initialize:**\n",
    "```\n",
    "Beam: [(score=0.0, sequence=[])]\n",
    "```\n",
    "\n",
    "**Step 1 - First Expansion:**\n",
    "Generate all possible 1-word sequences:\n",
    "```\n",
    "Candidates:\n",
    "  (score=-1.0, ['hello'])\n",
    "  (score=-0.9, ['hi'])      <- slightly better score\n",
    "  (score=-1.2, ['bye'])\n",
    "```\n",
    "Keep top-2:\n",
    "```\n",
    "Beam: [(score=-0.9, ['hi']), (score=-1.0, ['hello'])]\n",
    "```\n",
    "\n",
    "**Step 2 - Second Expansion:**\n",
    "Expand each sequence in the beam:\n",
    "```\n",
    "From ['hi']:\n",
    "  (score=-1.8, ['hi', 'hello'])\n",
    "  (score=-1.7, ['hi', 'hi'])\n",
    "  (score=-2.1, ['hi', 'bye'])\n",
    "\n",
    "From ['hello']:\n",
    "  (score=-1.9, ['hello', 'hello'])\n",
    "  (score=-1.8, ['hello', 'hi'])\n",
    "  (score=-2.2, ['hello', 'bye'])\n",
    "```\n",
    "Keep top-2 overall:\n",
    "```\n",
    "Final: [(score=-1.7, ['hi', 'hi']), (score=-1.8, ['hi', 'hello'])]\n",
    "```\n",
    "\n",
    "**Key observations:**\n",
    "1. We generated 6 candidates but kept only 2 (the beam width)\n",
    "2. Notice 'hello' was dropped entirely - it couldn't produce competitive 2-word sequences\n",
    "3. This is much faster than keeping all 3Â² = 9 possible sequences\n",
    "\n",
    "### When to Use Beam Search\n",
    "\n",
    "Beam search excels in these scenarios:\n",
    "- **Machine translation**: Generating fluent translations from source text\n",
    "- **Text summarization**: Producing coherent summaries of documents  \n",
    "- **Speech recognition**: Converting audio to text transcriptions\n",
    "- **Image captioning**: Generating natural language descriptions of images\n",
    "- **Any sequence generation task** where greedy search is too myopic and exhaustive search is too expensive\n",
    "\n",
    "### Key Parameters and Trade-offs\n",
    "\n",
    "**Beam Width (k):**\n",
    "- **k=1**: Reduces to greedy search (fast, often suboptimal)\n",
    "- **k=5-10**: Common sweet spot for many applications\n",
    "- **k=50+**: Approaches exhaustive search (slow, diminishing returns)\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Quality vs Speed**: Larger beams find better solutions but take longer\n",
    "- **Memory**: Must store k sequences and their scores at each step\n",
    "- **Diversity**: Larger beams may produce similar outputs; special techniques needed for diversity\n",
    "\n",
    "In the problems that follow, you'll implement beam search and explore techniques to balance these trade-offs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Simple Beam Search (Easy)\n",
    "\n",
    "### Contextual Introduction\n",
    "In machine translation or text summarization, we often need to generate a sequence of words. A simple approach, called greedy search, is to pick the most likely word at each step. However, this can lead to suboptimal results. For example, the best-scoring sentence might not start with the single best word. Beam search is a more effective alternative that keeps track of the `k` most promising sequences (the \"beam\") at each step, leading to better overall results.\n",
    "\n",
    "### Key Concepts\n",
    "- **Greedy Search**: Always choosing the locally optimal option at each step.\n",
    "- **Beam Search**: A graph search algorithm that explores a graph by expanding the most promising nodes in a limited set.\n",
    "- **Beam Width (k)**: The number of partial sequences (beams) to keep at each step.\n",
    "\n",
    "### Problem Statement\n",
    "Implement a basic beam search algorithm for sequence generation. You will be given a vocabulary and a simple scoring function. Your task is to generate the top `k` sequences of a given maximum length.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement beam search with a configurable beam width.\n",
    "- Support early stopping when an end-of-sequence token is reached.\n",
    "- Return the top `k` sequences and their scores.\n",
    "\n",
    "### Example: Understanding Beam Search with a Small Vocabulary\n",
    "\n",
    "```python\n",
    "# Let's see how beam search works step by step\n",
    "# Vocabulary: ['a', 'b', 'c', '<END>']\n",
    "# Beam width: 2 (keep top-2 sequences)\n",
    "# Max length: 2\n",
    "\n",
    "# STEP 0: Start with empty sequence\n",
    "# Beam: [(score=0.0, seq=[])]\n",
    "\n",
    "# STEP 1: Expand - try adding each word\n",
    "# Candidates: (score=-1, ['a']), (score=-1, ['b']), (score=-1, ['c']), (score=-1, ['<END>'])\n",
    "# After sorting by score and keeping top-2:\n",
    "# Beam: [(score=-1, ['a']), (score=-1, ['b'])]\n",
    "\n",
    "# STEP 2: Expand each sequence in beam\n",
    "# From ['a']: (score=-2, ['a','a']), (score=-2, ['a','b']), (score=-2, ['a','c']), ...\n",
    "# From ['b']: (score=-2, ['b','a']), (score=-2, ['b','b']), (score=-2, ['b','c']), ...\n",
    "# Top-2: [(score=-2, ['a','a']), (score=-2, ['a','b'])]\n",
    "\n",
    "# RESULT: [(['a', 'a'], -2), (['a', 'b'], -2)]\n",
    "\n",
    "# Why this is better than greedy:\n",
    "# - Greedy would pick ['a'] -> ['a','a'] only, missing other good options\n",
    "# - Beam search explores multiple paths and finds better combinations\n",
    "```\n",
    "\n",
    "### Deducing the Algorithm\n",
    "\n",
    "Now that we understand what beam search does, let's think through how to implement it. Several key design decisions need to be made:\n",
    "\n",
    "**1. How do we maintain the beam?**\n",
    "\n",
    "We need a data structure that can efficiently:\n",
    "- Store (score, sequence) pairs\n",
    "- Extract the top-k items by score\n",
    "- Handle insertions as we generate new candidates\n",
    "\n",
    "**Options:**\n",
    "- Simple list with sorting: Works but O(n log n) each iteration\n",
    "- Priority queue (heap): O(log n) insertions, perfect for our needs\n",
    "- We'll use Python's `heapq` for efficient top-k selection\n",
    "\n",
    "**2. What information must we track?**\n",
    "\n",
    "Each beam entry needs:\n",
    "- The **sequence** itself (list of tokens generated so far)\n",
    "- The **cumulative score** (total score of this partial sequence)\n",
    "\n",
    "Why both? We need the score for ranking but the sequence for extension and final output.\n",
    "\n",
    "**3. When do we move sequences to \"completed\"?**\n",
    "\n",
    "A sequence is complete when:\n",
    "- It ends with the special `<END>` token, OR  \n",
    "- It reaches maximum length\n",
    "\n",
    "These should be separated from active sequences since they shouldn't be extended further.\n",
    "\n",
    "**4. What are the loop termination conditions?**\n",
    "\n",
    "The algorithm terminates when:\n",
    "- We've completed `max_length` iterations, OR\n",
    "- All sequences in the beam are complete (end with `<END>`)\n",
    "\n",
    "At the end, we combine completed sequences with any remaining active sequences and return the top-k.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "**Data Structures: Using `heapq` for Top-K Selection**\n",
    "\n",
    "Python's `heapq` gives us a min-heap. Since we want the highest scores, we can either:\n",
    "- Negate scores (higher scores become lower negative values)\n",
    "- Use `heapq.nlargest()` for final selection\n",
    "\n",
    "```python\n",
    "import heapq\n",
    "candidates = [(-1.0, ['a']), (-0.5, ['b']), (-2.0, ['c'])]\n",
    "heapq.heappush(candidates, (-0.3, ['d']))  # O(log n)\n",
    "best = heapq.heappop(candidates)  # Gets smallest (most negative = highest score)\n",
    "```\n",
    "\n",
    "**Score Tracking: Cumulative vs Normalized**\n",
    "\n",
    "In this basic version, we use **cumulative scores**:\n",
    "- Each token adds to the total score\n",
    "- Longer sequences naturally have lower scores (if using log probabilities)\n",
    "- Later, we'll add length normalization to fix this bias\n",
    "\n",
    "**Handling the End Token**\n",
    "\n",
    "When `<END>` is generated:\n",
    "- The sequence is complete\n",
    "- Move it to a separate \"completed\" list\n",
    "- Don't expand it in future iterations\n",
    "\n",
    "This prevents wasteful generation of sequences like `['hello', '<END>', 'world']`.\n",
    "\n",
    "**Edge Cases**\n",
    "\n",
    "- **beam_width=0**: Return empty list immediately\n",
    "- **Empty vocabulary**: No candidates to generate, return beam as-is\n",
    "- **All sequences complete early**: Stop iterating if beam is empty\n",
    "\n",
    "**Time Complexity:** O(max_length Ã— beam_width Ã— |vocabulary| Ã— log(beam_width Ã— |vocabulary|))\n",
    "- For each of max_length iterations\n",
    "- Expand each of beam_width sequences  \n",
    "- Generate |vocabulary| candidates\n",
    "- Maintain heap of size beam_width Ã— |vocabulary|\n",
    "\n",
    "**Space Complexity:** O(beam_width Ã— max_length)\n",
    "- Store beam_width sequences, each up to max_length tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import heapq\n",
    "\n",
    "class BeamSearch:\n",
    "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.end_token = end_token\n",
    "\n",
    "    def score_sequence(self, sequence: List[str]) -> float:\n",
    "        # In a real scenario, this would be a language model score.\n",
    "        # For this exercise, we use a simple length-based score.\n",
    "        return -len(sequence)\n",
    "\n",
    "    def search(self, beam_width: int, max_length: int) -> List[Tuple[float, List[str]]]:\n",
    "        # TODO: Implement the beam search algorithm.\n",
    "        # Remember to handle the beam, generate candidates, and manage completed sequences.\n",
    "        pass\n",
    "\n",
    "def test_beam_search():\n",
    "    vocabulary = ['a', 'b', 'c', '<END>']\n",
    "    # We are providing a correct implementation here for the sake of the test\n",
    "    class CorrectBeamSearch(BeamSearch):\n",
    "        def search(self, beam_width: int, max_length: int) -> List[Tuple[float, List[str]]]:\n",
    "            if beam_width == 0: return []\n",
    "            beam = [(0.0, [])]  # (score, sequence)\n",
    "            completed_sequences = []\n",
    "            for _ in range(max_length):\n",
    "                new_beam = []\n",
    "                for score, seq in beam:\n",
    "                    if not seq or seq[-1] == self.end_token:\n",
    "                        completed_sequences.append((score, seq))\n",
    "                        continue\n",
    "                    for token in self.vocabulary:\n",
    "                        new_seq = seq + [token]\n",
    "                        new_score = self.score_sequence(new_seq)\n",
    "                        heapq.heappush(new_beam, (new_score, new_seq))\n",
    "                beam.clear()\n",
    "                while new_beam and len(beam) < beam_width:\n",
    "                    score, seq = heapq.heappop(new_beam)\n",
    "                    beam.append((score, seq))\n",
    "            all_sequences = completed_sequences + beam\n",
    "            return sorted(all_sequences, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "    beam_search = CorrectBeamSearch(vocabulary)\n",
    "\n",
    "    # Test 1: Basic search\n",
    "    results = beam_search.search(beam_width=2, max_length=2)\n",
    "    assert len(results) == 2\n",
    "    assert results[0][1] == []\n",
    "\n",
    "    # Test 2: Zero beam width\n",
    "    results = beam_search.search(beam_width=0, max_length=2)\n",
    "    assert len(results) == 0\n",
    "\n",
    "    print(\"ðŸŽ‰ All beam search tests passed!\")\n",
    "\n",
    "test_beam_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 1</summary>\n",
    "\n",
    "**Hint**: Use a priority queue (like Python's `heapq`) to maintain the beam of the top `k` sequences at each step. The priority queue should store tuples of `(score, sequence)`. At each step of the generation, expand each sequence in the beam with all possible next tokens, score the new sequences, and use the priority queue to keep only the top `k`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Top-k Beam Search with Scores (Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "Standard beam search can sometimes produce sequences that are very similar to each other. To encourage more diversity, we can introduce techniques like length normalization and a diversity penalty. Length normalization prevents the search from favoring shorter sequences, while a diversity penalty discourages sequences that are too similar to already selected ones. These techniques are crucial for applications like creative text generation or offering multiple diverse translation options.\n",
    "\n",
    "### Key Concepts\n",
    "- **Length Normalization**: A technique to reduce the bias of beam search towards shorter sequences by dividing the score by the sequence length raised to some power.\n",
    "- **Diversity Penalty**: A penalty applied to the score of a sequence based on its similarity to other sequences in the beam, encouraging more diverse outputs.\n",
    "\n",
    "### Problem Statement\n",
    "Extend the simple beam search to include length normalization and a diversity penalty. You will implement a `TopKBeamSearch` class that generates more diverse and higher-quality sequences.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement length normalization in the scoring function.\n",
    "- Implement a diversity penalty based on n-gram overlap.\n",
    "- Combine these techniques in the search algorithm to produce diverse sequences.\n",
    "\n",
    "### Example: Length Normalization and Diversity Penalty\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "# WITHOUT length normalization: Shorter sequences get higher scores\n",
    "sequences = [\n",
    "    ['hello'],           # score = -1\n",
    "    ['hello', 'world']   # score = -2\n",
    "]\n",
    "# Result: First sequence wins! (score -1 > -2)\n",
    "\n",
    "# WITH length normalization (dividing by length^alpha):\n",
    "# alpha = 0.6 is commonly used\n",
    "sequences_with_norm = [\n",
    "    ['hello'],           # normalized = -1 / (1^0.6) = -1.0\n",
    "    ['hello', 'world']   # normalized = -2 / (2^0.6) = -1.32\n",
    "]\n",
    "# Result: First still wins, but the gap is smaller\n",
    "\n",
    "# DIVERSITY PENALTY - preventing similar sequences:\n",
    "# Sequence 1: ['a', 'b', 'c']  -> bigrams: {('a','b'), ('b','c')}\n",
    "# Sequence 2: ['a', 'b', 'd']  -> bigrams: {('a','b'), ('b','d')}\n",
    "# Overlap: 1 bigram ('a','b')\n",
    "# Diversity penalty = 1 * penalty_weight (e.g., 0.5)\n",
    "\n",
    "# In beam search with diversity:\n",
    "# - First sequence ['a', 'b', 'c'] is selected with score 1.0\n",
    "# - Second sequence ['a', 'b', 'd'] gets penalty for overlapping bigram\n",
    "# - New score = 0.8 - (1 * 0.5) = 0.3\n",
    "# - Different sequence like ['x', 'y', 'z'] with no overlap keeps full score 0.9\n",
    "# Result: More diverse outputs!\n",
    "```\n",
    "\n",
    "### Intuition: The Length Bias Problem\n",
    "\n",
    "Standard beam search has a subtle but serious flaw: **it strongly favors shorter sequences**.\n",
    "\n",
    "**Why does this happen?**\n",
    "\n",
    "In most applications, scores represent log probabilities. Each additional token multiplies another probability (typically < 1) to the sequence:\n",
    "- Sequence \"hello\": P = 0.5, log P = -0.69\n",
    "- Sequence \"hello world\": P = 0.5 Ã— 0.6 = 0.3, log P = -1.20\n",
    "\n",
    "Even though the longer sequence might be better, its score is mathematically guaranteed to be lower (more negative). This creates a systematic bias against longer, potentially better outputs.\n",
    "\n",
    "**The solution: Length Normalization**\n",
    "\n",
    "Instead of comparing raw scores, we normalize by sequence length raised to a power Î± (alpha):\n",
    "\n",
    "```\n",
    "normalized_score = raw_score / (length^Î±)\n",
    "```\n",
    "\n",
    "**Common Î± values:**\n",
    "- Î± = 0.0: No normalization (original bias)\n",
    "- Î± = 0.6: Google's NMT default, mild normalization\n",
    "- Î± = 1.0: Full per-token average, may over-correct\n",
    "\n",
    "Let's see the effect with real numbers:\n",
    "```\n",
    "Sequence A: \"hello\" \n",
    "  Raw score: -0.69\n",
    "  Normalized (Î±=0.6): -0.69 / (1^0.6) = -0.69\n",
    "\n",
    "Sequence B: \"hello world today\"\n",
    "  Raw score: -2.1  (looks worse!)\n",
    "  Normalized (Î±=0.6): -2.1 / (3^0.6) = -2.1 / 2.05 = -1.02\n",
    "  \n",
    "After normalization, we can fairly compare: -0.69 vs -1.02\n",
    "```\n",
    "\n",
    "Without normalization, beam search often produces terse, incomplete outputs. Length normalization levels the playing field.\n",
    "\n",
    "### Deducing the Diversity Penalty\n",
    "\n",
    "Even with length normalization, beam search has another problem: **it produces very similar outputs**.\n",
    "\n",
    "**Why?** The beam often gets dominated by sequences with identical prefixes:\n",
    "```\n",
    "Beam after step 5:\n",
    "  1. \"I want to go to\"\n",
    "  2. \"I want to go home\"\n",
    "  3. \"I want to go there\"\n",
    "```\n",
    "\n",
    "All three sequences share the prefix \"I want to go\", so they're not giving us much diversity. For applications like offering translation alternatives or creative generation, this is problematic.\n",
    "\n",
    "**The diversity penalty solution:**\n",
    "\n",
    "We penalize sequences based on their similarity to sequences already in the beam. The idea:\n",
    "1. Calculate n-gram overlap between candidate and existing beam sequences\n",
    "2. Subtract a penalty proportional to this overlap\n",
    "3. This encourages new candidates to differ from what we already have\n",
    "\n",
    "**N-gram overlap as a similarity metric:**\n",
    "\n",
    "Bigrams (2-grams) work well because they capture local structure:\n",
    "- \"the cat sat\" â†’ bigrams: {(the, cat), (cat, sat)}\n",
    "- \"the dog sat\" â†’ bigrams: {(the, dog), (dog, sat)}\n",
    "- Overlap: 0 bigrams (0% similar despite sharing 2 words!)\n",
    "\n",
    "```python\n",
    "seq1_bigrams = {(the, cat), (cat, sat)}\n",
    "seq2_bigrams = {(the, cat), (cat, runs)}\n",
    "overlap = len(seq1_bigrams âˆ© seq2_bigrams) = 1  # just (the, cat)\n",
    "penalty = overlap Ã— penalty_weight = 1 Ã— 0.5 = 0.5\n",
    "```\n",
    "\n",
    "**Balancing quality and diversity:**\n",
    "\n",
    "The `penalty_weight` parameter controls the trade-off:\n",
    "- **Low weight (0.1-0.3)**: Slight diversity boost, mostly quality-focused\n",
    "- **Medium weight (0.5-0.7)**: Balanced approach\n",
    "- **High weight (0.8-1.0)**: Aggressive diversity, may sacrifice quality\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "**Computing N-grams Efficiently with `zip()`**\n",
    "\n",
    "Python's `zip()` makes bigram extraction elegant:\n",
    "\n",
    "```python\n",
    "sequence = ['the', 'cat', 'sat']\n",
    "bigrams = list(zip(sequence, sequence[1:]))\n",
    "# Result: [('the', 'cat'), ('cat', 'sat')]\n",
    "\n",
    "# For sets (fast intersection):\n",
    "bigram_set = set(zip(sequence, sequence[1:]))\n",
    "```\n",
    "\n",
    "The `zip(sequence, sequence[1:])` idiom pairs each element with its successor - perfect for bigrams.\n",
    "\n",
    "**When to Apply Penalties: Scoring vs Selection**\n",
    "\n",
    "Two approaches:\n",
    "\n",
    "1. **During scoring** (recommended):\n",
    "   - Calculate penalty when scoring each candidate\n",
    "   - Penalty = similarity to all current beam sequences\n",
    "   - Pro: Natural integration into score-based selection\n",
    "   - Con: Must recalculate if beam changes\n",
    "\n",
    "2. **During selection**:\n",
    "   - Generate all candidates first\n",
    "   - Apply penalties only to top candidates\n",
    "   - Pro: Fewer penalty calculations\n",
    "   - Con: May miss diverse candidates that scored lower initially\n",
    "\n",
    "Most implementations use approach #1 for better diversity.\n",
    "\n",
    "**Tuning the Penalty Weight Parameter**\n",
    "\n",
    "The penalty weight is application-specific:\n",
    "\n",
    "- **Machine translation**: Low (0.2-0.4) - accuracy matters most\n",
    "- **Creative writing**: High (0.7-1.0) - diversity is the goal  \n",
    "- **Summarization**: Medium (0.4-0.6) - balance coverage and accuracy\n",
    "\n",
    "Start with 0.5 and adjust based on output inspection. If outputs are too similar, increase; if quality drops, decrease.\n",
    "\n",
    "**Time Complexity Addition:** O(beam_widthÂ² Ã— max_length) for diversity calculations\n",
    "- For each new candidate, compare against beam_width existing sequences\n",
    "- Each comparison iterates through up to max_length bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "class TopKBeamSearch(BeamSearch):\n",
    "    def score_sequence(self, sequence: List[str], length_penalty: float = 0.6) -> float:\n",
    "        # TODO: Implement length-normalized scoring\n",
    "        pass\n",
    "\n",
    "    def calculate_diversity_penalty(self, sequence: List[str], existing_sequences: List[List[str]], penalty_weight: float = 0.5) -> float:\n",
    "        # TODO: Implement diversity penalty calculation based on n-gram overlap\n",
    "        pass\n",
    "\n",
    "    def search(self, beam_width: int, max_length: int, diversity_penalty: float = 0.5) -> List[Tuple[float, List[str]]]:\n",
    "        # TODO: Implement the search method incorporating the new scoring and penalty functions\n",
    "        pass\n",
    "\n",
    "def test_top_k_beam_search():\n",
    "    vocabulary = ['a', 'b', 'c', 'd', '<END>']\n",
    "    # Correct implementation for testing purposes\n",
    "    class CorrectTopK(TopKBeamSearch):\n",
    "        def score_sequence(self, sequence: List[str], length_penalty: float = 0.6) -> float:\n",
    "            raw_score = -len(sequence) * 0.5\n",
    "            return raw_score / (len(sequence)**length_penalty) if sequence else 0\n",
    "        def calculate_diversity_penalty(self, sequence: List[str], existing_sequences: List[List[str]], penalty_weight: float = 0.5) -> float:\n",
    "            penalty = 0.0\n",
    "            for existing_seq in existing_sequences:\n",
    "                seq_bigrams = set(zip(sequence, sequence[1:]))\n",
    "                existing_bigrams = set(zip(existing_seq, existing_seq[1:]))\n",
    "                overlap = len(seq_bigrams.intersection(existing_bigrams))\n",
    "                penalty += overlap * penalty_weight\n",
    "            return penalty\n",
    "        def search(self, beam_width: int, max_length: int, diversity_penalty: float = 0.5) -> List[Tuple[float, List[str]]]:\n",
    "            beam = [(0.0, [])]\n",
    "            completed = []\n",
    "            for _ in range(max_length):\n",
    "                new_beam = []\n",
    "                for score, seq in beam:\n",
    "                    if not seq or seq[-1] == self.end_token: continue\n",
    "                    for token in self.vocabulary:\n",
    "                        new_seq = seq + [token]\n",
    "                        new_score = self.score_sequence(new_seq) - self.calculate_diversity_penalty(new_seq, [s for _, s in beam], diversity_penalty)\n",
    "                        heapq.heappush(new_beam, (new_score, new_seq))\n",
    "                beam.clear()\n",
    "                while new_beam and len(beam) < beam_width:\n",
    "                    score, seq = heapq.heappop(new_beam)\n",
    "                    if seq[-1] == self.end_token: completed.append((score, seq))\n",
    "                    else: beam.append((score, seq))\n",
    "            return sorted(completed + beam, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "    top_k_search = CorrectTopK(vocabulary)\n",
    "\n",
    "    # Test 1: With diversity penalty, we should get more diverse results\n",
    "    results_diverse = top_k_search.search(beam_width=3, max_length=3, diversity_penalty=1.0)\n",
    "    results_no_diversity = top_k_search.search(beam_width=3, max_length=3, diversity_penalty=0.0)\n",
    "\n",
    "    assert results_diverse[0][1] != results_no_diversity[0][1]\n",
    "\n",
    "    print(\"ðŸŽ‰ All top-k beam search tests passed!\")\n",
    "\n",
    "test_top_k_beam_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 2</summary>\n",
    "\n",
    "**Hint**: For length normalization, divide the sequence score by `len(sequence)**alpha`, where `alpha` is the length penalty factor. For the diversity penalty, you can calculate the n-gram overlap between a candidate sequence and the sequences already in the beam. Subtract a penalty proportional to this overlap from the candidate's score.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Viterbi Algorithm for Sequence Tagging (Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "Part-of-speech (POS) tagging is a classic NLP task where we assign a grammatical tag (like noun, verb, or adjective) to each word in a sentence. A simple approach of picking the most likely tag for each word independently can fail because it ignores the context (e.g., \"the fish\" is more likely than \"the and\"). The Viterbi algorithm, a dynamic programming method, solves this by finding the most likely sequence of tags given the sequence of words. It's used in Hidden Markov Models (HMMs) and is fundamental to many sequence labeling tasks.\n",
    "\n",
    "### Key Concepts\n",
    "- **Hidden Markov Model (HMM)**: A statistical model with hidden states (the tags) and observable outputs (the words).\n",
    "- **Transition Probability**: The probability of moving from one state to another, P(tag_i | tag_{i-1}).\n",
    "- **Emission Probability**: The probability of observing a word given a state, P(word_i | tag_i).\n",
    "- **Dynamic Programming**: The Viterbi algorithm uses a table to store the maximum probability of being in a certain state at a certain time, avoiding re-computation.\n",
    "\n",
    "### Problem Statement\n",
    "Implement the Viterbi algorithm for a simple POS tagger. You will be given the transition, emission, and initial probabilities of an HMM. Your task is to find the most likely sequence of POS tags for a given sentence.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement the Viterbi algorithm using dynamic programming.\n",
    "- Use logarithms to prevent underflow with small probabilities.\n",
    "- Handle unknown words with a simple smoothing technique.\n",
    "- Reconstruct the most likely path of tags.\n",
    "\n",
    "### Example: Manual Viterbi Computation\n",
    "\n",
    "```python\n",
    "# Let's trace through a simple example: sentence = ['the', 'cat', 'sat']\n",
    "# Tags: DET (determiner), NOUN (noun), VERB (verb)\n",
    "\n",
    "# Transition probabilities P(tag_i | tag_{i-1}):\n",
    "#           DET   NOUN  VERB\n",
    "#  DET:    [0.1   0.8   0.1]\n",
    "#  NOUN:   [0.3   0.1   0.6]\n",
    "#  VERB:   [0.2   0.7   0.1]\n",
    "\n",
    "# Emission probabilities P(word | tag):\n",
    "#         'the'  'cat'  'sat'\n",
    "#  DET:  [0.9    0.05   0.05]\n",
    "#  NOUN: [0.05   0.9    0.05]\n",
    "#  VERB: [0.05   0.05   0.9]\n",
    "\n",
    "# Initial probabilities: [0.8 (DET), 0.1 (NOUN), 0.1 (VERB)]\n",
    "\n",
    "# STEP 1: Process word 'the'\n",
    "# For each tag, compute: initial_prob * emission_prob['the']\n",
    "# DET:  0.8 * 0.9 = 0.72   <- best\n",
    "# NOUN: 0.1 * 0.05 = 0.005\n",
    "# VERB: 0.1 * 0.05 = 0.005\n",
    "# Viterbi table: [0.72, 0.005, 0.005]\n",
    "\n",
    "# STEP 2: Process word 'cat'\n",
    "# For NOUN tag: max(\n",
    "#     0.72 * 0.8 * 0.9,     <- from DET\n",
    "#     0.005 * 0.1 * 0.9,    <- from NOUN\n",
    "#     0.005 * 0.7 * 0.9     <- from VERB\n",
    "# ) = max(0.518, 0.0005, 0.0032) = 0.518  <- from DET\n",
    "# Similarly for other tags...\n",
    "\n",
    "# STEP 3: Process word 'sat'\n",
    "# Similar computation, tracking best path\n",
    "\n",
    "# FINAL RESULT: ['DET', 'NOUN', 'VERB']\n",
    "# Because this path has the highest joint probability\n",
    "```\n",
    "\n",
    "### Intuition: Why Dynamic Programming?\n",
    "\n",
    "At first glance, finding the best tag sequence seems straightforward: just try all possibilities and pick the best. But let's examine the problem size:\n",
    "\n",
    "**The Exponential Explosion:**\n",
    "\n",
    "For a sentence with T words and N possible tags:\n",
    "- Word 1: N choices\n",
    "- Word 2: N choices  \n",
    "- Word 3: N choices\n",
    "- Total: N^T possible sequences\n",
    "\n",
    "For just 10 words with 45 POS tags (typical English tagset): 45^10 â‰ˆ 3.4 Ã— 10^16 possibilities!\n",
    "\n",
    "Even at 1 billion sequences/second, this would take ~1 million years to enumerate.\n",
    "\n",
    "**The Key Insight: Optimal Substructure**\n",
    "\n",
    "Here's the crucial observation that makes dynamic programming possible:\n",
    "\n",
    "> The best tag sequence ending at word i with tag t depends ONLY on the best tag sequence ending at word i-1 (not the entire history).\n",
    "\n",
    "Why? Because of the Markov property in HMMs: the probability of the current tag depends only on the previous tag, not the entire sequence before it.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Sentence: \"the cat sat\"\n",
    "Best path to \"cat\" ending in NOUN:\n",
    "  [DET, NOUN] with probability 0.518\n",
    "\n",
    "To extend to \"sat\", we only need:\n",
    "  - The probability 0.518 from the previous step\n",
    "  - Transition NOUN â†’ VERB\n",
    "  - Emission probability of \"sat\" given VERB\n",
    "\n",
    "We don't need to know it was specifically \"the\" before \"cat\" - \n",
    "just that we arrived at NOUN with probability 0.518.\n",
    "```\n",
    "\n",
    "This optimal substructure allows us to build the solution incrementally, reducing complexity from O(N^T) to O(T Ã— N^2).\n",
    "\n",
    "**Comparison to Naive Enumeration:**\n",
    "\n",
    "```\n",
    "Naive approach (3 words, 3 tags):\n",
    "  DET-DET-DET, DET-DET-NOUN, DET-DET-VERB,\n",
    "  DET-NOUN-DET, DET-NOUN-NOUN, ...\n",
    "  Total: 27 complete sequences evaluated\n",
    "\n",
    "Viterbi (3 words, 3 tags):\n",
    "  Step 1: 3 calculations (one per tag)\n",
    "  Step 2: 9 calculations (3 tags Ã— 3 previous)  \n",
    "  Step 3: 9 calculations\n",
    "  Total: 21 partial sequences evaluated (and reused!)\n",
    "```\n",
    "\n",
    "The difference becomes dramatic as T grows: 27 vs 21 for T=3, but 59,049 vs 39 for T=10!\n",
    "\n",
    "### Deducing the DP Table\n",
    "\n",
    "Let's design the dynamic programming table step by step.\n",
    "\n",
    "**What do we need to store?**\n",
    "\n",
    "At each position, we need to know: \"What's the highest probability of reaching this word with this tag?\"\n",
    "\n",
    "**Table Dimensions: [num_words Ã— num_tags]**\n",
    "\n",
    "```\n",
    "        DET    NOUN   VERB\n",
    "the    [0.72   0.005  0.005]  <- probabilities for word 1\n",
    "cat    [0.036  0.518  0.003]  <- probabilities for word 2  \n",
    "sat    [0.011  0.018  0.280]  <- probabilities for word 3\n",
    "```\n",
    "\n",
    "**What each cell represents:**\n",
    "\n",
    "`table[word_i][tag_j]` = Maximum probability of ANY tag sequence of length i that ends with tag_j at word_i.\n",
    "\n",
    "For example, `table[1][NOUN] = 0.518` means:\n",
    "- \"The best way to reach word 1 ('cat') with tag NOUN has probability 0.518\"\n",
    "- That path happened to be [DET, NOUN], but we don't store that yet\n",
    "\n",
    "**Why we need backpointers:**\n",
    "\n",
    "The table tells us the PROBABILITY of the best path, but not the path itself. We need a separate backpointer table:\n",
    "\n",
    "```\n",
    "Backpointers:     DET    NOUN   VERB\n",
    "the              [-1     -1     -1]      <- no previous tag (start)\n",
    "cat              [DET    DET    DET]     <- best previous tag for each\n",
    "sat              [DET    NOUN   NOUN]    <- arrows pointing backward\n",
    "```\n",
    "\n",
    "`backpointer[word_i][tag_j]` = Which tag at word i-1 gave us the best path to tag_j at word_i?\n",
    "\n",
    "**The DP Recurrence Relation:**\n",
    "\n",
    "For each word i and tag j:\n",
    "\n",
    "```\n",
    "table[i][j] = max over all tags k of:\n",
    "    table[i-1][k] Ã— transition[kâ†’j] Ã— emission[word_i|j]\n",
    "```\n",
    "\n",
    "In log space (to prevent underflow):\n",
    "\n",
    "```\n",
    "table[i][j] = max over all tags k of:\n",
    "    table[i-1][k] + log_transition[kâ†’j] + log_emission[word_i|j]\n",
    "```\n",
    "\n",
    "**Forward Pass vs Backward Reconstruction:**\n",
    "\n",
    "1. **Forward pass** (filling the table):\n",
    "   - Start: word 0, use initial probabilities\n",
    "   - Iterate: words 1 to T-1, apply recurrence relation\n",
    "   - End: maximum value in table[T-1][:] is best probability\n",
    "\n",
    "2. **Backward reconstruction** (following backpointers):\n",
    "   - Start: argmax of table[T-1][:] gives last tag\n",
    "   - Iterate: follow backpointers from end to start\n",
    "   - End: complete sequence of tags\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "**Using Log Probabilities to Prevent Underflow**\n",
    "\n",
    "Probabilities multiply: 0.8 Ã— 0.7 Ã— 0.6 Ã— 0.5 = 0.168\n",
    "\n",
    "But small probabilities underflow quickly:\n",
    "```python\n",
    "prob = 0.1 ** 50  # Result: 1e-50, close to machine epsilon!\n",
    "```\n",
    "\n",
    "**Solution:** Use logarithms to convert products to sums:\n",
    "```python\n",
    "log(a Ã— b Ã— c) = log(a) + log(b) + log(c)\n",
    "log(0.8 Ã— 0.7) = -0.22 + (-0.36) = -0.58\n",
    "```\n",
    "\n",
    "Log probabilities are negative (since probs < 1), and we want to maximize (least negative = highest prob).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "# Add small epsilon before log to handle zeros\n",
    "log_prob = np.log(prob + 1e-10)\n",
    "```\n",
    "\n",
    "**Handling Unknown Words with Smoothing**\n",
    "\n",
    "When a word isn't in the vocabulary, we can't look up its emission probability. Simple solutions:\n",
    "\n",
    "1. **Uniform smoothing**: Assign equal small probability to all tags\n",
    "   ```python\n",
    "   if word not in vocabulary:\n",
    "       emission = np.full(num_tags, 1e-10)\n",
    "   ```\n",
    "\n",
    "2. **Open class assumption**: Give unknown words higher probability for NOUN, VERB (content words)\n",
    "\n",
    "3. **Character-level features** (more advanced): Use suffix patterns like \"-ing\" suggests VERB\n",
    "\n",
    "**Backtracking to Reconstruct the Path**\n",
    "\n",
    "After filling the table, we have the best score but not the sequence:\n",
    "\n",
    "```python\n",
    "# Find best final tag\n",
    "best_final_prob = np.max(table[T-1, :])\n",
    "best_final_tag = np.argmax(table[T-1, :])\n",
    "\n",
    "# Trace backward\n",
    "path = [best_final_tag]\n",
    "for t in range(T-1, 0, -1):\n",
    "    prev_tag = backpointer[t, path[0]]\n",
    "    path.insert(0, prev_tag)  # prepend\n",
    "```\n",
    "\n",
    "**Time Complexity: O(T Ã— N^2)**\n",
    "\n",
    "- T words in the sentence\n",
    "- For each word, iterate through N tags (outer loop)\n",
    "- For each tag, check N possible previous tags (inner loop)\n",
    "- Result: T Ã— N Ã— N = O(T Ã— N^2)\n",
    "\n",
    "For a 20-word sentence with 45 tags: 20 Ã— 45^2 = 40,500 operations (instant!)\n",
    "\n",
    "Compare to naive O(N^T) = 45^20 â‰ˆ 10^33 operations (impossible!)\n",
    "\n",
    "**Space Complexity: O(T Ã— N)**\n",
    "\n",
    "- Viterbi table: T Ã— N\n",
    "- Backpointer table: T Ã— N\n",
    "- Total: 2 Ã— T Ã— N = O(T Ã— N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class HMMTagger:\n",
    "    def __init__(self, tags: List[str], words: List[str], initial_prob: np.ndarray, transition_prob: np.ndarray, emission_prob: np.ndarray):\n",
    "        self.tags = tags\n",
    "        self.words = words\n",
    "        self.tag_to_idx = {tag: i for i, tag in enumerate(tags)}\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "        self.log_initial = np.log(initial_prob + 1e-10)\n",
    "        self.log_transition = np.log(transition_prob + 1e-10)\n",
    "        self.log_emission = np.log(emission_prob + 1e-10)\n",
    "\n",
    "    def viterbi(self, sentence: List[str]) -> Tuple[List[str], float]:\n",
    "        # TODO: Implement the Viterbi algorithm, including initialization, recursion, and backtracking.\n",
    "        pass\n",
    "\n",
    "def test_viterbi_algorithm():\n",
    "    tags = ['DET', 'NOUN', 'VERB']\n",
    "    words = ['the', 'cat', 'sat']\n",
    "    initial_prob = np.array([0.8, 0.1, 0.1])\n",
    "    transition_prob = np.array([[0.1, 0.8, 0.1], [0.3, 0.1, 0.6], [0.2, 0.7, 0.1]])\n",
    "    emission_prob = np.array([[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
    "\n",
    "    class CorrectHMM(HMMTagger):\n",
    "        def viterbi(self, sentence: List[str]) -> Tuple[List[str], float]:\n",
    "            T = len(sentence); N = len(self.tags)\n",
    "            if T == 0: return [], 0.0\n",
    "            viterbi_table = np.zeros((T, N)); backpointer = np.zeros((T, N), dtype=int)\n",
    "            word_idx = self.word_to_idx.get(sentence[0], -1)\n",
    "            emission = self.log_emission[:, word_idx] if word_idx != -1 else np.log(np.full(N, 1e-10))\n",
    "            viterbi_table[0, :] = self.log_initial + emission\n",
    "            for t in range(1, T):\n",
    "                word_idx = self.word_to_idx.get(sentence[t], -1)\n",
    "                emission = self.log_emission[:, word_idx] if word_idx != -1 else np.log(np.full(N, 1e-10))\n",
    "                for s in range(N):\n",
    "                    trans_probs = viterbi_table[t-1, :] + self.log_transition[:, s]\n",
    "                    viterbi_table[t, s] = np.max(trans_probs) + emission[s]\n",
    "                    backpointer[t, s] = np.argmax(trans_probs)\n",
    "            best_prob = np.max(viterbi_table[T-1, :]); last_state = np.argmax(viterbi_table[T-1, :])\n",
    "            path = [self.tags[last_state]]\n",
    "            for t in range(T - 1, 0, -1):\n",
    "                last_state = backpointer[t, last_state]\n",
    "                path.insert(0, self.tags[last_state])\n",
    "            return path, np.exp(best_prob)\n",
    "\n",
    "    tagger = CorrectHMM(tags, words, initial_prob, transition_prob, emission_prob)\n",
    "\n",
    "    # Test 1: A likely sequence\n",
    "    sentence1 = ['the', 'cat', 'sat']\n",
    "    path, prob = tagger.viterbi(sentence1)\n",
    "    assert path == ['DET', 'NOUN', 'VERB']\n",
    "\n",
    "    # Test 2: A sequence with an unknown word\n",
    "    sentence2 = ['the', 'dog', 'sat']\n",
    "    path, prob = tagger.viterbi(sentence2)\n",
    "    assert path == ['DET', 'NOUN', 'VERB']\n",
    "\n",
    "    print(\"ðŸŽ‰ All Viterbi algorithm tests passed!\")\n",
    "\n",
    "test_viterbi_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 3</summary>\n",
    "\n",
    "**Hint**: Create a dynamic programming table of size `(num_words, num_tags)`. Each cell `(i, j)` will store the maximum probability of a tag sequence of length `i` ending with tag `j`. Also, create a `backpointer` table to store the path. Iterate through the words and for each word, calculate the probabilities for each tag based on the previous word's tag probabilities and the transition probabilities. After filling the table, backtrack from the end to find the most likely path.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Constrained Beam Search (Medium-Hard)\n",
    "\n",
    "### Contextual Introduction\n",
    "In many real-world applications, we need to generate text that adheres to certain rules. For example, a chatbot must avoid generating toxic language, or a text summarization model might be required to include certain keywords. Constrained beam search extends the standard algorithm by pruning partial sequences that violate predefined constraints, ensuring that the final output meets the requirements.\n",
    "\n",
    "### Key Concepts\n",
    "- **Constraint Satisfaction**: The process of finding a solution that satisfies a set of constraints.\n",
    "- **Hard Constraints vs. Soft Constraints**: Hard constraints must be satisfied, while soft constraints are desirable but not mandatory.\n",
    "- **Pruning**: The process of eliminating partial solutions that cannot lead to a valid final solution.\n",
    "\n",
    "### Problem Statement\n",
    "Implement a constrained beam search algorithm that can handle two types of constraints: `MustContainConstraint` and `MustNotContainConstraint`. You will integrate these constraints into the beam search process to generate sequences that satisfy all given rules.\n",
    "\n",
    "**Requirements**:\n",
    "- Design an abstract `Constraint` class and specific implementations for `MustContainConstraint` and `MustNotContainConstraint`.\n",
    "- Integrate constraint checking into the beam search algorithm.\n",
    "- Prune the search space by discarding partial sequences that violate constraints.\n",
    "\n",
    "### Example: How Constraints Filter Beam Search\n",
    "\n",
    "```python\n",
    "# Vocabulary: ['a', 'b', 'c', 'toxic', '<END>']\n",
    "# Beam width: 2\n",
    "# Constraint: MustNotContainConstraint('toxic')\n",
    "\n",
    "# STEP 1: Generate candidates\n",
    "# Candidates: (score=-1, ['a']), (score=-1, ['b']), (score=-1, ['toxic']), (score=-1, ['c'])\n",
    "\n",
    "# STEP 2: Apply constraints\n",
    "# Check each candidate:\n",
    "# - ['a']: Contains 'toxic'? No -> KEEP\n",
    "# - ['b']: Contains 'toxic'? No -> KEEP\n",
    "# - ['toxic']: Contains 'toxic'? Yes -> PRUNE (remove)\n",
    "# - ['c']: Contains 'toxic'? No -> KEEP\n",
    "\n",
    "# STEP 3: Select top-2 after filtering\n",
    "# Beam: [(score=-1, ['a']), (score=-1, ['b'])]\n",
    "\n",
    "# STEP 4: Continue expanding only valid sequences\n",
    "# From ['a']: ['a','a'], ['a','b'], ['a','c'] (not ['a','toxic']!)\n",
    "# From ['b']: ['b','a'], ['b','b'], ['b','c'] (not ['b','toxic']!)\n",
    "\n",
    "# FINAL RESULT: Only sequences without 'toxic' are generated\n",
    "# This is much more efficient than generating toxic sequences and filtering later!\n",
    "```\n",
    "\n",
    "### Intuition: Early Pruning\n",
    "\n",
    "Consider two approaches to ensuring generated text doesn't contain toxic language:\n",
    "\n",
    "**Approach 1: Generate then Filter**\n",
    "```\n",
    "1. Run beam search normally\n",
    "2. Generate 1000 candidate sequences\n",
    "3. Filter out sequences containing banned words\n",
    "4. Result: Maybe 200 sequences remain\n",
    "```\n",
    "\n",
    "**Approach 2: Prune During Search (Constrained Beam Search)**\n",
    "```\n",
    "1. At each step, check if adding a token violates constraints\n",
    "2. Don't add invalid candidates to the beam\n",
    "3. Result: Never waste time exploring invalid paths\n",
    "```\n",
    "\n",
    "**Why is early pruning so much better?**\n",
    "\n",
    "Imagine the search space as a tree with 1000 branches at each level:\n",
    "- Without pruning: Explore toxic branches for 5-10 steps, then discard (wasted computation)\n",
    "- With pruning: Cut toxic branches immediately at step 1 (10,000Ã— less work!)\n",
    "\n",
    "**Real-world example:**\n",
    "\n",
    "```\n",
    "Generating chatbot response, max_length=20, beam_width=10, vocab_size=10000\n",
    "\n",
    "WITHOUT constraints:\n",
    "  Step 1: 10,000 candidates â†’ keep 10\n",
    "  Step 2: 100,000 candidates â†’ keep 10\n",
    "  ...\n",
    "  Step 20: Generate millions of sequences, then filter\n",
    "  \n",
    "WITH constraints (pruning toxic words):\n",
    "  Step 1: 10,000 candidates â†’ filter 50 toxic â†’ keep 10 safe ones\n",
    "  Step 2: Only expand 10 safe sequences (not toxic branches!)\n",
    "  ...\n",
    "  Step 20: Never explored toxic paths, saved ~90% computation\n",
    "```\n",
    "\n",
    "**How constraints reduce the search space:**\n",
    "\n",
    "Think of constraints as pruning shears in a search tree:\n",
    "- Early cuts prevent entire subtrees from being explored\n",
    "- The earlier you cut, the more you save (exponential savings)\n",
    "- Each constraint violation detected at depth d saves vocab_size^(max_depth-d) candidate evaluations\n",
    "\n",
    "### Deducing the Design\n",
    "\n",
    "How should we design a system that supports multiple types of constraints?\n",
    "\n",
    "**Design Decision 1: Abstract Constraint Interface**\n",
    "\n",
    "We want to support many constraint types:\n",
    "- MustContain: \"Response must include the user's name\"\n",
    "- MustNotContain: \"No profanity allowed\"\n",
    "- MaxLength: \"Keep it under 50 characters\"\n",
    "- RequireFormat: \"Must be valid JSON\"\n",
    "\n",
    "**Solution:** Abstract base class with a single interface:\n",
    "\n",
    "```python\n",
    "class Constraint(ABC):\n",
    "    @abstractmethod\n",
    "    def check(self, sequence: List[str]) -> bool:\n",
    "        \"\"\"Returns True if sequence satisfies constraint\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "This allows us to add new constraints without modifying the beam search code!\n",
    "\n",
    "**Design Decision 2: When to Check Constraints?**\n",
    "\n",
    "Three options:\n",
    "\n",
    "1. **Check every candidate** (recommended):\n",
    "   - During beam expansion, test each new sequence\n",
    "   - Pro: Maximum pruning, catches violations immediately\n",
    "   - Con: Most constraint checks\n",
    "   \n",
    "2. **Check every step** (too early):\n",
    "   - Before expanding, check current beam\n",
    "   - Pro: Fewer checks\n",
    "   - Con: Misses violations introduced by next token\n",
    "\n",
    "3. **Check at the end** (too late):\n",
    "   - After search completes, filter results\n",
    "   - Pro: Fewest checks\n",
    "   - Con: No pruning benefit at all!\n",
    "\n",
    "Option 1 is best: we check each candidate right after generation, before adding to the beam.\n",
    "\n",
    "**Design Decision 3: Combining Multiple Constraints**\n",
    "\n",
    "When we have multiple constraints, how do we combine them?\n",
    "\n",
    "**AND Logic (all must be satisfied):**\n",
    "```python\n",
    "valid = all(constraint.check(seq) for constraint in constraints)\n",
    "```\n",
    "\n",
    "This is almost always what we want. The sequence must satisfy ALL constraints.\n",
    "\n",
    "**OR Logic (at least one must be satisfied):**\n",
    "```python\n",
    "valid = any(constraint.check(seq) for constraint in constraints)\n",
    "```\n",
    "\n",
    "Rarely used, but could be implemented with a `CompositeConstraint` class if needed.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "**Using Abstract Base Class for `Constraint`**\n",
    "\n",
    "Python's `abc` module provides clean interface definition:\n",
    "\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Constraint(ABC):\n",
    "    @abstractmethod\n",
    "    def check(self, sequence: List[str]) -> bool:\n",
    "        pass\n",
    "\n",
    "# Concrete implementations:\n",
    "class MustContainConstraint(Constraint):\n",
    "    def __init__(self, required_token: str):\n",
    "        self.required_token = required_token\n",
    "    \n",
    "    def check(self, sequence: List[str]) -> bool:\n",
    "        return self.required_token in sequence\n",
    "\n",
    "class MustNotContainConstraint(Constraint):\n",
    "    def __init__(self, forbidden_token: str):\n",
    "        self.forbidden_token = forbidden_token\n",
    "    \n",
    "    def check(self, sequence: List[str]) -> bool:\n",
    "        return self.forbidden_token not in sequence\n",
    "```\n",
    "\n",
    "The key insight: both implement the same `check()` interface but with different logic.\n",
    "\n",
    "**Constraint Checking in the Expansion Loop**\n",
    "\n",
    "Integration point in beam search:\n",
    "\n",
    "```python\n",
    "for score, seq in beam:\n",
    "    for token in vocabulary:\n",
    "        new_seq = seq + [token]\n",
    "        \n",
    "        # CHECK CONSTRAINTS HERE (before scoring/adding to beam)\n",
    "        if all(constraint.check(new_seq) for constraint in self.constraints):\n",
    "            new_score = self.score_sequence(new_seq)\n",
    "            heapq.heappush(new_beam, (new_score, new_seq))\n",
    "        # Else: skip this candidate entirely (pruned!)\n",
    "```\n",
    "\n",
    "Notice: we never score or consider sequences that violate constraints.\n",
    "\n",
    "**Performance Implications of Constraint Complexity**\n",
    "\n",
    "Different constraints have different computational costs:\n",
    "\n",
    "1. **MustNotContain**: O(sequence_length)\n",
    "   - Simple membership check: `token in sequence`\n",
    "   - Very fast, negligible overhead\n",
    "\n",
    "2. **MustContain**: O(sequence_length)  \n",
    "   - Same as MustNotContain\n",
    "   - But may allow fewer valid sequences (more pruning)\n",
    "\n",
    "3. **Pattern matching** (regex): O(sequence_length Ã— pattern_complexity)\n",
    "   - More expensive, but still usually acceptable\n",
    "   - Example: \"Must match email format\"\n",
    "\n",
    "4. **External validation** (API call): O(network_latency)\n",
    "   - Very expensive! Avoid in inner loop\n",
    "   - Example: \"Check if URL is valid\" - better to validate after search\n",
    "\n",
    "**Best practice:** Keep constraint checks lightweight. If validation is expensive, do it once after search rather than for every candidate.\n",
    "\n",
    "**Constraint Violation Tracking (Optional Enhancement)**\n",
    "\n",
    "For debugging, you might track why sequences were rejected:\n",
    "\n",
    "```python\n",
    "class ConstraintViolationTracker:\n",
    "    def __init__(self):\n",
    "        self.violations = defaultdict(int)\n",
    "    \n",
    "    def check_and_track(self, seq, constraints):\n",
    "        for constraint in constraints:\n",
    "            if not constraint.check(seq):\n",
    "                self.violations[constraint.__class__.__name__] += 1\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "# Analysis:\n",
    "# MustNotContainConstraint: 1,245 sequences rejected\n",
    "# MustContainConstraint: 89 sequences rejected\n",
    "```\n",
    "\n",
    "This helps identify which constraints are most restrictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Set\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Constraint(ABC):\n",
    "    @abstractmethod\n",
    "    def check(self, sequence: List[str]) -> bool:\n",
    "        pass\n",
    "\n",
    "class MustContainConstraint(Constraint):\n",
    "    def __init__(self, required_token: str):\n",
    "        self.required_token = required_token\n",
    "\n",
    "    def check(self, sequence: List[str]) -> bool:\n",
    "        # TODO: Implement the check for must-contain constraint\n",
    "        pass\n",
    "\n",
    "class MustNotContainConstraint(Constraint):\n",
    "    def __init__(self, forbidden_token: str):\n",
    "        self.forbidden_token = forbidden_token\n",
    "\n",
    "    def check(self, sequence: List[str]) -> bool:\n",
    "        # TODO: Implement the check for must-not-contain constraint\n",
    "        pass\n",
    "\n",
    "class ConstrainedBeamSearch(BeamSearch):\n",
    "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
    "        super().__init__(vocabulary, end_token)\n",
    "        self.constraints = []\n",
    "\n",
    "    def add_constraint(self, constraint: Constraint):\n",
    "        self.constraints.append(constraint)\n",
    "\n",
    "    def search(self, beam_width: int, max_length: int) -> List[Tuple[float, List[str]]]:\n",
    "        # TODO: Implement the constrained beam search\n",
    "        pass\n",
    "\n",
    "def test_constrained_beam_search():\n",
    "    vocabulary = ['a', 'b', 'c', 'd', '<END>']\n",
    "    class CorrectConstrained(ConstrainedBeamSearch):\n",
    "        def search(self, beam_width: int, max_length: int) -> List[Tuple[float, List[str]]]:\n",
    "            beam = [(0.0, [])]\n",
    "            completed = []\n",
    "            for _ in range(max_length):\n",
    "                new_beam = []\n",
    "                for score, seq in beam:\n",
    "                    if not seq or seq[-1] == self.end_token: continue\n",
    "                    for token in self.vocabulary:\n",
    "                        new_seq = seq + [token]\n",
    "                        if all(c.check(new_seq) for c in self.constraints):\n",
    "                            new_score = self.score_sequence(new_seq)\n",
    "                            heapq.heappush(new_beam, (new_score, new_seq))\n",
    "                beam.clear()\n",
    "                while new_beam and len(beam) < beam_width:\n",
    "                    score, seq = heapq.heappop(new_beam)\n",
    "                    if seq[-1] == self.end_token: completed.append((score, seq))\n",
    "                    else: beam.append((score, seq))\n",
    "            return sorted(completed + beam, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "    \n",
    "    # Test 1: Must contain 'c'\n",
    "    searcher_must_contain = CorrectConstrained(vocabulary)\n",
    "    searcher_must_contain.add_constraint(MustContainConstraint('c'))\n",
    "    results = searcher_must_contain.search(beam_width=2, max_length=3)\n",
    "    for _, seq in results:\n",
    "        assert 'c' in seq\n",
    "\n",
    "    # Test 2: Must not contain 'b'\n",
    "    searcher_must_not_contain = CorrectConstrained(vocabulary)\n",
    "    searcher_must_not_contain.add_constraint(MustNotContainConstraint('b'))\n",
    "    results = searcher_must_not_contain.search(beam_width=2, max_length=3)\n",
    "    for _, seq in results:\n",
    "        assert 'b' not in seq\n",
    "\n",
    "    print(\"ðŸŽ‰ All constrained beam search tests passed!\")\n",
    "\n",
    "test_constrained_beam_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 4</summary>\n",
    "\n",
    "**Hint**: Create an abstract base class `Constraint` with a `check` method. Then, implement concrete constraint classes like `MustContainConstraint` and `MustNotContainConstraint`. In your beam search loop, after generating a new candidate sequence, iterate through your list of constraints and only add the sequence to the new beam if it satisfies all of them.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Diverse Beam Search with Groups (Hard)\n",
    "\n",
    "### Contextual Introduction\n",
    "While standard beam search is good at finding high-quality sequences, it often produces a set of very similar results. For creative applications like story generation or offering multiple translation choices, we need diversity. Diverse beam search addresses this by partitioning the beam into groups and encouraging each group to explore a different part of the search space. This ensures that the final set of sequences is both high-quality and diverse.\n",
    "\n",
    "### Key Concepts\n",
    "- **Sequence Similarity**: A metric to quantify how similar two sequences are (e.g., n-gram overlap, Jaccard similarity).\n",
    "- **Clustering/Grouping**: The process of partitioning a set of items into groups based on similarity.\n",
    "- **Quality-Diversity Trade-off**: The balance between generating high-scoring (quality) sequences and generating a wide variety of (diverse) sequences.\n",
    "\n",
    "### Problem Statement\n",
    "Implement a diverse beam search algorithm that groups similar sequences and selects the best from each group. This will involve calculating sequence similarity, grouping sequences, and modifying the beam search to maintain diversity across groups.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement a function to calculate sequence similarity (e.g., Jaccard similarity of bigrams).\n",
    "- In each step of the beam search, group the candidate sequences.\n",
    "- Select the best sequence from each group to form the new beam, ensuring diversity.\n",
    "\n",
    "### Example: Sequence Similarity and Grouping\n",
    "\n",
    "```python\n",
    "# Calculating Jaccard similarity between sequences using bigrams\n",
    "\n",
    "# Sequence 1: ['hello', 'world', 'end']\n",
    "# Bigrams: {('hello', 'world'), ('world', 'end')}\n",
    "\n",
    "# Sequence 2: ['hello', 'world', 'now']\n",
    "# Bigrams: {('hello', 'world'), ('world', 'now')}\n",
    "\n",
    "# Intersection: {('hello', 'world')}  (1 common bigram)\n",
    "# Union: {('hello', 'world'), ('world', 'end'), ('world', 'now')}  (3 total unique)\n",
    "# Jaccard similarity = 1 / 3 â‰ˆ 0.33  (quite similar)\n",
    "\n",
    "# Sequence 3: ['goodbye', 'friend', 'soon']\n",
    "# Bigrams: {('goodbye', 'friend'), ('friend', 'soon')}\n",
    "\n",
    "# Intersection with Seq1: {} (0 common bigrams)\n",
    "# Union with Seq1: {('hello', 'world'), ('world', 'end'), ('goodbye', 'friend'), ('friend', 'soon')}\n",
    "# Jaccard similarity = 0 / 4 = 0.0  (very different!)\n",
    "\n",
    "# GROUPING IN DIVERSE BEAM SEARCH:\n",
    "# Beam width: 4, Groups: 2\n",
    "# Candidates (sorted by score):\n",
    "# 1. (score=0.9, ['hello', 'world', 'end'])\n",
    "# 2. (score=0.88, ['hello', 'world', 'now'])      <- Similar to #1\n",
    "# 3. (score=0.85, ['goodbye', 'friend', 'soon'])  <- Different from #1\n",
    "# 4. (score=0.82, ['hi', 'there', 'friend'])      <- Different from #1 and #3\n",
    "\n",
    "# GROUP 1 (from sequence 1): [#1, #2]  <- Similar sequences\n",
    "# GROUP 2 (from sequence 3): [#3, #4]  <- Different sequences\n",
    "\n",
    "# SELECT BEST FROM EACH GROUP:\n",
    "# From GROUP 1: #1 (best score)\n",
    "# From GROUP 2: #3 (best score in this group)\n",
    "\n",
    "# RESULT: [['hello', 'world', 'end'], ['goodbye', 'friend', 'soon']]\n",
    "# More diverse than ['hello', 'world', 'end'], ['hello', 'world', 'now']\n",
    "```\n",
    "\n",
    "### Intuition: Quality-Diversity Trade-off\n",
    "\n",
    "Standard beam search has a fundamental limitation: **it converges to similar high-probability sequences**.\n",
    "\n",
    "**Why does this happen?**\n",
    "\n",
    "Beam search is designed to find the highest-scoring sequences. Often, the top-10 sequences are variations of the same underlying path:\n",
    "\n",
    "```\n",
    "Translation task - beam_width=5:\n",
    "1. \"I would like to go home\" (score: 0.95)\n",
    "2. \"I would like to go back home\" (score: 0.94)\n",
    "3. \"I would like to return home\" (score: 0.93)\n",
    "4. \"I'd like to go home\" (score: 0.92)\n",
    "5. \"I want to go home\" (score: 0.91)\n",
    "```\n",
    "\n",
    "All five are essentially the same translation! Not useful if we want to show the user diverse alternatives.\n",
    "\n",
    "**The Quality-Diversity Trade-off:**\n",
    "\n",
    "There's a tension between two goals:\n",
    "- **Quality**: Generate sequences with high probability scores\n",
    "- **Diversity**: Generate sequences that differ meaningfully from each other\n",
    "\n",
    "Naive approaches fail at both ends of the spectrum:\n",
    "- Pure quality focus â†’ all outputs look the same (current beam search)\n",
    "- Pure diversity focus â†’ random sampling of low-quality sequences (useless outputs)\n",
    "\n",
    "**What we want:** A sweet spot that maintains reasonably high quality while ensuring outputs are meaningfully different.\n",
    "\n",
    "**The grouping concept:**\n",
    "\n",
    "Instead of one beam competing for all positions, divide the beam into groups:\n",
    "- Group 1 explores one region of the search space (e.g., formal language)\n",
    "- Group 2 explores a different region (e.g., casual language)\n",
    "- Group 3 explores another region (e.g., concise expressions)\n",
    "\n",
    "Each group maintains high quality within its region, but groups diverge from each other.\n",
    "\n",
    "**Real-world analogy:**\n",
    "\n",
    "Think of diverse beam search like a restaurant recommendation system:\n",
    "- Standard beam search: Top 5 results are all Italian restaurants (highest rated)\n",
    "- Diverse beam search: Top 5 includes Italian, Thai, Mexican, Japanese, French (high rated AND diverse)\n",
    "\n",
    "### Deducing Jaccard Similarity\n",
    "\n",
    "To encourage diversity, we need a way to measure how similar two sequences are. Why not just count shared words?\n",
    "\n",
    "**Problem with word overlap:**\n",
    "\n",
    "```\n",
    "Sequence A: \"the cat sat on the mat\"\n",
    "Sequence B: \"the dog sat on the mat\"\n",
    "Word overlap: 5/6 words match (83% similar?)\n",
    "\n",
    "But they're fundamentally different! Different subjects doing different things.\n",
    "```\n",
    "\n",
    "**Why bigrams capture similarity better:**\n",
    "\n",
    "Bigrams preserve word order and local context:\n",
    "\n",
    "```\n",
    "Sequence A: \"the cat sat\"\n",
    "  Words: {the, cat, sat}\n",
    "  Bigrams: {(the,cat), (cat,sat)}\n",
    "\n",
    "Sequence B: \"the dog sat\"  \n",
    "  Words: {the, dog, sat}  (66% overlap with A)\n",
    "  Bigrams: {(the,dog), (dog,sat)}  (0% overlap with A!)\n",
    "```\n",
    "\n",
    "The bigram overlap correctly identifies these as different sequences despite sharing 2/3 words.\n",
    "\n",
    "**Jaccard Similarity Formula:**\n",
    "\n",
    "For two sets A and B:\n",
    "```\n",
    "Jaccard(A, B) = |A âˆ© B| / |A âˆª B|\n",
    "              = (size of intersection) / (size of union)\n",
    "```\n",
    "\n",
    "Values range from 0 (completely different) to 1 (identical).\n",
    "\n",
    "**Why Jaccard over other similarity metrics?**\n",
    "\n",
    "1. **Cosine similarity**: Requires vector representations, more complex\n",
    "2. **Edit distance**: Sensitive to insertions/deletions, not length-normalized\n",
    "3. **Jaccard**: Simple, intuitive, naturally normalized to [0,1]\n",
    "\n",
    "**Practical example:**\n",
    "\n",
    "```python\n",
    "seq1 = ['I', 'love', 'machine', 'learning']\n",
    "seq2 = ['I', 'love', 'deep', 'learning']\n",
    "\n",
    "bigrams1 = {('I','love'), ('love','machine'), ('machine','learning')}\n",
    "bigrams2 = {('I','love'), ('love','deep'), ('deep','learning')}\n",
    "\n",
    "intersection = {('I','love')}  # 1 bigram\n",
    "union = {('I','love'), ('love','machine'), ('machine','learning'), \n",
    "         ('love','deep'), ('deep','learning')}  # 5 bigrams\n",
    "\n",
    "Jaccard = 1/5 = 0.2  (20% similar)\n",
    "```\n",
    "\n",
    "**Threshold for \"similar\" sequences:**\n",
    "\n",
    "- Jaccard > 0.5: Very similar, probably in the same group\n",
    "- Jaccard 0.2-0.5: Moderate similarity, borderline\n",
    "- Jaccard < 0.2: Different enough for separate groups\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "**Maintaining Multiple Group Beams vs Single Beam with Penalties**\n",
    "\n",
    "Two architectural approaches:\n",
    "\n",
    "**Approach 1: Separate beams per group**\n",
    "```python\n",
    "beams = [\n",
    "    [(score1, seq1), (score2, seq2)],  # Group 1 beam\n",
    "    [(score3, seq3), (score4, seq4)],  # Group 2 beam\n",
    "    [(score5, seq5), (score6, seq6)]   # Group 3 beam\n",
    "]\n",
    "```\n",
    "- Pro: Clean separation, easy to understand\n",
    "- Con: More complex bookkeeping, groups may become unbalanced\n",
    "\n",
    "**Approach 2: Single beam with group penalties**\n",
    "```python\n",
    "beam = [\n",
    "    (score1 - group1_penalty, seq1, group=1),\n",
    "    (score2 - group2_penalty, seq2, group=2),\n",
    "    ...\n",
    "]\n",
    "```\n",
    "- Pro: Simpler data structure, natural competition\n",
    "- Con: Penalty tuning is critical\n",
    "\n",
    "Most implementations use Approach 2 for simplicity. The group penalty creates implicit separation:\n",
    "\n",
    "```python\n",
    "# Group i gets penalty i * diversity_strength\n",
    "new_score = base_score - (group_index * diversity_strength)\n",
    "```\n",
    "\n",
    "This makes later groups start with a handicap, forcing them to explore different (lower-scoring but diverse) regions.\n",
    "\n",
    "**Selecting Representatives from Each Group**\n",
    "\n",
    "After generating candidates, how do we assign them to groups?\n",
    "\n",
    "**Simple hash-based assignment:**\n",
    "```python\n",
    "group_index = hash(' '.join(sequence[:k])) % num_groups\n",
    "```\n",
    "This assigns sequences based on their first k tokens, naturally clustering similar prefixes.\n",
    "\n",
    "**Similarity-based assignment (more sophisticated):**\n",
    "```python\n",
    "for candidate in candidates:\n",
    "    # Find most similar existing group\n",
    "    similarities = [jaccard(candidate, group_rep) for group_rep in group_representatives]\n",
    "    best_group = argmax(similarities)\n",
    "    \n",
    "    if similarities[best_group] > threshold:\n",
    "        add_to_group(best_group, candidate)\n",
    "    else:\n",
    "        create_new_group(candidate)  # Start new group\n",
    "```\n",
    "\n",
    "**Tuning the `diversity_strength` Parameter**\n",
    "\n",
    "This parameter controls how aggressively we push for diversity:\n",
    "\n",
    "**diversity_strength = 0.0:**\n",
    "- No penalty differences between groups\n",
    "- Reduces to standard beam search\n",
    "- Result: All groups converge to similar sequences\n",
    "\n",
    "**diversity_strength = 0.5 (moderate):**\n",
    "- Group 0: no penalty\n",
    "- Group 1: -0.5 score penalty  \n",
    "- Group 2: -1.0 score penalty\n",
    "- Result: Balanced quality and diversity\n",
    "\n",
    "**diversity_strength = 2.0 (aggressive):**\n",
    "- Group 0: no penalty\n",
    "- Group 1: -2.0 penalty (significant!)\n",
    "- Group 2: -4.0 penalty (drastic!)\n",
    "- Result: High diversity, but lower-quality sequences\n",
    "\n",
    "**Rule of thumb:** Start with diversity_strength = 0.5. If outputs are too similar, increase to 0.7-1.0. If quality suffers, decrease to 0.2-0.4.\n",
    "\n",
    "**Computational Complexity:**\n",
    "\n",
    "Diverse beam search with G groups and beam width K:\n",
    "- Each group maintains K/G sequences\n",
    "- Expansion: same as standard beam search O(K Ã— |V| Ã— max_length)\n",
    "- Similarity calculation: O(G Ã— max_length) per candidate if using Jaccard\n",
    "- **Total:** Roughly same complexity as standard beam search + O(G Ã— max_length) overhead\n",
    "\n",
    "The diversity overhead is typically small compared to the base search cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "class DiverseBeamSearch(BeamSearch):\n",
    "    def calculate_similarity(self, seq1: List[str], seq2: List[str]) -> float:\n",
    "        # TODO: Implement Jaccard similarity for bigrams\n",
    "        pass\n",
    "\n",
    "    def search(self, beam_width: int, max_length: int, num_groups: int, diversity_strength: float = 0.5) -> List[Tuple[float, List[str]]]:\n",
    "        # TODO: Implement diverse beam search using groups\n",
    "        pass\n",
    "\n",
    "def test_diverse_beam_search():\n",
    "    vocabulary = ['a', 'b', 'c', 'd', 'e', '<END>']\n",
    "    class CorrectDiverse(DiverseBeamSearch):\n",
    "        def calculate_similarity(self, seq1: List[str], seq2: List[str]) -> float:\n",
    "            set1 = set(zip(seq1, seq1[1:])); set2 = set(zip(seq2, seq2[1:]))\n",
    "            intersection = len(set1.intersection(set2)); union = len(set1.union(set2))\n",
    "            return intersection / union if union > 0 else 0\n",
    "        def search(self, beam_width: int, max_length: int, num_groups: int, diversity_strength: float = 0.5) -> List[Tuple[float, List[str]]]:\n",
    "            if num_groups > beam_width: num_groups = beam_width\n",
    "            beams = [[(0.0, [])] for _ in range(num_groups)]\n",
    "            completed = []\n",
    "            for _ in range(max_length):\n",
    "                all_candidates = []\n",
    "                for i in range(num_groups):\n",
    "                    for score, seq in beams[i]:\n",
    "                        if not seq or seq[-1] == self.end_token: continue\n",
    "                        for token in self.vocabulary:\n",
    "                            new_seq = seq + [token]\n",
    "                            new_score = self.score_sequence(new_seq) - (i * diversity_strength)\n",
    "                            all_candidates.append((new_score, new_seq))\n",
    "                beams = [[] for _ in range(num_groups)]\n",
    "                sorted_candidates = sorted(all_candidates, key=lambda x: x[0], reverse=True)\n",
    "                for score, seq in sorted_candidates:\n",
    "                    group_idx = hash(' '.join(seq[:1])) % num_groups\n",
    "                    if len(beams[group_idx]) < beam_width / num_groups:\n",
    "                        if seq[-1] == self.end_token: completed.append((score, seq))\n",
    "                        else: beams[group_idx].append((score, seq))\n",
    "            flat_beam = [item for sublist in beams for item in sublist]\n",
    "            return sorted(completed + flat_beam, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "    diverse_searcher = CorrectDiverse(vocabulary)\n",
    "\n",
    "    # Test that with diversity, we get different results\n",
    "    results_diverse = diverse_searcher.search(beam_width=4, max_length=3, num_groups=2, diversity_strength=0.8)\n",
    "    results_regular = diverse_searcher.search(beam_width=4, max_length=3, num_groups=1, diversity_strength=0.0)\n",
    "\n",
    "    assert results_diverse[0][1] != results_regular[0][1]\n",
    "\n",
    "    print(\"ðŸŽ‰ All diverse beam search tests passed!\")\n",
    "\n",
    "test_diverse_beam_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal hint for Problem 5</summary>\n",
    "\n",
    "**Hint**: A simple approach to grouping is to use a penalty. In each step of the beam search, when you are scoring new candidate sequences, add a penalty to the score that is proportional to the sequence's similarity to the other sequences in its group. This will encourage the groups to diverge. For example, you can use `group_index * diversity_penalty` as a simple penalty.\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
