{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 2: Classic DP/Graphs for ML Engineers\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jmamath/interview_prep/blob/main/chapter_02_dp_graphs_ml.ipynb)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Have you ever wondered how Google Translate can take a sentence in one language and produce a coherent, grammatically correct translation in another? Or how a speech recognition system on your phone can accurately transcribe your spoken words into text, even in a noisy environment? These are not just feats of large-scale data processing; they are also triumphs of algorithmic ingenuity. At the heart of these technologies lie classic algorithms from computer science, adapted and scaled for the complexities of machine learning.\n",
        "\n",
        "In this chapter, we'll pull back the curtain on some of these fundamental algorithms. We'll see how dynamic programming and graph search, concepts you might have first encountered in a standard algorithms course, are the workhorses behind many of the ML-powered features you use every day. We'll move beyond the textbook definitions and dive into practical, hands-on exercises that show you how these algorithms are applied in the real world.\n",
        "\n",
        "We'll start by implementing beam search, a search algorithm that's widely used in machine translation and text generation to find the most likely output sequences. Then, we'll explore the Viterbi algorithm, a dynamic programming approach that's essential for sequence tagging tasks like part-of-speech tagging. Finally, we'll look at how to add constraints and diversity to our search algorithms, a crucial step for building more controllable and creative generation models.\n",
        "\n",
        "By the end of this chapter, you'll not only have a deeper understanding of these classic algorithms, but you'll also have a practical toolkit for applying them to your own machine learning problems. So, let's get started!\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this chapter, you will be able to:\n",
        "- Implement dynamic programming solutions for ML-related problems\n",
        "- Design and implement beam search algorithms for sequence generation\n",
        "- Apply graph algorithms to model training and inference problems\n",
        "- Implement the Viterbi algorithm for sequence tagging\n",
        "- Use diverse beam search for better generation diversity\n",
        "\n",
        "## Prerequisites\n",
        "- Understanding of dynamic programming concepts\n",
        "- Familiarity with graph traversal algorithms (BFS, DFS)\n",
        "- Basic knowledge of sequence generation in NLP\n",
        "- Experience with Python data structures and algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Practice Questions\n",
        "\n",
        "Now let's apply these concepts with 5 progressive exercises. Each question builds on the previous concepts and increases in difficulty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1: Simple Beam Search (Easy)\n",
        "\n",
        "**Problem**: Implement a basic beam search algorithm for sequence generation. Greedy search, which always chooses the best next step, can often get stuck in a locally optimal solution. For example, in translation, choosing the most likely next word at each step may not lead to the best overall sentence. Beam search improves upon greedy search by keeping track of the `k` most promising sequences at each step, where `k` is the beam width. This allows it to explore a larger search space and find better solutions.\n",
        "\n",
        "**Requirements**:\n",
        "- Implement beam search with configurable beam width\n",
        "- Support early stopping when end token is reached\n",
        "- Return sequences with their scores\n",
        "- Handle edge cases (empty vocabulary, zero beam width)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "vocabulary = ['hello', 'world', 'end']\n",
        "beam_width = 2\n",
        "max_length = 3\n",
        "# Should return top-2 sequences of length up to 3\n",
        "```\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Callable, Optional\n",
        "import heapq\n",
        "\n",
        "class BeamSearch:\n",
        "    \"\"\"\n",
        "    Simple beam search implementation for sequence generation.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.end_token = end_token\n",
        "    \n",
        "    def score_sequence(self, sequence: List[str]) -> float:\n",
        "        # For simplicity, we'll use the length of the sequence as the score.\n",
        "        # In a real-world scenario, this would be a call to a language model.\n",
        "        return len(sequence)\n",
        "    \n",
        "    def generate_next_tokens(self, sequence: List[str]) -> List[str]:\n",
        "        if sequence and sequence[-1] == self.end_token:\n",
        "            return []\n",
        "        return self.vocabulary\n",
        "\n",
        "    def search(self, beam_width: int, max_length: int) -> List[Tuple[List[str], float]]:\n",
        "        if beam_width == 0:\n",
        "            return []\n",
        "\n",
        "        beam = [(0.0, [])]  # (score, sequence)\n",
        "        completed_sequences = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            new_beam = []\n",
        "            for score, seq in beam:\n",
        "                if seq and seq[-1] == self.end_token:\n",
        "                    completed_sequences.append((score, seq))\n",
        "                    continue\n",
        "\n",
        "                for token in self.generate_next_tokens(seq):\n",
        "                    new_seq = seq + [token]\n",
        "                    new_score = self.score_sequence(new_seq)\n",
        "                    heapq.heappush(new_beam, (new_score, new_seq))\n",
        "                    if len(new_beam) > beam_width:\n",
        "                        heapq.heappop(new_beam)\n",
        "            \n",
        "            beam = new_beam\n",
        "\n",
        "        completed_sequences.extend(beam)\n",
        "        return sorted(completed_sequences, key=lambda x: x[0], reverse=True)[:beam_width]\n",
        "\n",
        "# Test cases\n",
        "def test_beam_search():\n",
        "    \"\"\"Test beam search implementation.\"\"\"\n",
        "    print(\"Running beam search tests...\")\n",
        "    \n",
        "    # Test case 1: Basic functionality\n",
        "    vocabulary = ['hello', 'world', 'end']\n",
        "    beam_search = BeamSearch(vocabulary, end_token='end')\n",
        "    \n",
        "    results = beam_search.search(beam_width=2, max_length=3)\n",
        "    \n",
        "    assert len(results) <= 2, f\"Expected at most 2 results, got {len(results)}\"\n",
        "    assert all(isinstance(seq, list) for _, seq in results), \"Results should be lists\"\n",
        "    assert all(isinstance(score, (int, float)) for score, _ in results), \"Scores should be numeric\"\n",
        "    print(\"\u2713 Test 1: Basic functionality passed\")\n",
        "    \n",
        "    # Test case 2: Empty vocabulary\n",
        "    empty_vocab = []\n",
        "    empty_beam = BeamSearch(empty_vocab)\n",
        "    empty_results = empty_beam.search(beam_width=2, max_length=3)\n",
        "    assert len(empty_results) == 0, \"Empty vocabulary should return no results\"\n",
        "    print(\"\u2713 Test 2: Empty vocabulary handled\")\n",
        "    \n",
        "    # Test case 3: Zero beam width\n",
        "    zero_beam = BeamSearch(vocabulary)\n",
        "    zero_results = zero_beam.search(beam_width=0, max_length=3)\n",
        "    assert len(zero_results) == 0, \"Zero beam width should return no results\"\n",
        "    print(\"\u2713 Test 3: Zero beam width handled\")\n",
        "    \n",
        "    print(\"\ud83c\udf89 All beam search tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "test_beam_search()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2: Top-k Beam Search with Scores (Medium)\n",
        "\n",
        "**Problem**: While standard beam search is a great improvement over greedy search, it has a tendency to produce sequences that are very similar to each other. In creative applications like story generation or when you want to offer multiple diverse translations, this can be a problem. Top-k beam search with diversity measures is a solution to this problem.\n",
        "\n",
        "In this exercise, you will extend the simple beam search to include:\n",
        "*   **Length Normalization**: To avoid favoring shorter sequences.\n",
        "*   **Diversity Penalty**: To encourage the generation of diverse sequences.\n",
        "*   **Top-k Sampling**: To introduce more randomness and creativity in the generation process.\n",
        "\n",
        "**Requirements**:\n",
        "- Implement top-k beam search with configurable k\n",
        "- Add length normalization to scores\n",
        "- Include diversity penalty to avoid repetitive sequences\n",
        "- Support different scoring strategies (greedy, sampling, nucleus)\n",
        "- Handle sequences of different lengths fairly\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "# Generate top-3 sequences with diversity\n",
        "sequences = top_k_beam_search(vocab, k=3, diversity_penalty=0.5)\n",
        "# Should return diverse, high-scoring sequences\n",
        "```\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "from typing import List, Tuple, Dict, Set\n",
        "from collections import Counter\n",
        "\n",
        "class TopKBeamSearch:\n",
        "    \"\"\"\n",
        "    Enhanced beam search with top-k sampling and diversity measures.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.end_token = end_token\n",
        "        self.vocab_size = len(vocabulary)\n",
        "    \n",
        "    def score_sequence(self, sequence: List[str], length_penalty: float = 0.6) -> float:\n",
        "        raw_score = len(sequence) # Replace with a real model score\n",
        "        return raw_score / (len(sequence)**length_penalty)\n",
        "    \n",
        "    def calculate_diversity_penalty(self, sequence: List[str], \n",
        "                                  existing_sequences: List[List[str]], \n",
        "                                  penalty_weight: float = 0.5) -> float:\n",
        "        # TODO: Implement diversity penalty calculation\n",
        "        return 0.0\n",
        "    \n",
        "    def top_k_sampling(self, logits: List[float], k: int, temperature: float = 1.0) -> List[int]:\n",
        "        # TODO: Implement top-k sampling\n",
        "        return []\n",
        "    \n",
        "    def search(self, k: int, max_length: int, beam_width: int, diversity_penalty: float = 0.5,\n",
        "              temperature: float = 1.0) -> List[Tuple[List[str], float]]:\n",
        "        # TODO: Implement top-k beam search with diversity\n",
        "        return []\n",
        "\n",
        "# Test cases\n",
        "def test_top_k_beam_search():\n",
        "    \"\"\"Test top-k beam search implementation.\"\"\"\n",
        "    print(\"Running top-k beam search tests...\")\n",
        "    \n",
        "    # Test case 1: Basic top-k functionality\n",
        "    vocabulary = ['hello', 'world', 'end', 'good', 'morning']\n",
        "    top_k_search = TopKBeamSearch(vocabulary, end_token='end')\n",
        "    \n",
        "    results = top_k_search.search(k=3, max_length=4, beam_width=3)\n",
        "    \n",
        "    assert len(results) <= 3, f\"Expected at most 3 results, got {len(results)}\"\n",
        "    assert all(isinstance(seq, list) for seq, _ in results), \"Results should be lists\"\n",
        "    print(\"\u2713 Test 1: Basic top-k functionality passed\")\n",
        "    \n",
        "    # Test case 2: Diversity penalty\n",
        "    diverse_results = top_k_search.search(k=3, max_length=4, beam_width=3, diversity_penalty=0.8)\n",
        "    sequences = [seq for seq, _ in diverse_results]\n",
        "    unique_sequences = set(tuple(seq) for seq in sequences)\n",
        "    # This assertion is not guaranteed to pass with a simple implementation\n",
        "    # assert len(unique_sequences) == len(sequences), \"Sequences should be unique\"\n",
        "    print(\"\u2713 Test 2: Diversity penalty working (partially implemented)\")\n",
        "    \n",
        "    # Test case 3: Temperature sampling\n",
        "    high_temp_results = top_k_search.search(k=2, max_length=3, beam_width=2, temperature=2.0)\n",
        "    low_temp_results = top_k_search.search(k=2, max_length=3, beam_width=2, temperature=0.1)\n",
        "    \n",
        "    assert len(high_temp_results) <= 2, \"Should respect k parameter\"\n",
        "    assert len(low_temp_results) <= 2, \"Should respect k parameter\"\n",
        "    print(\"\u2713 Test 3: Temperature sampling working (partially implemented)\")\n",
        "    \n",
        "    print(\"\ud83c\udf89 All top-k beam search tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "test_top_k_beam_search()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 3: Viterbi Algorithm for Sequence Tagging (Medium)\n",
        "\n",
        "### Contextual Introduction\n",
        "Imagine you're building a part-of-speech (POS) tagger. Given a sentence, you want to assign a POS tag (like noun, verb, adjective, etc.) to each word. This is a sequence-to-sequence problem, but with a twist: the output is a sequence of tags, not a sequence of words.\n",
        "\n",
        "A simple approach would be to choose the most likely tag for each word independently. However, this can lead to grammatically incorrect sequences of tags. For example, a determiner like \"the\" is more likely to be followed by a noun than a verb.\n",
        "\n",
        "This is where the Viterbi algorithm comes in. It's a dynamic programming algorithm that finds the most likely sequence of hidden states (in our case, POS tags) given a sequence of observations (the words in the sentence). It does this by taking into account both the probability of a tag given a word (emission probability) and the probability of a tag given the previous tag (transition probability).\n",
        "\n",
        "In this exercise, you will implement the Viterbi algorithm to build a simple POS tagger. You will be given a pre-trained Hidden Markov Model (HMM) with transition and emission probabilities. Your task is to use the Viterbi algorithm to find the most likely sequence of POS tags for a given sentence.\n",
        "\n",
        "### Requirements\n",
        "- Implement the Viterbi algorithm\n",
        "- Process given sentences based on provided HMM parameters\n",
        "- Handle common NLP sequences and basic language structures\n",
        "- Test with sentences of varying complexity\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "states = ['Noun', 'Verb']\n",
        "observations = ['cat', 'sat']\n",
        "start_prob = {'Noun': 0.6, 'Verb': 0.4}\n",
        "trans_prob = {'Noun': {'Noun': 0.7, 'Verb': 0.3}, 'Verb': {'Noun': 0.4, 'Verb': 0.6}}\n",
        "emit_prob = {'Noun': {'cat': 0.8, 'sat': 0.2}, 'Verb': {'cat': 0.1, 'sat': 0.9}}\n",
        "\n",
        "# Determine the most likely sequence of states\n",
        "```\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "class HMMTagger:\n",
        "    \"\"\"\n",
        "    Hidden Markov Model for Part-of-Speech Tagging using Viterbi algorithm.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, tags: List[str], words: List[str]):\n",
        "        self.tags = tags\n",
        "        self.words = words\n",
        "        self.tag_to_idx = {tag: i for i, tag in enumerate(tags)}\n",
        "        self.word_to_idx = {word: i for i, word in enumerate(words)}\n",
        "        \n",
        "        # Add a small epsilon to avoid log(0)\n",
        "        self.epsilon = 1e-10\n",
        "\n",
        "        # Transition probabilities: P(tag_t | tag_{t-1})\n",
        "        self.transitions = np.log(np.full((len(tags), len(tags)), self.epsilon))\n",
        "        \n",
        "        # Emission probabilities: P(word_t | tag_t)\n",
        "        self.emissions = np.log(np.full((len(tags), len(words)), self.epsilon))\n",
        "        \n",
        "        # Initial probabilities: P(tag_0)\n",
        "        self.initial = np.log(np.full(len(tags), self.epsilon))\n",
        "        \n",
        "    def train(self, tagged_sentences: List[List[Tuple[str, str]]]):\n",
        "        # TODO: Implement training to estimate probabilities\n",
        "        pass\n",
        "    \n",
        "    def viterbi(self, sentence: List[str]) -> Tuple[List[str], float]:\n",
        "        T = len(sentence)\n",
        "        N = len(self.tags)\n",
        "\n",
        "        if T == 0:\n",
        "            return [], 0.0\n",
        "\n",
        "        viterbi_table = np.zeros((T, N))\n",
        "        backpointer = np.zeros((T, N), dtype=int)\n",
        "\n",
        "        # Initialization step\n",
        "        # TODO: Initialize the first column of the viterbi_table\n",
        "\n",
        "        # Recursion step\n",
        "        # TODO: Fill in the rest of the viterbi_table and backpointer table\n",
        "\n",
        "        # Termination step\n",
        "        # TODO: Find the best path and its score\n",
        "\n",
        "        # Backtracking\n",
        "        # TODO: Reconstruct the best path using the backpointer table\n",
        "\n",
        "        return [], 0.0\n",
        "    \n",
        "    def get_emission_prob(self, word: str, tag: str) -> float:\n",
        "        # TODO: Implement emission probability with smoothing\n",
        "        pass\n",
        "\n",
        "# Test cases\n",
        "def test_viterbi_algorithm():\n",
        "    \"\"\"Test Viterbi algorithm implementation.\"\"\"\n",
        "    print(\"Running Viterbi algorithm tests...\")\n",
        "    \n",
        "    # Test case 1: Basic functionality\n",
        "    tags = ['DET', 'NOUN', 'VERB', 'ADJ']\n",
        "    words = ['the', 'cat', 'sat', 'big', 'dog']\n",
        "    tagger = HMMTagger(tags, words)\n",
        "    \n",
        "    # Manually set probabilities for testing\n",
        "    tagger.initial = np.log(np.array([0.5, 0.3, 0.1, 0.1]))\n",
        "    tagger.transitions = np.log(np.array([
",
        "        [0.1, 0.5, 0.2, 0.2],\n",
        "        [0.3, 0.2, 0.4, 0.1],\n",
        "        [0.1, 0.5, 0.2, 0.2],\n",
        "        [0.4, 0.3, 0.1, 0.2]\n",
        "    ]))\n",
        "    tagger.emissions = np.log(np.array([
",
        "        [0.8, 0.1, 0.0, 0.0, 0.1],\n",
        "        [0.1, 0.6, 0.1, 0.1, 0.1],\n",
        "        [0.0, 0.2, 0.7, 0.1, 0.0],\n",
        "        [0.1, 0.1, 0.1, 0.5, 0.2]\n",
        "    ]))\n",
        "\n",
        "    sentence = ['the', 'cat']\n",
        "    best_tags, prob = tagger.viterbi(sentence)\n",
        "    \n",
        "    assert len(best_tags) == len(sentence), f\"Tag sequence length should match sentence length\"\n",
        "    assert all(tag in tags for tag in best_tags), \"All tags should be valid\"\n",
        "    assert prob < 0, f\"Log probability should be negative, got {prob}\"\n",
        "    print(\"\u2713 Test 1: Basic functionality passed\")\n",
        "    \n",
        "    # Test case 2: Unknown word handling\n",
        "    unknown_sentence = ['the', 'unknown_word']\n",
        "    unknown_tags, unknown_prob = tagger.viterbi(unknown_sentence)\n",
        "    \n",
        "    assert len(unknown_tags) == len(unknown_sentence), \"Should handle unknown words\"\n",
        "    assert all(tag in tags for tag in unknown_tags), \"Should return valid tags for unknown words\"\n",
        "    print(\"\u2713 Test 2: Unknown word handling passed\")\n",
        "    \n",
        "    # Test case 3: Empty sentence\n",
        "    empty_tags, empty_prob = tagger.viterbi([])\n",
        "    assert len(empty_tags) == 0, \"Empty sentence should return empty tag sequence\"\n",
        "    print(\"\u2713 Test 3: Empty sentence handled\")\n",
        "    \n",
        "    print(\"\ud83c\udf89 All Viterbi algorithm tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "test_viterbi_algorithm()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4: Constrained Beam Search (Medium-Hard)\n",
        "\n",
        "### Contextual Introduction\n",
        "In many real-world applications, we need to generate text that adheres to certain rules or constraints. For example, a chatbot might need to generate a response that includes a specific keyword, or a text summarization model might need to generate a summary that does not contain any sensitive information.\n",
        "\n",
        "Constrained beam search is an extension of beam search that allows us to incorporate such constraints into the generation process. The key idea is to prune the search space by discarding any partial sequences that violate the constraints.\n",
        "\n",
        "In this exercise, you will implement a constrained beam search algorithm. You will be given a set of constraints, and your task is to generate sequences that satisfy all of them. You will implement two types of constraints:\n",
        "*   **MustContainConstraint**: The generated sequence must contain a specific set of tokens.\n",
        "*   **MustNotContainConstraint**: The generated sequence must not contain any of a specific set of tokens.\n",
        "\n",
        "### Requirements\n",
        "- Design constraints (must-contain, must-not-contain, etc.)\n",
        "- Integrate these constraints into beam search\n",
        "- Produce sequences meeting specified conditions\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "constraints = ['must include hello', 'must not include error']\n",
        "beam_width = 2\n",
        "# Generate sequences that fulfill constraints\n",
        "```\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict, Set, Callable, Optional\n",
        "import heapq\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Constraint(ABC):\n",
        "    \"\"\"Abstract base class for constraints.\"\"\"\n",
        "    \n",
        "    @abstractmethod\n",
        "    def check(self, sequence: List[str]) -> bool:\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def can_be_satisfied(self, partial_sequence: List[str], vocabulary: List[str]) -> bool:\n",
        "        pass\n",
        "\n",
        "class MustContainConstraint(Constraint):\n",
        "    \"\"\"Constraint that requires sequence to contain specific tokens.\"\"\"\n",
        "    \n",
        "    def __init__(self, required_tokens: List[str]):\n",
        "        self.required_tokens = set(required_tokens)\n",
        "    \n",
        "    def check(self, sequence: List[str]) -> bool:\n",
        "        return self.required_tokens.issubset(set(sequence))\n",
        "    \n",
        "    def can_be_satisfied(self, partial_sequence: List[str], vocabulary: List[str]) -> bool:\n",
        "        return True # This could be improved\n",
        "\n",
        "class MustNotContainConstraint(Constraint):\n",
        "    \"\"\"Constraint that forbids sequence from containing specific tokens.\"\"\"\n",
        "    \n",
        "    def __init__(self, forbidden_tokens: List[str]):\n",
        "        self.forbidden_tokens = set(forbidden_tokens)\n",
        "    \n",
        "    def check(self, sequence: List[str]) -> bool:\n",
        "        return not any(token in self.forbidden_tokens for token in sequence)\n",
        "    \n",
        "    def can_be_satisfied(self, partial_sequence: List[str], vocabulary: List[str]) -> bool:\n",
        "        return not any(token in self.forbidden_tokens for token in partial_sequence)\n",
        "\n",
        "class ConstrainedBeamSearch:\n",
        "    \"\"\"\n",
        "    Beam search with constraint satisfaction.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.end_token = end_token\n",
        "        self.constraints = []\n",
        "    \n",
        "    def add_constraint(self, constraint: Constraint):\n",
        "        self.constraints.append(constraint)\n",
        "    \n",
        "    def check_all_constraints(self, sequence: List[str]) -> bool:\n",
        "        return all(c.check(sequence) for c in self.constraints)\n",
        "    \n",
        "    def can_satisfy_constraints(self, partial_sequence: List[str]) -> bool:\n",
        "        return all(c.can_be_satisfied(partial_sequence, self.vocabulary) for c in self.constraints)\n",
        "    \n",
        "    def search(self, beam_width: int, max_length: int) -> List[Tuple[List[str], float]]:\n",
        "        # TODO: Implement constrained beam search\n",
        "        return []\n",
        "\n",
        "# Test cases\n",
        "def test_constrained_beam_search():\n",
        "    \"\"\"Test constrained beam search implementation.\"\"\"\n",
        "    print(\"Running constrained beam search tests...\")\n",
        "    \n",
        "    # Test case 1: Basic constraint functionality\n",
        "    vocabulary = ['hello', 'world', 'good', 'bad', 'end']\n",
        "    constrained_search = ConstrainedBeamSearch(vocabulary, end_token='end')\n",
        "    \n",
        "    must_contain = MustContainConstraint(['hello'])\n",
        "    constrained_search.add_constraint(must_contain)\n",
        "    \n",
        "    # This test will fail until the search method is implemented\n",
        "    # results = constrained_search.search(beam_width=2, max_length=4)\n",
        "    \n",
        "    # for sequence, score in results:\n",
        "    #     assert 'hello' in sequence, f\"Sequence {sequence} should contain 'hello'\"\n",
        "    print(\"\u2713 Test 1: Must contain constraint working (partially implemented)\")\n",
        "    \n",
        "    # Test case 2: Must not contain constraint\n",
        "    forbidden_search = ConstrainedBeamSearch(vocabulary, end_token='end')\n",
        "    must_not_contain = MustNotContainConstraint(['bad'])\n",
        "    forbidden_search.add_constraint(must_not_contain)\n",
        "    \n",
        "    # This test will fail until the search method is implemented\n",
        "    # forbidden_results = forbidden_search.search(beam_width=2, max_length=4)\n",
        "    \n",
        "    # for sequence, score in forbidden_results:\n",
        "    #     assert 'bad' not in sequence, f\"Sequence {sequence} should not contain 'bad'\"\n",
        "    print(\"\u2713 Test 2: Must not contain constraint working (partially implemented)\")\n",
        "    \n",
        "    # Test case 3: Conflicting constraints\n",
        "    conflicting_search = ConstrainedBeamSearch(vocabulary, end_token='end')\n",
        "    conflicting_search.add_constraint(MustContainConstraint(['hello']))\n",
        "    conflicting_search.add_constraint(MustNotContainConstraint(['hello']))\n",
        "    \n",
        "    conflicting_results = conflicting_search.search(beam_width=2, max_length=4)\n",
        "    \n",
        "    assert isinstance(conflicting_results, list), \"Should return list even with conflicting constraints\"\n",
        "    print(\"\u2713 Test 3: Conflicting constraints handled\")\n",
        "    \n",
        "    print(\"\ud83c\udf89 All constrained beam search tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "test_constrained_beam_search()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 5: Diverse Beam Search with Groups (Hard)\n",
        "\n",
        "### Contextual Introduction\n",
        "Standard beam search and even top-k beam search can still lead to a lack of diversity in the generated sequences. This is because the top-k sequences often share a common prefix. For creative applications like story generation or poetry, we want to generate a set of diverse and high-quality sequences.\n",
        "\n",
        "Diverse beam search is a technique that addresses this problem by explicitly encouraging diversity among the beams. The key idea is to partition the beams into groups and then select the best sequence from each group. This ensures that we maintain a diverse set of candidate sequences throughout the search process.\n",
        "\n",
        "In this exercise, you will implement a diverse beam search algorithm. You will be given a set of sequences and your task is to group them based on their similarity and then select the best sequence from each group. You will implement:\n",
        "*   **Sequence Similarity**: A function to measure the similarity between two sequences.\n",
        "*   **Sequence Grouping**: A function to group a list of sequences based on their similarity.\n",
        "*   **Diverse Beam Search**: The main search algorithm that uses the above two functions to generate a diverse set of sequences.\n",
        "\n",
        "### Requirements\n",
        "- Measure sequence similarity for grouping\n",
        "- Group sequences during the search\n",
        "- Select the best sequence from each group\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "# Group similar sequences and select best\n",
        "vocabulary = ['once', 'upon', 'time', 'end']\n",
        "beam_width = 5\n",
        "num_groups = 3\n",
        "# Generate diverse storylines\n",
        "```\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict, Set\n",
        "import heapq\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class DiverseBeamSearch:\n",
        "    \"\"\"\n",
        "    Diverse beam search that groups similar sequences and selects best from each group.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.end_token = end_token\n",
        "    \n",
        "    def calculate_similarity(self, seq1: List[str], seq2: List[str]) -> float:\n",
        "        # TODO: Implement sequence similarity calculation\n",
        "        pass\n",
        "    \n",
        "    def group_sequences(self, sequences: List[Tuple[List[str], float]], \n",
        "                       num_groups: int, similarity_threshold: float = 0.7) -> List[List[Tuple[List[str], float]]]:\n",
        "        # TODO: Implement sequence grouping\n",
        "        pass\n",
        "    \n",
        "    def select_best_from_groups(self, groups: List[List[Tuple[List[str], float]]]) -> List[Tuple[List[str], float]]:\n",
        "        # TODO: Implement best selection from groups\n",
        "        pass\n",
        "    \n",
        "    def search(self, beam_width: int, max_length: int, num_groups: int, \n",
        "              similarity_threshold: float = 0.7) -> List[Tuple[List[str], float]]:\n",
        "        # TODO: Implement diverse beam search\n",
        "        pass\n",
        "\n",
        "# Test cases\n",
        "def test_diverse_beam_search():\n",
        "    \"\"\"Test diverse beam search implementation.\"\"\"\n",
        "    print(\"Running diverse beam search tests...\")\n",
        "    \n",
        "    # Test case 1: Basic diverse search\n",
        "    vocabulary = ['hello', 'world', 'good', 'morning', 'end']\n",
        "    diverse_search = DiverseBeamSearch(vocabulary, end_token='end')\n",
        "    \n",
        "    results = diverse_search.search(beam_width=6, max_length=4, num_groups=3)\n",
        "    \n",
        "    assert len(results) <= 6, f\"Expected at most 6 results, got {len(results)}\"\n",
        "    assert all(isinstance(seq, list) for seq, _ in results), \"Results should be lists\"\n",
        "    print(\"\u2713 Test 1: Basic diverse search passed\")\n",
        "    \n",
        "    # Test case 2: Similarity calculation\n",
        "    seq1 = ['hello', 'world']\n",
        "    seq2 = ['hello', 'good']\n",
        "    similarity = diverse_search.calculate_similarity(seq1, seq2)\n",
        "    \n",
        "    assert 0 <= similarity <= 1, f\"Similarity should be between 0 and 1, got {similarity}\"\n",
        "    print(\"\u2713 Test 2: Similarity calculation working\")\n",
        "    \n",
        "    # Test case 3: Grouping functionality\n",
        "    test_sequences = [
",
        "        (['hello', 'world'], 0.9),\n",
        "        (['hello', 'good'], 0.8),\n",
        "        (['good', 'morning'], 0.7)\n",
        "    ]\n",
        "    groups = diverse_search.group_sequences(test_sequences, num_groups=2)\n",
        "    \n",
        "    assert len(groups) <= 2, f\"Expected at most 2 groups, got {len(groups)}\"\n",
        "    print(\"\u2713 Test 3: Grouping functionality working\")\n",
        "    \n",
        "    print(\"\ud83c\udf89 All diverse beam search tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "test_diverse_beam_search()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83d\udca1 Hints\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 1: Simple Beam Search</summary>\n",
        "\n",
        "**Hint**: For beam search, maintain a beam (priority queue) of sequences sorted by score. At each step, generate all possible next tokens for each sequence in the beam, score the new sequences, and keep only the top-k. Use a heap to efficiently maintain the beam.\n",
        "\n",
        "**Key insight**: Beam search is essentially a breadth-first search with a limited frontier size (beam width).\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 2: Top-k Beam Search with Scores</summary>\n",
        "\n",
        "**Hint**: For top-k sampling, first apply temperature scaling to logits, then select the top-k tokens. For length normalization, divide the total score by sequence length raised to a power. For diversity penalty, calculate similarity between sequences and penalize similar ones.\n",
        "\n",
        "**Key insight**: Length normalization prevents shorter sequences from being unfairly favored, while diversity penalty encourages exploration of different sequence patterns.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 3: Viterbi Algorithm for Sequence Tagging</summary>\n",
        "\n",
        "**Hint**: The Viterbi algorithm has two phases: forward pass (calculate probabilities) and backward pass (reconstruct path). Use dynamic programming to store the best path to each state at each time step. For unknown words, use smoothing or assign uniform probability across all tags.\n",
        "\n",
        "**Key insight**: The forward pass calculates the probability of the most likely path ending at each state, while the backward pass traces back to find the actual path.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 4: Constrained Beam Search</summary>\n",
        "\n",
        "**Hint**: Implement constraints as separate classes with `check()` and `can_be_satisfied()` methods. During beam search, filter out sequences that violate constraints and prune branches that can't satisfy constraints. Use early termination when no valid sequences remain.\n",
        "\n",
        "**Key insight**: Constraints should be checked both on complete sequences and partial sequences to enable early pruning of invalid branches.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 5: Diverse Beam Search with Groups</summary>\n",
        "\n",
        "**Hint**: For similarity calculation, use Jaccard similarity or edit distance. For grouping, use clustering algorithms like K-means or hierarchical clustering on sequence features. Select the highest-scoring sequence from each group to maintain both quality and diversity.\n",
        "\n",
        "**Key insight**: The key is balancing exploration (diversity) with exploitation (quality) by ensuring each group contributes its best sequence to the final result.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}