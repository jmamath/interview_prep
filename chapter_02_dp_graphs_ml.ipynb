{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 2: Classic DP/Graphs for ML Engineers\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jmamath/interview_prep/blob/main/chapter_02_dp_graphs_ml.ipynb)\n",
        "\n",
        "## Introduction\n",
        "In the realm of machine learning, dynamic programming (DP) and graph algorithms serve as powerful tools for solving a wide array of complex problems. These algorithms are foundational to applications including sequence generation, speech recognition, and natural language processing (NLP). In this chapter, we will delve into these concepts through exercises that highlight their utility in real-world ML scenarios.\n",
        "\n",
        "## Problem Exploration`\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this chapter, you will be able to:\n",
        "- Implement dynamic programming solutions for ML-related problems\n",
        "- Design and implement beam search algorithms for sequence generation\n",
        "- Apply graph algorithms to model training and inference problems\n",
        "- Implement the Viterbi algorithm for sequence tagging\n",
        "- Use diverse beam search for better generation diversity\n",
        "\n",
        "## Prerequisites\n",
        "- Understanding of dynamic programming concepts\n",
        "- Familiarity with graph traversal algorithms (BFS, DFS)\n",
        "- Basic knowledge of sequence generation in NLP\n",
        "- Experience with Python data structures and algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Dynamic Programming Refresher\n",
        "\n",
        "### Core DP Concepts\n",
        "Dynamic Programming is a method for solving complex problems by breaking them down into simpler subproblems and storing the results to avoid redundant calculations.\n",
        "\n",
        "**Key Principles:**\n",
        "1. **Optimal Substructure**: Optimal solution contains optimal solutions to subproblems\n",
        "2. **Overlapping Subproblems**: Same subproblems are solved multiple times\n",
        "3. **Memoization**: Store results of subproblems to avoid recomputation\n",
        "\n",
        "### Common DP Patterns in ML\n",
        "\n",
        "**1. Sequence Problems**\n",
        "- Longest Common Subsequence (LCS)\n",
        "- Edit Distance (Levenshtein)\n",
        "- Sequence alignment in bioinformatics\n",
        "\n",
        "**2. Optimization Problems**\n",
        "- Knapsack variants for feature selection\n",
        "- Resource allocation in distributed training\n",
        "- Path optimization in neural architecture search\n",
        "\n",
        "**3. Probability Problems**\n",
        "- Forward-backward algorithm in HMMs\n",
        "- Viterbi algorithm for sequence tagging\n",
        "- Belief propagation in graphical models\n",
        "\n",
        "### DP vs Greedy vs Divide-and-Conquer\n",
        "- **DP**: Optimal solution, overlapping subproblems\n",
        "- **Greedy**: Local optimal choice, no overlapping\n",
        "- **Divide-and-Conquer**: Independent subproblems, no overlapping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 1: Simple Beam Search (Easy)\n",
        "\n",
        "### Contextual Introduction\n",
        "In machine translation, choosing the right words to construct a sentence is key. Beam search is often used to decide between sentence translations by maintaining a balance between exploration and exploitation. Imagine translating, \"The cat sat on the mat,\" where multiple valid word combinations exist.\n",
        "\n",
        "### Key Concepts\n",
        "Beam search explores multiple translation paths simultaneously. Unlike greedy search methods which choose the best option at each step, beam search keeps the top-k sequences (beam width), providing a more nuanced result.\n",
        "\n",
        "### Problem\n",
        "Implement a basic beam search algorithm for sequence generation. Given a vocabulary and a scoring function, generate the top-k sequences.\n",
        "\n",
        "### Requirements\n",
        "- Implement beam search with configurable beam width\n",
        "- Support early stopping when end token is reached\n",
        "- Return sequences with their scores\n",
        "- Handle edge cases (empty vocabulary, zero beam width)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "vocabulary = ['hello', 'world', 'end']\n",
        "beam_width = 2\n",
        "max_length = 3\n",
        "# Should return top-2 sequences of length up to 3\n",
        "```\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Graph Algorithms for ML\n",
        "\n",
        "### Graph Representation\n",
        "```python\n",
        "# Adjacency List (most common for ML)\n",
        "graph = {\n",
        "    'A': ['B', 'C'],\n",
        "    'B': ['A', 'D'],\n",
        "    'C': ['A', 'D'],\n",
        "    'D': ['B', 'C']\n",
        "}\n",
        "\n",
        "# Adjacency Matrix (for dense graphs)\n",
        "adj_matrix = [\n",
        "    [0, 1, 1, 0],\n",
        "    [1, 0, 0, 1],\n",
        "    [1, 0, 0, 1],\n",
        "    [0, 1, 1, 0]\n",
        "]\n",
        "```\n",
        "\n",
        "### Essential Graph Algorithms\n",
        "\n",
        "**1. Breadth-First Search (BFS)**\n",
        "- Level-order traversal\n",
        "- Shortest path in unweighted graphs\n",
        "- Connected components\n",
        "\n",
        "**2. Depth-First Search (DFS)**\n",
        "- Preorder, inorder, postorder traversals\n",
        "- Cycle detection\n",
        "- Topological sorting\n",
        "\n",
        "**3. Shortest Path Algorithms**\n",
        "- Dijkstra's algorithm (single-source, non-negative weights)\n",
        "- Bellman-Ford (single-source, negative weights allowed)\n",
        "- Floyd-Warshall (all-pairs shortest paths)\n",
        "\n",
        "### ML Applications of Graph Algorithms\n",
        "\n",
        "**1. Neural Architecture Search (NAS)**\n",
        "- Graph-based representation of neural networks\n",
        "- Search space exploration using graph traversal\n",
        "- Architecture optimization using shortest path algorithms\n",
        "\n",
        "**2. Knowledge Graphs**\n",
        "- Entity relationship modeling\n",
        "- Graph neural networks (GNNs)\n",
        "- Link prediction and recommendation systems\n",
        "\n",
        "**3. Dependency Parsing**\n",
        "- Parse tree construction\n",
        "- Graph-based parsing algorithms\n",
        "- Syntactic analysis in NLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Beam Search and Sequence Generation\n",
        "\n",
        "### Beam Search Algorithm\n",
        "Beam search is a heuristic search algorithm that explores a graph by expanding the most promising nodes in a limited set (beam width).\n",
        "\n",
        "**Algorithm Steps:**\n",
        "1. Start with initial state(s)\n",
        "2. At each step, generate all possible next states\n",
        "3. Score all candidates\n",
        "4. Keep only the top-k (beam width) candidates\n",
        "5. Repeat until termination condition\n",
        "\n",
        "### Beam Search Variants\n",
        "\n",
        "**1. Standard Beam Search**\n",
        "```python\n",
        "def beam_search(initial_states, beam_width, max_length):\n",
        "    beam = initial_states\n",
        "    for step in range(max_length):\n",
        "        candidates = []\n",
        "        for state in beam:\n",
        "            candidates.extend(generate_next_states(state))\n",
        "        beam = select_top_k(candidates, beam_width)\n",
        "    return beam\n",
        "```\n",
        "\n",
        "**2. Top-k Beam Search**\n",
        "- Select top-k candidates at each step\n",
        "- More diverse than greedy search\n",
        "- Balances quality and diversity\n",
        "\n",
        "**3. Diverse Beam Search**\n",
        "- Group candidates by similarity\n",
        "- Select best from each group\n",
        "- Reduces redundancy in generated sequences\n",
        "\n",
        "### Applications in ML\n",
        "\n",
        "**1. Neural Machine Translation**\n",
        "- Generate target sequences word by word\n",
        "- Maintain multiple translation hypotheses\n",
        "- Select best overall translation\n",
        "\n",
        "**2. Text Generation**\n",
        "- Language model inference\n",
        "- Story generation\n",
        "- Code generation\n",
        "\n",
        "**3. Speech Recognition**\n",
        "- Acoustic model + language model\n",
        "- Generate most likely word sequences\n",
        "- Handle multiple pronunciation variants\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Viterbi Algorithm\n",
        "\n",
        "### Hidden Markov Models (HMMs)\n",
        "HMMs are statistical models where the system being modeled is assumed to be a Markov process with unobserved (hidden) states.\n",
        "\n",
        "**Components:**\n",
        "- **States**: Hidden variables we want to infer\n",
        "- **Observations**: Visible variables we can measure\n",
        "- **Transition Probabilities**: P(state_t | state_{t-1})\n",
        "- **Emission Probabilities**: P(observation_t | state_t)\n",
        "- **Initial Probabilities**: P(state_0)\n",
        "\n",
        "### Viterbi Algorithm\n",
        "The Viterbi algorithm finds the most likely sequence of hidden states given a sequence of observations.\n",
        "\n",
        "**Dynamic Programming Formulation:**\n",
        "```\n",
        "v[t][s] = max over all previous states s' of:\n",
        "    v[t-1][s'] * transition[s'][s] * emission[s][observation[t]]\n",
        "```\n",
        "\n",
        "**Backtracking:**\n",
        "- Keep track of which previous state led to each current state\n",
        "- Trace back from the final state to get the optimal path\n",
        "\n",
        "### Applications in ML\n",
        "\n",
        "**1. Part-of-Speech Tagging**\n",
        "- Hidden states: POS tags (noun, verb, adjective, etc.)\n",
        "- Observations: words in a sentence\n",
        "- Find most likely sequence of POS tags\n",
        "\n",
        "**2. Named Entity Recognition**\n",
        "- Hidden states: entity types (person, location, organization, etc.)\n",
        "- Observations: words in text\n",
        "- Identify and classify named entities\n",
        "\n",
        "**3. Speech Recognition**\n",
        "- Hidden states: phonemes or words\n",
        "- Observations: acoustic features\n",
        "- Convert speech to text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Practice Questions\n",
        "\n",
        "Now let's apply these concepts with 5 progressive exercises. Each question builds on the previous concepts and increases in difficulty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1: Simple Beam Search (Easy)\n",
        "\n",
        "**Problem**: Implement a basic beam search algorithm for sequence generation. Given a vocabulary and a scoring function, generate the top-k sequences.\n",
        "\n",
        "**Requirements**:\n",
        "- Implement beam search with configurable beam width\n",
        "- Support early stopping when end token is reached\n",
        "- Return sequences with their scores\n",
        "- Handle edge cases (empty vocabulary, zero beam width)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "vocabulary = ['hello', 'world', 'end']\n",
        "beam_width = 2\n",
        "max_length = 3\n",
        "# Should return top-2 sequences of length up to 3\n",
        "```\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Callable, Optional\n",
        "import heapq\n",
        "\n",
        "class BeamSearch:\n",
        "    \"\"\"\n",
        "    Simple beam search implementation for sequence generation.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
        "        \"\"\"\n",
        "        Initialize beam search.\n",
        "        \n",
        "        Args:\n",
        "            vocabulary: List of possible tokens\n",
        "            end_token: Token that signals sequence end\n",
        "        \"\"\"\n",
        "        self.vocabulary = vocabulary\n",
        "        self.end_token = end_token\n",
        "        # TODO: Add any additional initialization\n",
        "    \n",
        "    def score_sequence(self, sequence: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Score a sequence (higher is better).\n",
        "        Simple scoring function - can be replaced with more sophisticated models.\n",
        "        \n",
        "        Args:\n",
        "            sequence: List of tokens\n",
        "            \n",
        "        Returns:\n",
        "            Score for the sequence\n",
        "        \"\"\"\n",
        "        # TODO: Implement scoring function\n",
        "        pass\n",
        "    \n",
        "    def generate_next_tokens(self, sequence: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate possible next tokens for a sequence.\n",
        "        \n",
        "        Args:\n",
        "            sequence: Current sequence\n",
        "            \n",
        "        Returns:\n",
        "            List of possible next tokens\n",
        "        \"\"\"\n",
        "        # TODO: Implement next token generation\n",
        "        pass\n",
        "    \n",
        "    def search(self, beam_width: int, max_length: int) -> List[Tuple[List[str], float]]:\n",
        "        \"\"\"\n",
        "        Perform beam search.\n",
        "        \n",
        "        Args:\n",
        "            beam_width: Number of sequences to keep at each step\n",
        "            max_length: Maximum sequence length\n",
        "            \n",
        "        Returns:\n",
        "            List of (sequence, score) tuples, sorted by score (descending)\n",
        "        \"\"\"\n",
        "        # TODO: Implement beam search algorithm\n",
        "        pass\n",
        "\n",
        "# Test cases\n",
        "def test_beam_search():\n",
        "    \"\"\"Test beam search implementation.\"\"\"\n",
        "    print(\"Running beam search tests...\")\n",
        "    \n",
        "    # Test case 1: Basic functionality\n",
        "    vocabulary = ['hello', 'world', 'end']\n",
        "    beam_search = BeamSearch(vocabulary, end_token='end')\n",
        "    \n",
        "    results = beam_search.search(beam_width=2, max_length=3)\n",
        "    \n",
        "    assert len(results) <= 2, f\"Expected at most 2 results, got {len(results)}\"\n",
        "    assert all(isinstance(seq, list) for seq, _ in results), \"Results should be lists\"\n",
        "    assert all(isinstance(score, (int, float)) for _, score in results), \"Scores should be numeric\"\n",
        "    print(\"âœ“ Test 1: Basic functionality passed\")\n",
        "    \n",
        "    # Test case 2: Empty vocabulary\n",
        "    empty_vocab = []\n",
        "    empty_beam = BeamSearch(empty_vocab)\n",
        "    empty_results = empty_beam.search(beam_width=2, max_length=3)\n",
        "    assert len(empty_results) == 0, \"Empty vocabulary should return no results\"\n",
        "    print(\"âœ“ Test 2: Empty vocabulary handled\")\n",
        "    \n",
        "    # Test case 3: Zero beam width\n",
        "    zero_beam = BeamSearch(vocabulary)\n",
        "    zero_results = zero_beam.search(beam_width=0, max_length=3)\n",
        "    assert len(zero_results) == 0, \"Zero beam width should return no results\"\n",
        "    print(\"âœ“ Test 3: Zero beam width handled\")\n",
        "    \n",
        "    print(\"ðŸŽ‰ All beam search tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "test_beam_search()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2: Top-k Beam Search with Scores (Medium)\n",
        "\n",
        "**Problem**: Extend the beam search to implement top-k sampling with proper scoring and ranking. Include length normalization and diversity measures.\n",
        "\n",
        "**Requirements**:\n",
        "- Implement top-k beam search with configurable k\n",
        "- Add length normalization to scores\n",
        "- Include diversity penalty to avoid repetitive sequences\n",
        "- Support different scoring strategies (greedy, sampling, nucleus)\n",
        "- Handle sequences of different lengths fairly\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "# Generate top-3 sequences with diversity\n",
        "sequences = top_k_beam_search(vocab, k=3, diversity_penalty=0.5)\n",
        "# Should return diverse, high-scoring sequences\n",
        "```\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "from typing import List, Tuple, Dict, Set\n",
        "from collections import Counter\n",
        "\n",
        "class TopKBeamSearch:\n",
        "    \"\"\"\n",
        "    Enhanced beam search with top-k sampling and diversity measures.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
        "        \"\"\"\n",
        "        Initialize top-k beam search.\n",
        "        \n",
        "        Args:\n",
        "            vocabulary: List of possible tokens\n",
        "            end_token: Token that signals sequence end\n",
        "        \"\"\"\n",
        "        self.vocabulary = vocabulary\n",
        "        self.end_token = end_token\n",
        "        # TODO: Add initialization for diversity tracking\n",
        "    \n",
        "    def score_sequence(self, sequence: List[str], length_penalty: float = 0.6) -> float:\n",
        "        \"\"\"\n",
        "        Score a sequence with length normalization.\n",
        "        \n",
        "        Args:\n",
        "            sequence: List of tokens\n",
        "            length_penalty: Penalty factor for length (0 = no penalty, 1 = full penalty)\n",
        "            \n",
        "        Returns:\n",
        "            Normalized score for the sequence\n",
        "        \"\"\"\n",
        "        # TODO: Implement length-normalized scoring\n",
        "        pass\n",
        "    \n",
        "    def calculate_diversity_penalty(self, sequence: List[str], \n",
        "                                  existing_sequences: List[List[str]], \n",
        "                                  penalty_weight: float = 0.5) -> float:\n",
        "        \"\"\"\n",
        "        Calculate diversity penalty based on similarity to existing sequences.\n",
        "        \n",
        "        Args:\n",
        "            sequence: Current sequence\n",
        "            existing_sequences: Previously generated sequences\n",
        "            penalty_weight: Weight of diversity penalty\n",
        "            \n",
        "        Returns:\n",
        "            Diversity penalty score\n",
        "        \"\"\"\n",
        "        # TODO: Implement diversity penalty calculation\n",
        "        pass\n",
        "    \n",
        "    def top_k_sampling(self, logits: List[float], k: int, temperature: float = 1.0) -> List[int]:\n",
        "        \"\"\"\n",
        "        Sample top-k tokens from logits.\n",
        "        \n",
        "        Args:\n",
        "            logits: Raw scores for each token\n",
        "            k: Number of top tokens to consider\n",
        "            temperature: Sampling temperature (higher = more random)\n",
        "            \n",
        "        Returns:\n",
        "            Indices of top-k tokens\n",
        "        \"\"\"\n",
        "        # TODO: Implement top-k sampling\n",
        "        pass\n",
        "    \n",
        "    def search(self, k: int, max_length: int, diversity_penalty: float = 0.5,\n",
        "              temperature: float = 1.0) -> List[Tuple[List[str], float]]:\n",
        "        \"\"\"\n",
        "        Perform top-k beam search with diversity.\n",
        "        \n",
        "        Args:\n",
        "            k: Number of sequences to generate\n",
        "            max_length: Maximum sequence length\n",
        "            diversity_penalty: Weight for diversity penalty\n",
        "            temperature: Sampling temperature\n",
        "            \n",
        "        Returns:\n",
        "            List of (sequence, score) tuples, sorted by score\n",
        "        \"\"\"\n",
        "        # TODO: Implement top-k beam search with diversity\n",
        "        pass\n",
        "\n",
        "# Test cases\n",
        "def test_top_k_beam_search():\n",
        "    \"\"\"Test top-k beam search implementation.\"\"\"\n",
        "    print(\"Running top-k beam search tests...\")\n",
        "    \n",
        "    # Test case 1: Basic top-k functionality\n",
        "    vocabulary = ['hello', 'world', 'end', 'good', 'morning']\n",
        "    top_k_search = TopKBeamSearch(vocabulary, end_token='end')\n",
        "    \n",
        "    results = top_k_search.search(k=3, max_length=4)\n",
        "    \n",
        "    assert len(results) <= 3, f\"Expected at most 3 results, got {len(results)}\"\n",
        "    assert all(isinstance(seq, list) for seq, _ in results), \"Results should be lists\"\n",
        "    print(\"âœ“ Test 1: Basic top-k functionality passed\")\n",
        "    \n",
        "    # Test case 2: Diversity penalty\n",
        "    diverse_results = top_k_search.search(k=3, max_length=4, diversity_penalty=0.8)\n",
        "    # Check that sequences are different (simple check)\n",
        "    sequences = [seq for seq, _ in diverse_results]\n",
        "    unique_sequences = set(tuple(seq) for seq in sequences)\n",
        "    assert len(unique_sequences) == len(sequences), \"Sequences should be unique\"\n",
        "    print(\"âœ“ Test 2: Diversity penalty working\")\n",
        "    \n",
        "    # Test case 3: Temperature sampling\n",
        "    high_temp_results = top_k_search.search(k=2, max_length=3, temperature=2.0)\n",
        "    low_temp_results = top_k_search.search(k=2, max_length=3, temperature=0.1)\n",
        "    \n",
        "    # Higher temperature should produce more diverse results\n",
        "    assert len(high_temp_results) <= 2, \"Should respect k parameter\"\n",
        "    assert len(low_temp_results) <= 2, \"Should respect k parameter\"\n",
        "    print(\"âœ“ Test 3: Temperature sampling working\")\n",
        "    \n",
        "    print(\"ðŸŽ‰ All top-k beam search tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "test_top_k_beam_search()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 3: Viterbi Algorithm for Sequence Tagging (Medium)\n",
        "\n",
        "### Contextual Introduction\n",
        "In Natural Language Processing (NLP), sequence tagging is essential for tasks like Part-of-Speech (POS) tagging. The Viterbi algorithm, crucial in Hidden Markov Models (HMMs), is widely used to find the most probable sequence of hidden states.\n",
        "\n",
        "### Key Concepts\n",
        "- **Hidden Markov Models**: Statistical models where the system is assumed to be a Markov process with hidden states.\n",
        "- **Transition Probabilities**: Probability of moving from one state to another.\n",
        "- **Emission Probabilities**: Probability of an observed output given a state.\n",
        "\n",
        "### Problem\n",
        "Implement the Viterbi algorithm for POS tagging a sentence using a given HMM with transition and emission probabilities.\n",
        "\n",
        "### Requirements\n",
        "- Implement the Viterbi algorithm\n",
        "- Process given sentences based on provided HMM parameters\n",
        "- Handle common NLP sequences and basic language structures\n",
        "- Test with sentences of varying complexity\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "states = ['Noun', 'Verb']\n",
        "observations = ['cat', 'sat']\n",
        "start_prob = {'Noun': 0.6, 'Verb': 0.4}\n",
        "trans_prob = {'Noun': {'Noun': 0.7, 'Verb': 0.3}, 'Verb': {'Noun': 0.4, 'Verb': 0.6}}\n",
        "emit_prob = {'Noun': {'cat': 0.8, 'sat': 0.2}, 'Verb': {'cat': 0.1, 'sat': 0.9}}\n",
        "\n",
        "# Determine the most likely sequence of states\n",
        "```\n",
        "\n",
        "**Starter Code**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "class HMMTagger:\n",
        "    \"\"\"\n",
        "    Hidden Markov Model for Part-of-Speech Tagging using Viterbi algorithm.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, tags: List[str], words: List[str]):\n",
        "        \"\"\"\n",
        "        Initialize HMM tagger.\n",
        "        \n",
        "        Args:\n",
        "            tags: List of possible POS tags\n",
        "            words: List of possible words\n",
        "        \"\"\"\n",
        "        self.tags = tags\n",
        "        self.words = words\n",
        "        self.tag_to_idx = {tag: i for i, tag in enumerate(tags)}\n",
        "        self.word_to_idx = {word: i for i, word in enumerate(words)}\n",
        "        \n",
        "        # Transition probabilities: P(tag_t | tag_{t-1})\n",
        "        self.transitions = np.zeros((len(tags), len(tags)))\n",
        "        \n",
        "        # Emission probabilities: P(word_t | tag_t)\n",
        "        self.emissions = np.zeros((len(tags), len(words)))\n",
        "        \n",
        "        # Initial probabilities: P(tag_0)\n",
        "        self.initial = np.zeros(len(tags))\n",
        "        \n",
        "        # TODO: Initialize probabilities (can be loaded from training data)\n",
        "    \n",
        "    def train(self, tagged_sentences: List[List[Tuple[str, str]]]):\n",
        "        \"\"\"\n",
        "        Train the HMM on tagged sentences.\n",
        "        \n",
        "        Args:\n",
        "            tagged_sentences: List of sentences, each is list of (word, tag) tuples\n",
        "        \"\"\"\n",
        "        # TODO: Implement training to estimate probabilities\n",
        "        pass\n",
        "    \n",
        "    def viterbi(self, sentence: List[str]) -> Tuple[List[str], float]:\n",
        "        \"\"\"\n",
        "        Find most likely tag sequence using Viterbi algorithm.\n",
        "        \n",
        "        Args:\n",
        "            sentence: List of words\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (best_tag_sequence, probability)\n",
        "        \"\"\"\n",
        "        # TODO: Implement Viterbi algorithm\n",
        "        pass\n",
        "    \n",
        "    def forward_pass(self, sentence: List[str]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Forward pass: calculate probabilities for each state at each time step.\n",
        "        \n",
        "        Args:\n",
        "            sentence: List of words\n",
        "            \n",
        "        Returns:\n",
        "            Array of shape (len(sentence), len(tags)) with probabilities\n",
        "        \"\"\"\n",
        "        # TODO: Implement forward pass\n",
        "        pass\n",
        "    \n",
        "    def backward_pass(self, forward_probs: np.ndarray, sentence: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Backward pass: reconstruct the best path.\n",
        "        \n",
        "        Args:\n",
        "            forward_probs: Probabilities from forward pass\n",
        "            sentence: List of words\n",
        "            \n",
        "        Returns:\n",
        "            Best tag sequence\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass\n",
        "        pass\n",
        "    \n",
        "    def get_emission_prob(self, word: str, tag: str) -> float:\n",
        "        \"\"\"\n",
        "        Get emission probability with smoothing for unknown words.\n",
        "        \n",
        "        Args:\n",
        "            word: Word\n",
        "            tag: POS tag\n",
        "            \n",
        "        Returns:\n",
        "            Emission probability\n",
        "        \"\"\"\n",
        "        # TODO: Implement emission probability with smoothing\n",
        "        pass\n",
        "\n",
        "# Test cases\n",
        "def test_viterbi_algorithm():\n",
        "    \"\"\"Test Viterbi algorithm implementation.\"\"\"\n",
        "    print(\"Running Viterbi algorithm tests...\")\n",
        "    \n",
        "    # Test case 1: Basic functionality\n",
        "    tags = ['DET', 'NOUN', 'VERB', 'ADJ']\n",
        "    words = ['the', 'cat', 'sat', 'big', 'dog']\n",
        "    tagger = HMMTagger(tags, words)\n",
        "    \n",
        "    # Simple test sentence\n",
        "    sentence = ['the', 'cat']\n",
        "    best_tags, prob = tagger.viterbi(sentence)\n",
        "    \n",
        "    assert len(best_tags) == len(sentence), f\"Tag sequence length should match sentence length\"\n",
        "    assert all(tag in tags for tag in best_tags), \"All tags should be valid\"\n",
        "    assert 0 <= prob <= 1, f\"Probability should be between 0 and 1, got {prob}\"\n",
        "    print(\"âœ“ Test 1: Basic functionality passed\")\n",
        "    \n",
        "    # Test case 2: Unknown word handling\n",
        "    unknown_sentence = ['the', 'unknown_word']\n",
        "    unknown_tags, unknown_prob = tagger.viterbi(unknown_sentence)\n",
        "    \n",
        "    assert len(unknown_tags) == len(unknown_sentence), \"Should handle unknown words\"\n",
        "    assert all(tag in tags for tag in unknown_tags), \"Should return valid tags for unknown words\"\n",
        "    print(\"âœ“ Test 2: Unknown word handling passed\")\n",
        "    \n",
        "    # Test case 3: Empty sentence\n",
        "    empty_tags, empty_prob = tagger.viterbi([])\n",
        "    assert len(empty_tags) == 0, \"Empty sentence should return empty tag sequence\"\n",
        "    print(\"âœ“ Test 3: Empty sentence handled\")\n",
        "    \n",
        "    print(\"ðŸŽ‰ All Viterbi algorithm tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "test_viterbi_algorithm()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4: Constrained Beam Search (Medium-Hard)\n",
        "\n",
        "### Contextual Introduction\n",
        "Constrained beam search is valuable in chatbot development, where responses need to fit specific conditions. By respecting constraints, we ensure outputs align with predetermined rules.\n",
        "\n",
        "### Key Concepts\n",
        "Constraints enforce rules on the generated sequences. Logical handling of these constraints allows the system to generate coherent and context-aware responses.\n",
        "\n",
        "### Problem\n",
        "Implement beam search respecting constraints for applications like constrained text generation in chatbots.\n",
        "\n",
        "### Requirements\n",
        "- Design constraints (must-contain, must-not-contain, etc.)\n",
        "- Integrate these constraints into beam search\n",
        "- Produce sequences meeting specified conditions\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "constraints = ['must include hello', 'must not include error']\n",
        "beam_width = 2\n",
        "# Generate sequences that fulfill constraints\n",
        "```\n",
        "\n",
        "**Starter Code**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict, Set, Callable, Optional\n",
        "import heapq\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Constraint(ABC):\n",
        "    \"\"\"Abstract base class for constraints.\"\"\"\n",
        "    \n",
        "    @abstractmethod\n",
        "    def check(self, sequence: List[str]) -> bool:\n",
        "        \"\"\"Check if sequence satisfies constraint.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def can_be_satisfied(self, partial_sequence: List[str], vocabulary: List[str]) -> bool:\n",
        "        \"\"\"Check if constraint can still be satisfied given partial sequence.\"\"\"\n",
        "        pass\n",
        "\n",
        "class MustContainConstraint(Constraint):\n",
        "    \"\"\"Constraint that requires sequence to contain specific tokens.\"\"\"\n",
        "    \n",
        "    def __init__(self, required_tokens: List[str]):\n",
        "        self.required_tokens = set(required_tokens)\n",
        "    \n",
        "    def check(self, sequence: List[str]) -> bool:\n",
        "        # TODO: Implement must contain check\n",
        "        pass\n",
        "    \n",
        "    def can_be_satisfied(self, partial_sequence: List[str], vocabulary: List[str]) -> bool:\n",
        "        # TODO: Implement can be satisfied check\n",
        "        pass\n",
        "\n",
        "class MustNotContainConstraint(Constraint):\n",
        "    \"\"\"Constraint that forbids sequence from containing specific tokens.\"\"\"\n",
        "    \n",
        "    def __init__(self, forbidden_tokens: List[str]):\n",
        "        self.forbidden_tokens = set(forbidden_tokens)\n",
        "    \n",
        "    def check(self, sequence: List[str]) -> bool:\n",
        "        # TODO: Implement must not contain check\n",
        "        pass\n",
        "    \n",
        "    def can_be_satisfied(self, partial_sequence: List[str], vocabulary: List[str]) -> bool:\n",
        "        # TODO: Implement can be satisfied check\n",
        "        pass\n",
        "\n",
        "class ConstrainedBeamSearch:\n",
        "    \"\"\"\n",
        "    Beam search with constraint satisfaction.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.end_token = end_token\n",
        "        self.constraints = []\n",
        "    \n",
        "    def add_constraint(self, constraint: Constraint):\n",
        "        \"\"\"Add a constraint to the search.\"\"\"\n",
        "        # TODO: Implement constraint addition\n",
        "        pass\n",
        "    \n",
        "    def check_all_constraints(self, sequence: List[str]) -> bool:\n",
        "        \"\"\"Check if sequence satisfies all constraints.\"\"\"\n",
        "        # TODO: Implement constraint checking\n",
        "        pass\n",
        "    \n",
        "    def can_satisfy_constraints(self, partial_sequence: List[str]) -> bool:\n",
        "        \"\"\"Check if partial sequence can still satisfy all constraints.\"\"\"\n",
        "        # TODO: Implement partial constraint checking\n",
        "        pass\n",
        "    \n",
        "    def search(self, beam_width: int, max_length: int) -> List[Tuple[List[str], float]]:\n",
        "        \"\"\"\n",
        "        Perform constrained beam search.\n",
        "        \n",
        "        Args:\n",
        "            beam_width: Number of sequences to keep at each step\n",
        "            max_length: Maximum sequence length\n",
        "            \n",
        "        Returns:\n",
        "            List of (sequence, score) tuples that satisfy all constraints\n",
        "        \"\"\"\n",
        "        # TODO: Implement constrained beam search\n",
        "        pass\n",
        "\n",
        "# Test cases\n",
        "def test_constrained_beam_search():\n",
        "    \"\"\"Test constrained beam search implementation.\"\"\"\n",
        "    print(\"Running constrained beam search tests...\")\n",
        "    \n",
        "    # Test case 1: Basic constraint functionality\n",
        "    vocabulary = ['hello', 'world', 'good', 'bad', 'end']\n",
        "    constrained_search = ConstrainedBeamSearch(vocabulary, end_token='end')\n",
        "    \n",
        "    # Add must contain constraint\n",
        "    must_contain = MustContainConstraint(['hello'])\n",
        "    constrained_search.add_constraint(must_contain)\n",
        "    \n",
        "    results = constrained_search.search(beam_width=2, max_length=4)\n",
        "    \n",
        "    # All results should contain 'hello'\n",
        "    for sequence, score in results:\n",
        "        assert 'hello' in sequence, f\"Sequence {sequence} should contain 'hello'\"\n",
        "    print(\"âœ“ Test 1: Must contain constraint working\")\n",
        "    \n",
        "    # Test case 2: Must not contain constraint\n",
        "    forbidden_search = ConstrainedBeamSearch(vocabulary, end_token='end')\n",
        "    must_not_contain = MustNotContainConstraint(['bad'])\n",
        "    forbidden_search.add_constraint(must_not_contain)\n",
        "    \n",
        "    forbidden_results = forbidden_search.search(beam_width=2, max_length=4)\n",
        "    \n",
        "    # No results should contain 'bad'\n",
        "    for sequence, score in forbidden_results:\n",
        "        assert 'bad' not in sequence, f\"Sequence {sequence} should not contain 'bad'\"\n",
        "    print(\"âœ“ Test 2: Must not contain constraint working\")\n",
        "    \n",
        "    # Test case 3: Conflicting constraints\n",
        "    conflicting_search = ConstrainedBeamSearch(vocabulary, end_token='end')\n",
        "    conflicting_search.add_constraint(MustContainConstraint(['hello']))\n",
        "    conflicting_search.add_constraint(MustNotContainConstraint(['hello']))\n",
        "    \n",
        "    conflicting_results = conflicting_search.search(beam_width=2, max_length=4)\n",
        "    \n",
        "    # Should handle conflicting constraints gracefully\n",
        "    assert isinstance(conflicting_results, list), \"Should return list even with conflicting constraints\"\n",
        "    print(\"âœ“ Test 3: Conflicting constraints handled\")\n",
        "    \n",
        "    print(\"ðŸŽ‰ All constrained beam search tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "test_constrained_beam_search()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 5: Diverse Beam Search with Groups (Hard)\n",
        "\n",
        "### Contextual Introduction\n",
        "Diverse beam search ensures varied output, crucial in creative applications like storytelling. By grouping similar sequences, we can select the best from each group, ensuring diversity.\n",
        "\n",
        "### Key Concepts\n",
        "- **Sequence Similarity**: Measurement to group similar sequences.\n",
        "- **Grouping Strategies**: Use clustering or other methods to form groups.\n",
        "- **Selection Process**: Choose the best sequence from each group.\n",
        "\n",
        "### Problem\n",
        "Implement a diverse beam search that groups sequences and selects the best from each.\n",
        "\n",
        "### Requirements\n",
        "- Measure sequence similarity for grouping\n",
        "- Group sequences during the search\n",
        "- Select the best sequence from each group\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "# Group similar sequences and select best\n",
        "vocabulary = ['once', 'upon', 'time', 'end']\n",
        "beam_width = 5\n",
        "num_groups = 3\n",
        "# Generate diverse storylines\n",
        "```\n",
        "\n",
        "**Starter Code**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict, Set\n",
        "import heapq\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class DiverseBeamSearch:\n",
        "    \"\"\"\n",
        "    Diverse beam search that groups similar sequences and selects best from each group.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocabulary: List[str], end_token: str = '<END>'):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.end_token = end_token\n",
        "    \n",
        "    def calculate_similarity(self, seq1: List[str], seq2: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calculate similarity between two sequences.\n",
        "        \n",
        "        Args:\n",
        "            seq1, seq2: Sequences to compare\n",
        "            \n",
        "        Returns:\n",
        "            Similarity score between 0 and 1\n",
        "        \"\"\"\n",
        "        # TODO: Implement sequence similarity calculation\n",
        "        pass\n",
        "    \n",
        "    def group_sequences(self, sequences: List[Tuple[List[str], float]], \n",
        "                       num_groups: int, similarity_threshold: float = 0.7) -> List[List[Tuple[List[str], float]]]:\n",
        "        \"\"\"\n",
        "        Group sequences by similarity.\n",
        "        \n",
        "        Args:\n",
        "            sequences: List of (sequence, score) tuples\n",
        "            num_groups: Target number of groups\n",
        "            similarity_threshold: Minimum similarity for grouping\n",
        "            \n",
        "        Returns:\n",
        "            List of groups, each containing similar sequences\n",
        "        \"\"\"\n",
        "        # TODO: Implement sequence grouping\n",
        "        pass\n",
        "    \n",
        "    def select_best_from_groups(self, groups: List[List[Tuple[List[str], float]]]) -> List[Tuple[List[str], float]]:\n",
        "        \"\"\"\n",
        "        Select the best sequence from each group.\n",
        "        \n",
        "        Args:\n",
        "            groups: List of groups of sequences\n",
        "            \n",
        "        Returns:\n",
        "            List of best sequences from each group\n",
        "        \"\"\"\n",
        "        # TODO: Implement best selection from groups\n",
        "        pass\n",
        "    \n",
        "    def search(self, beam_width: int, max_length: int, num_groups: int, \n",
        "              similarity_threshold: float = 0.7) -> List[Tuple[List[str], float]]:\n",
        "        \"\"\"\n",
        "        Perform diverse beam search.\n",
        "        \n",
        "        Args:\n",
        "            beam_width: Number of sequences to keep at each step\n",
        "            max_length: Maximum sequence length\n",
        "            num_groups: Number of diversity groups\n",
        "            similarity_threshold: Similarity threshold for grouping\n",
        "            \n",
        "        Returns:\n",
        "            List of diverse (sequence, score) tuples\n",
        "        \"\"\"\n",
        "        # TODO: Implement diverse beam search\n",
        "        pass\n",
        "\n",
        "# Test cases\n",
        "def test_diverse_beam_search():\n",
        "    \"\"\"Test diverse beam search implementation.\"\"\"\n",
        "    print(\"Running diverse beam search tests...\")\n",
        "    \n",
        "    # Test case 1: Basic diverse search\n",
        "    vocabulary = ['hello', 'world', 'good', 'morning', 'end']\n",
        "    diverse_search = DiverseBeamSearch(vocabulary, end_token='end')\n",
        "    \n",
        "    results = diverse_search.search(beam_width=6, max_length=4, num_groups=3)\n",
        "    \n",
        "    assert len(results) <= 6, f\"Expected at most 6 results, got {len(results)}\"\n",
        "    assert all(isinstance(seq, list) for seq, _ in results), \"Results should be lists\"\n",
        "    print(\"âœ“ Test 1: Basic diverse search passed\")\n",
        "    \n",
        "    # Test case 2: Similarity calculation\n",
        "    seq1 = ['hello', 'world']\n",
        "    seq2 = ['hello', 'good']\n",
        "    similarity = diverse_search.calculate_similarity(seq1, seq2)\n",
        "    \n",
        "    assert 0 <= similarity <= 1, f\"Similarity should be between 0 and 1, got {similarity}\"\n",
        "    print(\"âœ“ Test 2: Similarity calculation working\")\n",
        "    \n",
        "    # Test case 3: Grouping functionality\n",
        "    test_sequences = [\n",
        "        (['hello', 'world'], 0.9),\n",
        "        (['hello', 'good'], 0.8),\n",
        "        (['good', 'morning'], 0.7)\n",
        "    ]\n",
        "    groups = diverse_search.group_sequences(test_sequences, num_groups=2)\n",
        "    \n",
        "    assert len(groups) <= 2, f\"Expected at most 2 groups, got {len(groups)}\"\n",
        "    print(\"âœ“ Test 3: Grouping functionality working\")\n",
        "    \n",
        "    print(\"ðŸŽ‰ All diverse beam search tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "test_diverse_beam_search()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ’¡ Hints\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 1: Simple Beam Search</summary>\n",
        "\n",
        "**Hint**: For beam search, maintain a beam (priority queue) of sequences sorted by score. At each step, generate all possible next tokens for each sequence in the beam, score the new sequences, and keep only the top-k. Use a heap to efficiently maintain the beam.\n",
        "\n",
        "**Key insight**: Beam search is essentially a breadth-first search with a limited frontier size (beam width).\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 2: Top-k Beam Search with Scores</summary>\n",
        "\n",
        "**Hint**: For top-k sampling, first apply temperature scaling to logits, then select the top-k tokens. For length normalization, divide the total score by sequence length raised to a power. For diversity penalty, calculate similarity between sequences and penalize similar ones.\n",
        "\n",
        "**Key insight**: Length normalization prevents shorter sequences from being unfairly favored, while diversity penalty encourages exploration of different sequence patterns.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 3: Viterbi Algorithm for Sequence Tagging</summary>\n",
        "\n",
        "**Hint**: The Viterbi algorithm has two phases: forward pass (calculate probabilities) and backward pass (reconstruct path). Use dynamic programming to store the best path to each state at each time step. For unknown words, use smoothing or assign uniform probability across all tags.\n",
        "\n",
        "**Key insight**: The forward pass calculates the probability of the most likely path ending at each state, while the backward pass traces back to find the actual path.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 4: Constrained Beam Search</summary>\n",
        "\n",
        "**Hint**: Implement constraints as separate classes with `check()` and `can_be_satisfied()` methods. During beam search, filter out sequences that violate constraints and prune branches that can't satisfy constraints. Use early termination when no valid sequences remain.\n",
        "\n",
        "**Key insight**: Constraints should be checked both on complete sequences and partial sequences to enable early pruning of invalid branches.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal hint for Question 5: Diverse Beam Search with Groups</summary>\n",
        "\n",
        "**Hint**: For similarity calculation, use Jaccard similarity or edit distance. For grouping, use clustering algorithms like K-means or hierarchical clustering on sequence features. Select the highest-scoring sequence from each group to maintain both quality and diversity.\n",
        "\n",
        "**Key insight**: The key is balancing exploration (diversity) with exploitation (quality) by ensuring each group contributes its best sequence to the final result.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
