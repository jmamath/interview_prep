{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Optimization Algorithms for ML Engineers\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jmamath/interview_prep/blob/main/chapter_04_optimization_algorithms.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Picture this: You're training a large language model with billions of parameters. The training process will take weeks and cost hundreds of thousands of dollars in compute. You choose vanilla gradient descent with a fixed learning rate. After two days, the loss hasn't budged. The project is on hold, and your team is frustrated.\n",
    "\n",
    "Now imagine a different scenario: You use Adam optimizer with a carefully tuned learning rate schedule that includes warmup and cosine decay. The model converges smoothly, the loss drops steadily, and training completes successfully. The difference? A deep understanding of optimization algorithms.\n",
    "\n",
    "Optimization is the beating heart of machine learning. Every time you train a neural network, fine-tune a transformer, or update a recommendation system, you're running an optimization algorithm. Yet, many ML engineers treat optimizers as black boxes, reaching for Adam without understanding why it works or when it might fail.\n",
    "\n",
    "In this chapter, we'll demystify the optimization algorithms that power modern machine learning. We'll start with the fundamentals of gradient descent and momentum, build up to adaptive methods like Adam, explore learning rate schedules used in state-of-the-art LLM training, and tackle practical constraints like gradient clipping that prevent training instabilities. By the end, you'll have both the theoretical understanding and practical skills to choose, implement, and debug optimization algorithms for any ML project.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement gradient descent variants (SGD, momentum, Nesterov)\n",
    "- Understand adaptive learning rate methods (Adam, RMSprop)\n",
    "- Design and implement learning rate schedules for stable training\n",
    "- Apply constrained optimization with proximal operators\n",
    "- Handle gradient instabilities with clipping and trust regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Gradient Descent Variants (Easy)\n",
    "\n",
    "### Contextual Introduction\n",
    "When training a convolutional neural network on MNIST digit classification, you notice that vanilla stochastic gradient descent (SGD) takes many iterations to converge and the training loss oscillates wildly. Your colleague suggests adding momentum, and suddenly the training becomes much smoother and converges faster. What's happening?\n",
    "\n",
    "Momentum is one of the most important enhancements to gradient descent. Think of it like a ball rolling down a hill: it doesn't just respond to the current slope, but also carries velocity from previous steps. This helps the optimizer navigate ravines and plateaus in the loss landscape much more efficiently.\n",
    "\n",
    "### Key Concepts\n",
    "- **Vanilla SGD**: Updates parameters directly using the gradient: θ = θ - lr × ∇L\n",
    "- **Momentum**: Accumulates a velocity vector: v = β × v + ∇L, then θ = θ - lr × v\n",
    "- **Nesterov Momentum**: Looks ahead before computing the gradient, providing better convergence\n",
    "- **Convergence**: Momentum typically provides faster convergence and smoother training curves\n",
    "\n",
    "### Problem Statement\n",
    "Implement a gradient descent optimizer that supports vanilla SGD, momentum, and Nesterov accelerated gradient. Your implementation should follow PyTorch's optimizer interface.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement vanilla SGD parameter updates\n",
    "- Implement momentum with velocity accumulation\n",
    "- Implement Nesterov momentum (lookahead)\n",
    "- Support multiple parameters\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "optimizer = GradientDescentOptimizer([param], lr=0.1, momentum=0.9)\n",
    "optimizer.step()  # Update parameters\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Understanding Momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Let's optimize a simple quadratic function: f(x) = x^2\n",
    "# Starting point: x = 10.0\n",
    "# Learning rate: 0.1\n",
    "# Momentum: 0.9\n",
    "\n",
    "print(\"Optimizing f(x) = x^2 starting from x=10.0\\\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(\"VANILLA SGD (no momentum):\")\n",
    "print(\"=\" * 70)\n",
    "x_sgd = 10.0\n",
    "lr = 0.1\n",
    "for step in range(5):\n",
    "    grad = 2 * x_sgd  # Gradient of x^2 is 2x\n",
    "    x_sgd = x_sgd - lr * grad\n",
    "    print(f\"Step {step+1}: x = {x_sgd:.4f}, gradient = {grad:.4f}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 70)\n",
    "print(\"SGD WITH MOMENTUM (beta=0.9):\")\n",
    "print(\"=\" * 70)\n",
    "x_momentum = 10.0\n",
    "velocity = 0.0\n",
    "beta = 0.9\n",
    "for step in range(5):\n",
    "    grad = 2 * x_momentum\n",
    "    velocity = beta * velocity + grad  # Accumulate velocity\n",
    "    x_momentum = x_momentum - lr * velocity\n",
    "    print(f\"Step {step+1}: x = {x_momentum:.4f}, velocity = {velocity:.4f}, gradient = {grad:.4f}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 70)\n",
    "print(f\"RESULT: Vanilla SGD reached x={x_sgd:.4f}, Momentum reached x={x_momentum:.4f}\")\n",
    "print(f\"Momentum converges faster due to velocity accumulation!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "class GradientDescentOptimizer:\n",
    "    def __init__(self, params: List[torch.Tensor], lr: float = 0.01, momentum: float = 0.0, nesterov: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the gradient descent optimizer.\n",
    "        \n",
    "        Args:\n",
    "            params: List of parameters to optimize\n",
    "            lr: Learning rate\n",
    "            momentum: Momentum coefficient (0 means no momentum)\n",
    "            nesterov: Whether to use Nesterov momentum\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        \n",
    "        # TODO: Initialize velocity buffers for each parameter\n",
    "        # Hint: Create a list of zero tensors with the same shape as params\n",
    "        self.velocities = None\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Perform a single optimization step.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the parameter update logic\n",
    "        # If momentum == 0: vanilla SGD (theta = theta - lr * grad)\n",
    "        # If momentum > 0 and not nesterov: momentum SGD\n",
    "        #   v = momentum * v + grad\n",
    "        #   theta = theta - lr * v\n",
    "        # If momentum > 0 and nesterov: Nesterov momentum\n",
    "        #   v = momentum * v + grad\n",
    "        #   theta = theta - lr * (grad + momentum * v)\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out the gradients of all parameters.\"\"\"\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "\n",
    "def test_gradient_descent():\n",
    "    \"\"\"Test the gradient descent optimizer.\"\"\"\n",
    "    print(\"Testing Gradient Descent Optimizer...\\\\n\")\n",
    "    \n",
    "    # Test 1: Vanilla SGD on quadratic function\n",
    "    print(\"Test 1: Vanilla SGD\")\n",
    "    x = torch.tensor([10.0], requires_grad=True)\n",
    "    optimizer = GradientDescentOptimizer([x], lr=0.1, momentum=0.0)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        optimizer.zero_grad()\n",
    "        loss = x ** 2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    assert x.item() < 5.0, f\"Test 1 Failed: SGD should reduce x, got {x.item()}\"\n",
    "    print(f\"✓ Vanilla SGD: x converged from 10.0 to {x.item():.4f}\\\\n\")\n",
    "    \n",
    "    # Test 2: Momentum should converge faster\n",
    "    print(\"Test 2: Momentum SGD\")\n",
    "    x_momentum = torch.tensor([10.0], requires_grad=True)\n",
    "    optimizer_momentum = GradientDescentOptimizer([x_momentum], lr=0.1, momentum=0.9)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        optimizer_momentum.zero_grad()\n",
    "        loss = x_momentum ** 2\n",
    "        loss.backward()\n",
    "        optimizer_momentum.step()\n",
    "    \n",
    "    assert x_momentum.item() < x.item(), f\"Test 2 Failed: Momentum should converge faster than vanilla SGD\"\n",
    "    print(f\"✓ Momentum SGD: x converged to {x_momentum.item():.4f} (faster than vanilla)\\\\n\")\n",
    "    \n",
    "    # Test 3: Nesterov momentum\n",
    "    print(\"Test 3: Nesterov Momentum\")\n",
    "    x_nesterov = torch.tensor([10.0], requires_grad=True)\n",
    "    optimizer_nesterov = GradientDescentOptimizer([x_nesterov], lr=0.1, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        optimizer_nesterov.zero_grad()\n",
    "        loss = x_nesterov ** 2\n",
    "        loss.backward()\n",
    "        optimizer_nesterov.step()\n",
    "    \n",
    "    assert x_nesterov.item() < 5.0, f\"Test 3 Failed: Nesterov should converge, got {x_nesterov.item()}\"\n",
    "    print(f\"✓ Nesterov Momentum: x converged to {x_nesterov.item():.4f}\\\\n\")\n",
    "    \n",
    "    print(\"✓ All gradient descent tests passed!\")\n",
    "\n",
    "test_gradient_descent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Hint</b> (click to expand)</summary>\n",
    "\n",
    "For momentum SGD:\n",
    "1. Initialize velocities as zeros with the same shape as parameters\n",
    "2. Update velocity: `v = momentum * v + param.grad`\n",
    "3. Update parameter: `param = param - lr * v`\n",
    "\n",
    "For Nesterov momentum:\n",
    "- The key difference is looking ahead: update using `grad + momentum * v` instead of just `v`\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Adam Optimizer (Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "You're fine-tuning a BERT model for text classification. The model has millions of parameters with vastly different scales: embedding weights might need large updates while attention weights need tiny, careful adjustments. With vanilla SGD, you'd need to carefully tune the learning rate for each parameter group, which is impractical.\n",
    "\n",
    "Enter Adam (Adaptive Moment Estimation): an optimizer that automatically adapts the learning rate for each parameter based on the history of gradients. It combines the benefits of momentum (first moment) with adaptive learning rates (second moment), making it the default choice for training transformers and other deep networks.\n",
    "\n",
    "### Key Concepts\n",
    "- **First Moment (m)**: Exponential moving average of gradients (like momentum)\n",
    "- **Second Moment (v)**: Exponential moving average of squared gradients (for adaptive learning rates)\n",
    "- **Bias Correction**: Early iterations have biased estimates; we correct for this\n",
    "- **Update Rule**: θ = θ - lr × m̂ / (√v̂ + ε), where m̂ and v̂ are bias-corrected moments\n",
    "\n",
    "### Problem Statement\n",
    "Implement the Adam optimizer from scratch. Your implementation should maintain first and second moment estimates, apply bias correction, and handle numerical stability.\n",
    "\n",
    "**Requirements**:\n",
    "- Initialize moment buffers (m and v)\n",
    "- Update moments with exponential moving averages\n",
    "- Apply bias correction: m̂ = m / (1 - β1^t), v̂ = v / (1 - β2^t)\n",
    "- Handle numerical stability with epsilon\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "optimizer = AdamOptimizer([param], lr=0.001, betas=(0.9, 0.999))\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Adam Step-by-Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Let's trace Adam updates for 3 iterations\n",
    "# Parameter: theta = 1.0\n",
    "# Gradient: g = 2.0 (constant for simplicity)\n",
    "# Hyperparameters: lr=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8\n",
    "\n",
    "theta = 1.0\n",
    "g = 2.0\n",
    "lr = 0.1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "\n",
    "m = 0.0  # First moment\n",
    "v = 0.0  # Second moment\n",
    "\n",
    "print(\"Adam Optimizer - 3 Iteration Trace\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Initial: theta={theta:.6f}, m={m:.6f}, v={v:.6f}\\\\n\")\n",
    "\n",
    "for t in range(1, 4):\n",
    "    print(f\"Iteration {t}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Update biased first moment\n",
    "    m = beta1 * m + (1 - beta1) * g\n",
    "    print(f\"  1. Update m: m = {beta1} * {0 if t==1 else m/(beta1 + (1-beta1)):.6f} + {1-beta1} * {g:.6f} = {m:.6f}\")\n",
    "    \n",
    "    # Update biased second moment\n",
    "    v = beta2 * v + (1 - beta2) * (g ** 2)\n",
    "    print(f\"  2. Update v: v = {beta2} * {0 if t==1 else v/(beta2 + (1-beta2)):.6f} + {1-beta2} * {g**2:.6f} = {v:.6f}\")\n",
    "    \n",
    "    # Bias correction\n",
    "    m_hat = m / (1 - beta1 ** t)\n",
    "    v_hat = v / (1 - beta2 ** t)\n",
    "    print(f\"  3. Bias correction: m_hat = {m:.6f} / {1 - beta1**t:.6f} = {m_hat:.6f}\")\n",
    "    print(f\"                      v_hat = {v:.6f} / {1 - beta2**t:.6f} = {v_hat:.6f}\")\n",
    "    \n",
    "    # Parameter update\n",
    "    theta = theta - lr * m_hat / (v_hat ** 0.5 + epsilon)\n",
    "    print(f\"  4. Update theta: theta = {theta + lr * m_hat / (v_hat ** 0.5 + epsilon):.6f} - {lr} * {m_hat:.6f} / sqrt({v_hat:.6f}) = {theta:.6f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\\\nFinal: theta={theta:.6f}\")\n",
    "print(\"\\\\nNotice how bias correction in early iterations prevents slow startup!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, params: List[torch.Tensor], lr: float = 0.001, \n",
    "                 betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Initialize the Adam optimizer.\n",
    "        \n",
    "        Args:\n",
    "            params: List of parameters to optimize\n",
    "            lr: Learning rate\n",
    "            betas: Coefficients for computing running averages (beta1, beta2)\n",
    "            eps: Term added for numerical stability\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        self.t = 0  # Time step\n",
    "        \n",
    "        # TODO: Initialize first moment (m) and second moment (v) buffers\n",
    "        # Hint: Create lists of zero tensors matching parameter shapes\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Perform a single optimization step.\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # TODO: Implement Adam update rule for each parameter\n",
    "        # 1. Update biased first moment: m = beta1 * m + (1 - beta1) * grad\n",
    "        # 2. Update biased second moment: v = beta2 * v + (1 - beta2) * grad^2\n",
    "        # 3. Compute bias-corrected moments:\n",
    "        #    m_hat = m / (1 - beta1^t)\n",
    "        #    v_hat = v / (1 - beta2^t)\n",
    "        # 4. Update parameters: param = param - lr * m_hat / (sqrt(v_hat) + eps)\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out the gradients of all parameters.\"\"\"\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "\n",
    "def test_adam_optimizer():\n",
    "    \"\"\"Test the Adam optimizer implementation.\"\"\"\n",
    "    print(\"Testing Adam Optimizer...\\\\n\")\n",
    "    \n",
    "    # Test 1: Basic convergence\n",
    "    print(\"Test 1: Adam convergence on quadratic\")\n",
    "    x = torch.tensor([5.0], requires_grad=True)\n",
    "    optimizer = AdamOptimizer([x], lr=0.1)\n",
    "    \n",
    "    for _ in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        loss = x ** 2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    assert x.item() < 1.0, f\"Test 1 Failed: Adam should converge, got {x.item()}\"\n",
    "    print(f\"✓ Adam converged from 5.0 to {x.item():.6f}\\\\n\")\n",
    "    \n",
    "    # Test 2: Bias correction (early iterations)\n",
    "    print(\"Test 2: Bias correction in early iterations\")\n",
    "    x2 = torch.tensor([1.0], requires_grad=True)\n",
    "    optimizer2 = AdamOptimizer([x2], lr=0.1, betas=(0.9, 0.999))\n",
    "    \n",
    "    # First step\n",
    "    optimizer2.zero_grad()\n",
    "    loss = x2 ** 2\n",
    "    loss.backward()\n",
    "    initial_x = x2.item()\n",
    "    optimizer2.step()\n",
    "    \n",
    "    # Check that parameter changed\n",
    "    assert abs(x2.item() - initial_x) > 1e-6, \"Test 2 Failed: Parameter should update\"\n",
    "    print(f\"✓ Parameter updated from {initial_x:.6f} to {x2.item():.6f}\\\\n\")\n",
    "    \n",
    "    # Test 3: Handles different gradient scales\n",
    "    print(\"Test 3: Adaptive learning rates for different scales\")\n",
    "    x3 = torch.tensor([10.0, 0.1], requires_grad=True)\n",
    "    optimizer3 = AdamOptimizer([x3], lr=0.1)\n",
    "    \n",
    "    for _ in range(20):\n",
    "        optimizer3.zero_grad()\n",
    "        # Different scales: x3[0] has large gradients, x3[1] has small gradients\n",
    "        loss = x3[0] ** 2 + 100 * x3[1] ** 2\n",
    "        loss.backward()\n",
    "        optimizer3.step()\n",
    "    \n",
    "    assert x3[0].item() < 2.0 and x3[1].item() < 0.05, \"Test 3 Failed: Both parameters should converge\"\n",
    "    print(f\"✓ Both parameters converged: x[0]={x3[0].item():.6f}, x[1]={x3[1].item():.6f}\\\\n\")\n",
    "    \n",
    "    print(\"✓ All Adam optimizer tests passed!\")\n",
    "\n",
    "test_adam_optimizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Hint</b> (click to expand)</summary>\n",
    "\n",
    "Key steps for Adam:\n",
    "1. Initialize m and v as zero tensors: `self.m = [torch.zeros_like(p) for p in params]`\n",
    "2. Update moments with exponential moving average (EMA)\n",
    "3. Bias correction is crucial in early iterations: divide by `(1 - beta^t)`\n",
    "4. Use `torch.sqrt()` for the square root and add epsilon inside the sqrt for stability\n",
    "5. Remember to use `.data` when updating parameters to avoid building computation graph\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Learning Rate Scheduling (Medium)\n",
    "\n",
    "### Contextual Introduction\n",
    "You're pretraining a GPT-style language model from scratch. If you start with a high learning rate, the training diverges immediately—the loss shoots to infinity. If you start with a low learning rate, training is stable but painfully slow. The solution? A learning rate schedule with warmup.\n",
    "\n",
    "Modern LLM training uses sophisticated learning rate schedules: linear warmup for the first few thousand steps (to prevent early instability), followed by cosine decay to zero (for smooth convergence). These schedules are not just nice-to-haves; they're essential for stable, efficient training of large models.\n",
    "\n",
    "### Key Concepts\n",
    "- **Warmup**: Gradually increase learning rate from 0 to max over initial steps\n",
    "- **Step Decay**: Reduce learning rate by a factor every N epochs\n",
    "- **Cosine Annealing**: Smooth decay following a cosine curve: lr = lr_min + 0.5 × (lr_max - lr_min) × (1 + cos(π × t / T))\n",
    "- **Combined Schedules**: Warmup + cosine decay is the standard for transformer training\n",
    "\n",
    "### Problem Statement\n",
    "Implement a learning rate scheduler that supports step decay, cosine annealing, and combined warmup with cosine decay. The scheduler should track the current step and compute the learning rate dynamically.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement step decay (reduce by factor every N steps)\n",
    "- Implement cosine annealing with configurable min/max\n",
    "- Implement warmup + cosine decay combination\n",
    "- Return current learning rate for any step\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "scheduler = LRScheduler(base_lr=0.001, warmup_steps=100, total_steps=1000)\n",
    "lr = scheduler.get_lr()  # Get current learning rate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Learning Rate Schedules Visualized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's visualize 3 different learning rate schedules\n",
    "total_steps = 1000\n",
    "base_lr = 0.1\n",
    "steps = np.arange(total_steps)\n",
    "\n",
    "print(\"Learning Rate Schedule Examples\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Step Decay: Reduce by 0.5 every 250 steps\n",
    "print(\"\\\\n1. STEP DECAY (reduce by 0.5 every 250 steps):\")\n",
    "step_decay_lr = []\n",
    "for step in range(total_steps):\n",
    "    decay_factor = 0.5 ** (step // 250)\n",
    "    lr = base_lr * decay_factor\n",
    "    step_decay_lr.append(lr)\n",
    "    if step in [0, 250, 500, 750]:\n",
    "        print(f\"   Step {step:4d}: lr = {base_lr:.3f} * 0.5^{step//250} = {lr:.6f}\")\n",
    "\n",
    "# 2. Cosine Annealing: Smooth decay from base_lr to 0\n",
    "print(\"\\\\n2. COSINE ANNEALING (smooth decay to 0):\")\n",
    "cosine_lr = []\n",
    "for step in range(total_steps):\n",
    "    # Formula: lr = lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(pi * t / T))\n",
    "    progress = step / total_steps\n",
    "    lr = 0.5 * base_lr * (1 + np.cos(np.pi * progress))\n",
    "    cosine_lr.append(lr)\n",
    "    if step in [0, 250, 500, 750, 999]:\n",
    "        print(f\"   Step {step:4d}: progress={progress:.2f}, lr = {lr:.6f}\")\n",
    "\n",
    "# 3. Warmup + Cosine: Linear warmup (0-100 steps) then cosine decay\n",
    "print(\"\\\\n3. WARMUP + COSINE (100 step warmup, then cosine decay):\")\n",
    "warmup_steps = 100\n",
    "warmup_cosine_lr = []\n",
    "for step in range(total_steps):\n",
    "    if step < warmup_steps:\n",
    "        # Linear warmup: lr increases from 0 to base_lr\n",
    "        lr = base_lr * (step / warmup_steps)\n",
    "    else:\n",
    "        # Cosine decay from base_lr to 0\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        lr = 0.5 * base_lr * (1 + np.cos(np.pi * progress))\n",
    "    warmup_cosine_lr.append(lr)\n",
    "    if step in [0, 50, 100, 500, 999]:\n",
    "        phase = \"warmup\" if step < warmup_steps else \"cosine\"\n",
    "        print(f\"   Step {step:4d}: {phase:7s}, lr = {lr:.6f}\")\n",
    "\n",
    "# Plot all three schedules\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(steps, step_decay_lr, label='Step Decay', linewidth=2)\n",
    "plt.plot(steps, cosine_lr, label='Cosine Annealing', linewidth=2)\n",
    "plt.plot(steps, warmup_cosine_lr, label='Warmup + Cosine', linewidth=2)\n",
    "plt.xlabel('Training Step', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('Learning Rate Schedules Comparison', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"Note: Warmup + Cosine is the most common schedule for LLM training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LRScheduler:\n",
    "    def __init__(self, base_lr: float, warmup_steps: int = 0, total_steps: int = 1000):\n",
    "        \"\"\"\n",
    "        Initialize the learning rate scheduler.\n",
    "        \n",
    "        Args:\n",
    "            base_lr: Base learning rate (maximum LR after warmup)\n",
    "            warmup_steps: Number of warmup steps\n",
    "            total_steps: Total number of training steps\n",
    "        \"\"\"\n",
    "        self.base_lr = base_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Increment the step counter.\"\"\"\n",
    "        self.current_step += 1\n",
    "    \n",
    "    def step_decay(self, decay_rate: float = 0.5, decay_steps: int = 100) -> float:\n",
    "        \"\"\"\n",
    "        Compute learning rate with step decay.\n",
    "        \n",
    "        Args:\n",
    "            decay_rate: Factor to multiply LR by at each decay step\n",
    "            decay_steps: Number of steps between each decay\n",
    "            \n",
    "        Returns:\n",
    "            Current learning rate\n",
    "        \"\"\"\n",
    "        # TODO: Implement step decay\n",
    "        # Formula: lr = base_lr * decay_rate^(current_step // decay_steps)\n",
    "        pass\n",
    "    \n",
    "    def cosine_annealing(self, min_lr: float = 0.0) -> float:\n",
    "        \"\"\"\n",
    "        Compute learning rate with cosine annealing.\n",
    "        \n",
    "        Args:\n",
    "            min_lr: Minimum learning rate\n",
    "            \n",
    "        Returns:\n",
    "            Current learning rate\n",
    "        \"\"\"\n",
    "        # TODO: Implement cosine annealing\n",
    "        # Formula: lr = min_lr + 0.5 * (base_lr - min_lr) * (1 + cos(pi * t / T))\n",
    "        # where t is current_step and T is total_steps\n",
    "        pass\n",
    "    \n",
    "    def warmup_cosine(self, min_lr: float = 0.0) -> float:\n",
    "        \"\"\"\n",
    "        Compute learning rate with warmup followed by cosine decay.\n",
    "        \n",
    "        Args:\n",
    "            min_lr: Minimum learning rate after decay\n",
    "            \n",
    "        Returns:\n",
    "            Current learning rate\n",
    "        \"\"\"\n",
    "        # TODO: Implement warmup + cosine schedule\n",
    "        # If current_step < warmup_steps:\n",
    "        #     Linear warmup: lr = base_lr * (current_step / warmup_steps)\n",
    "        # Else:\n",
    "        #     Cosine decay from base_lr to min_lr\n",
    "        pass\n",
    "\n",
    "\n",
    "def test_lr_scheduler():\n",
    "    \"\"\"Test the learning rate scheduler.\"\"\"\n",
    "    print(\"Testing Learning Rate Scheduler...\\\\n\")\n",
    "    \n",
    "    # Test 1: Step decay\n",
    "    print(\"Test 1: Step Decay\")\n",
    "    scheduler = LRScheduler(base_lr=0.1, total_steps=1000)\n",
    "    \n",
    "    # Check decay at boundaries\n",
    "    scheduler.current_step = 0\n",
    "    lr0 = scheduler.step_decay(decay_rate=0.5, decay_steps=250)\n",
    "    assert abs(lr0 - 0.1) < 1e-6, f\"Test 1a Failed: Expected 0.1, got {lr0}\"\n",
    "    \n",
    "    scheduler.current_step = 250\n",
    "    lr250 = scheduler.step_decay(decay_rate=0.5, decay_steps=250)\n",
    "    assert abs(lr250 - 0.05) < 1e-6, f\"Test 1b Failed: Expected 0.05, got {lr250}\"\n",
    "    \n",
    "    scheduler.current_step = 500\n",
    "    lr500 = scheduler.step_decay(decay_rate=0.5, decay_steps=250)\n",
    "    assert abs(lr500 - 0.025) < 1e-6, f\"Test 1c Failed: Expected 0.025, got {lr500}\"\n",
    "    \n",
    "    print(f\"✓ Step decay: lr[0]={lr0:.4f}, lr[250]={lr250:.4f}, lr[500]={lr500:.4f}\\\\n\")\n",
    "    \n",
    "    # Test 2: Cosine annealing\n",
    "    print(\"Test 2: Cosine Annealing\")\n",
    "    scheduler2 = LRScheduler(base_lr=0.1, total_steps=1000)\n",
    "    \n",
    "    scheduler2.current_step = 0\n",
    "    lr_start = scheduler2.cosine_annealing(min_lr=0.0)\n",
    "    assert abs(lr_start - 0.1) < 1e-6, f\"Test 2a Failed: Should start at base_lr\"\n",
    "    \n",
    "    scheduler2.current_step = 1000\n",
    "    lr_end = scheduler2.cosine_annealing(min_lr=0.0)\n",
    "    assert lr_end < 0.01, f\"Test 2b Failed: Should decay close to 0\"\n",
    "    \n",
    "    print(f\"✓ Cosine annealing: start={lr_start:.4f}, end={lr_end:.6f}\\\\n\")\n",
    "    \n",
    "    # Test 3: Warmup + cosine\n",
    "    print(\"Test 3: Warmup + Cosine\")\n",
    "    scheduler3 = LRScheduler(base_lr=0.1, warmup_steps=100, total_steps=1000)\n",
    "    \n",
    "    # Check warmup phase\n",
    "    scheduler3.current_step = 50\n",
    "    lr_warmup = scheduler3.warmup_cosine(min_lr=0.0)\n",
    "    assert 0.04 < lr_warmup < 0.06, f\"Test 3a Failed: Warmup at step 50 should be ~0.05, got {lr_warmup}\"\n",
    "    \n",
    "    scheduler3.current_step = 100\n",
    "    lr_peak = scheduler3.warmup_cosine(min_lr=0.0)\n",
    "    assert abs(lr_peak - 0.1) < 0.01, f\"Test 3b Failed: Should reach base_lr after warmup, got {lr_peak}\"\n",
    "    \n",
    "    scheduler3.current_step = 1000\n",
    "    lr_final = scheduler3.warmup_cosine(min_lr=0.0)\n",
    "    assert lr_final < 0.01, f\"Test 3c Failed: Should decay to ~0, got {lr_final}\"\n",
    "    \n",
    "    print(f\"✓ Warmup+Cosine: warmup[50]={lr_warmup:.4f}, peak[100]={lr_peak:.4f}, end={lr_final:.6f}\\\\n\")\n",
    "    \n",
    "    print(\"✓ All learning rate scheduler tests passed!\")\n",
    "\n",
    "test_lr_scheduler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Hint</b> (click to expand)</summary>\n",
    "\n",
    "Key formulas:\n",
    "\n",
    "**Step Decay:**\n",
    "```python\n",
    "num_decays = current_step // decay_steps\n",
    "lr = base_lr * (decay_rate ** num_decays)\n",
    "```\n",
    "\n",
    "**Cosine Annealing:**\n",
    "```python\n",
    "progress = current_step / total_steps\n",
    "lr = min_lr + 0.5 * (base_lr - min_lr) * (1 + np.cos(np.pi * progress))\n",
    "```\n",
    "\n",
    "**Warmup + Cosine:**\n",
    "- During warmup: linear interpolation from 0 to base_lr\n",
    "- After warmup: apply cosine annealing from base_lr to min_lr\n",
    "- Remember to adjust the progress calculation for the cosine phase!\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Projected Gradient Descent for Constraints (Medium-Hard)\n",
    "\n",
    "### Contextual Introduction\n",
    "You're training a sparse neural network where you want most weights to be exactly zero (not just small). Standard gradient descent will give you small weights, but not exact zeros. Or perhaps you're training a reinforcement learning agent where certain parameters must stay within bounds for safety. How do you enforce these constraints during optimization?\n",
    "\n",
    "Projected Gradient Descent (PGD) is the answer. The idea is simple: after each gradient step, project the parameters back onto the constraint set. For L1 regularization, this projection is called the \"proximal operator\" or \"soft thresholding,\" and it's what actually creates sparse solutions.\n",
    "\n",
    "### Key Concepts\n",
    "- **Projection**: Map a point to the nearest point in the constraint set\n",
    "- **Soft Thresholding (L1 Proximal Operator)**: For threshold λ: if |x| < λ, set x = 0; else shrink x toward 0 by λ\n",
    "- **Box Constraints**: Project onto [min, max] by clipping\n",
    "- **Sparsity**: Soft thresholding creates exact zeros, inducing sparsity\n",
    "\n",
    "### Problem Statement\n",
    "Implement projected gradient descent with support for L1 regularization (soft thresholding) and box constraints. Your implementation should perform a gradient step followed by projection.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement soft thresholding (L1 proximal operator)\n",
    "- Implement box constraint projection\n",
    "- Combine gradient step with projection\n",
    "- Demonstrate sparsity-inducing behavior\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "optimizer = ProjectedGD([param], lr=0.1, constraint='l1', threshold=0.5)\n",
    "optimizer.step()  # Gradient step + soft thresholding\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Soft Thresholding Creates Sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Let's see how soft thresholding (L1 proximal operator) creates sparsity\n",
    "# We'll optimize a simple problem and see weights go to exactly zero\n",
    "\n",
    "print(\"Soft Thresholding Example: Creating Sparsity\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Soft thresholding function (L1 proximal operator)\n",
    "def soft_threshold(x, threshold):\n",
    "    \"\"\"\n",
    "    Apply soft thresholding: shrinks x toward 0 by threshold, sets to 0 if |x| < threshold\n",
    "    Formula: sign(x) * max(|x| - threshold, 0)\n",
    "    \"\"\"\n",
    "    return torch.sign(x) * torch.maximum(torch.abs(x) - threshold, torch.tensor(0.0))\n",
    "\n",
    "# Example values before and after soft thresholding with threshold=0.5\n",
    "values = torch.tensor([2.0, 1.0, 0.8, 0.4, 0.2, -0.3, -0.7, -1.5])\n",
    "threshold = 0.5\n",
    "\n",
    "print(f\"\\\\nThreshold: {threshold}\")\n",
    "print(f\"\\\\n{'Value':<10} {'After Soft Threshold':<25} {'Explanation'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for val in values:\n",
    "    result = soft_threshold(val, threshold)\n",
    "    if torch.abs(val) < threshold:\n",
    "        explanation = f\"|{val.item():.1f}| < {threshold} → set to 0\"\n",
    "    else:\n",
    "        explanation = f\"shrink by {threshold}: {val.item():.1f} → {result.item():.1f}\"\n",
    "    print(f\"{val.item():<10.1f} {result.item():<25.1f} {explanation}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"\\\\nNow let's see soft thresholding in action over 3 gradient steps:\")\n",
    "print(\"Optimizing f(w) = w^2 with learning rate 0.3 and threshold 0.5\\\\n\")\n",
    "\n",
    "w = 2.0\n",
    "lr = 0.3\n",
    "threshold = 0.5\n",
    "\n",
    "print(f\"Initial: w = {w:.4f}\\\\n\")\n",
    "\n",
    "for step in range(1, 4):\n",
    "    print(f\"Step {step}:\")\n",
    "    # Gradient step\n",
    "    grad = 2 * w  # Gradient of w^2\n",
    "    w_after_grad = w - lr * grad\n",
    "    print(f\"  1. Gradient step: w = {w:.4f} - {lr} * {grad:.4f} = {w_after_grad:.4f}\")\n",
    "    \n",
    "    # Soft thresholding (projection)\n",
    "    w = float(soft_threshold(torch.tensor(w_after_grad), threshold).item())\n",
    "    print(f\"  2. Soft threshold: apply threshold {threshold} → w = {w:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\\\nFinal: w = {w:.4f}\")\n",
    "if abs(w) < 1e-6:\n",
    "    print(\"\\\\n🎯 Soft thresholding drove w to EXACTLY zero (sparsity achieved!)\")\n",
    "else:\n",
    "    print(\"\\\\n📉 Soft thresholding is shrinking w toward zero\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Optional\n",
    "\n",
    "class ProjectedGD:\n",
    "    def __init__(self, params: List[torch.Tensor], lr: float = 0.01, \n",
    "                 constraint: str = 'none', threshold: float = 0.0,\n",
    "                 box_min: Optional[float] = None, box_max: Optional[float] = None):\n",
    "        \"\"\"\n",
    "        Initialize the Projected Gradient Descent optimizer.\n",
    "        \n",
    "        Args:\n",
    "            params: List of parameters to optimize\n",
    "            lr: Learning rate\n",
    "            constraint: Type of constraint ('none', 'l1', 'box')\n",
    "            threshold: Threshold for L1 soft thresholding\n",
    "            box_min: Minimum value for box constraints\n",
    "            box_max: Maximum value for box constraints\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.constraint = constraint\n",
    "        self.threshold = threshold\n",
    "        self.box_min = box_min\n",
    "        self.box_max = box_max\n",
    "    \n",
    "    def soft_threshold(self, x: torch.Tensor, threshold: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply soft thresholding (L1 proximal operator).\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            threshold: Threshold value\n",
    "            \n",
    "        Returns:\n",
    "            Soft-thresholded tensor\n",
    "        \"\"\"\n",
    "        # TODO: Implement soft thresholding\n",
    "        # Formula: sign(x) * max(|x| - threshold, 0)\n",
    "        # This sets values with |x| < threshold to exactly 0\n",
    "        # and shrinks other values toward 0 by threshold\n",
    "        pass\n",
    "    \n",
    "    def project_box(self, x: torch.Tensor, min_val: float, max_val: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Project onto box constraints [min_val, max_val].\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            min_val: Minimum value\n",
    "            max_val: Maximum value\n",
    "            \n",
    "        Returns:\n",
    "            Projected tensor\n",
    "        \"\"\"\n",
    "        # TODO: Implement box projection\n",
    "        # Hint: Use torch.clamp to clip values to [min_val, max_val]\n",
    "        pass\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Perform a single optimization step with projection.\n",
    "        \"\"\"\n",
    "        # TODO: Implement gradient step + projection\n",
    "        # 1. Gradient step: param = param - lr * grad\n",
    "        # 2. Apply projection based on self.constraint:\n",
    "        #    - 'l1': apply soft_threshold with self.threshold\n",
    "        #    - 'box': apply project_box with self.box_min and self.box_max\n",
    "        #    - 'none': no projection\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out the gradients of all parameters.\"\"\"\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "\n",
    "def test_projected_gd():\n",
    "    \"\"\"Test the Projected Gradient Descent optimizer.\"\"\"\n",
    "    print(\"Testing Projected Gradient Descent...\\\\n\")\n",
    "    \n",
    "    # Test 1: Soft thresholding\n",
    "    print(\"Test 1: Soft Thresholding (L1 proximal operator)\")\n",
    "    x = torch.tensor([2.0, 0.5, 0.3, -0.4, -1.0])\n",
    "    optimizer = ProjectedGD([x], lr=0.1, constraint='l1', threshold=0.5)\n",
    "    \n",
    "    result = optimizer.soft_threshold(x, 0.5)\n",
    "    expected = torch.tensor([1.5, 0.0, 0.0, 0.0, -0.5])  # Shrink by 0.5, set small values to 0\n",
    "    assert torch.allclose(result, expected, atol=1e-6), f\"Test 1 Failed: Expected {expected}, got {result}\"\n",
    "    print(f\"✓ Soft thresholding: {x.tolist()} → {result.tolist()}\\\\n\")\n",
    "    \n",
    "    # Test 2: Box constraints\n",
    "    print(\"Test 2: Box Constraint Projection\")\n",
    "    x2 = torch.tensor([5.0, 0.0, -3.0, 2.0])\n",
    "    optimizer2 = ProjectedGD([x2], lr=0.1, constraint='box', box_min=-1.0, box_max=3.0)\n",
    "    \n",
    "    result2 = optimizer2.project_box(x2, -1.0, 3.0)\n",
    "    expected2 = torch.tensor([3.0, 0.0, -1.0, 2.0])  # Clip to [-1, 3]\n",
    "    assert torch.allclose(result2, expected2, atol=1e-6), f\"Test 2 Failed: Expected {expected2}, got {result2}\"\n",
    "    print(f\"✓ Box projection to [-1, 3]: {x2.tolist()} → {result2.tolist()}\\\\n\")\n",
    "    \n",
    "    # Test 3: L1 induces sparsity\n",
    "    print(\"Test 3: L1 Constraint Induces Sparsity\")\n",
    "    x3 = torch.tensor([1.0, 0.5, 0.3], requires_grad=True)\n",
    "    optimizer3 = ProjectedGD([x3], lr=0.5, constraint='l1', threshold=0.3)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        optimizer3.zero_grad()\n",
    "        loss = (x3 ** 2).sum()  # Minimize squared values\n",
    "        loss.backward()\n",
    "        optimizer3.step()\n",
    "    \n",
    "    # Check that small values were driven to exactly 0\n",
    "    num_zeros = (x3 == 0).sum().item()\n",
    "    assert num_zeros >= 1, f\"Test 3 Failed: L1 should create sparsity (zeros), got {x3}\"\n",
    "    print(f\"✓ L1 regularization created {num_zeros} exact zeros: {x3.tolist()}\\\\n\")\n",
    "    \n",
    "    print(\"✓ All projected gradient descent tests passed!\")\n",
    "\n",
    "test_projected_gd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Hint</b> (click to expand)</summary>\n",
    "\n",
    "**Soft Thresholding:**\n",
    "```python\n",
    "def soft_threshold(x, threshold):\n",
    "    return torch.sign(x) * torch.maximum(torch.abs(x) - threshold, torch.tensor(0.0))\n",
    "```\n",
    "\n",
    "**Box Projection:**\n",
    "```python\n",
    "def project_box(x, min_val, max_val):\n",
    "    return torch.clamp(x, min=min_val, max=max_val)\n",
    "```\n",
    "\n",
    "**Key Insight:**\n",
    "- Soft thresholding creates **exact zeros**, not just small values\n",
    "- This is why L1 regularization leads to sparse models\n",
    "- Always apply projection **after** the gradient step\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Gradient Clipping and Practical Constraints (Hard)\n",
    "\n",
    "### Contextual Introduction\n",
    "You're training a large transformer model and everything seems fine for the first few hours. Then suddenly, at step 5000, the loss explodes to infinity. The culprit? Gradient explosion. Or perhaps you're implementing RLHF (Reinforcement Learning from Human Feedback) for a language model, and you need to ensure the new policy doesn't drift too far from the reference policy.\n",
    "\n",
    "Gradient clipping is one of the most important practical techniques in deep learning. It's not theoretically elegant, but it's absolutely essential for training RNNs, transformers, and other deep networks. Similarly, trust region methods keep optimization steps conservative, preventing catastrophic policy updates.\n",
    "\n",
    "### Key Concepts\n",
    "- **Gradient Norm**: Total magnitude of gradients: ||g|| = sqrt(sum(g_i^2))\n",
    "- **Gradient Clipping by Norm**: If ||g|| > threshold, scale g to have norm = threshold\n",
    "- **Gradient Clipping by Value**: Clip each gradient element to [-threshold, threshold]\n",
    "- **Trust Region**: Constrain updates to stay within a \"trust region\" using KL divergence or L2 penalty\n",
    "\n",
    "### Problem Statement\n",
    "Implement an optimizer with gradient clipping (by norm and by value) and support for constrained optimization with KL penalties. Your implementation should detect and prevent gradient explosion while maintaining stable training.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement gradient norm clipping (rescale if ||g|| > threshold)\n",
    "- Implement per-parameter value clipping\n",
    "- Support constrained optimization with penalty term\n",
    "- Track gradient statistics for adaptive clipping\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "optimizer = ClippingOptimizer([param], lr=0.01, clip_norm=1.0)\n",
    "optimizer.step()  # Clips gradients if norm exceeds 1.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Gradient Explosion and Clipping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"Gradient Clipping Example: Preventing Explosion\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate a scenario where gradients grow over time (common in RNNs)\n",
    "# We'll show what happens with and without clipping\n",
    "\n",
    "def compute_grad_norm(grads):\n",
    "    \"\"\"Compute the L2 norm of gradients.\"\"\"\n",
    "    return torch.sqrt(sum(torch.sum(g ** 2) for g in grads)).item()\n",
    "\n",
    "def clip_by_norm(grads, max_norm):\n",
    "    \"\"\"Clip gradients by their global norm.\"\"\"\n",
    "    total_norm = compute_grad_norm(grads)\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / total_norm\n",
    "        for g in grads:\n",
    "            g.mul_(scale)\n",
    "    return total_norm\n",
    "\n",
    "# Example: Gradient explosion scenario\n",
    "print(\"\\\\nScenario: Gradients doubling each step (like vanishing gradient problem in reverse)\\\\n\")\n",
    "\n",
    "print(\"WITHOUT CLIPPING:\")\n",
    "print(\"-\" * 80)\n",
    "grad = torch.tensor([1.0, 1.0])  # Initial gradient\n",
    "for step in range(1, 6):\n",
    "    norm = compute_grad_norm([grad])\n",
    "    print(f\"Step {step}: gradient = {grad.tolist()}, norm = {norm:.2f}\")\n",
    "    grad = grad * 2  # Gradients double (explosion!)\n",
    "    if norm > 100:\n",
    "        print(\"\\\\n💥 GRADIENT EXPLOSION! Training would diverge here.\")\n",
    "        break\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"WITH NORM CLIPPING (max_norm=5.0):\")\n",
    "print(\"-\" * 80)\n",
    "grad = torch.tensor([1.0, 1.0])  # Reset\n",
    "max_norm = 5.0\n",
    "\n",
    "for step in range(1, 6):\n",
    "    original_norm = compute_grad_norm([grad])\n",
    "    clipped_norm = clip_by_norm([grad], max_norm)\n",
    "    \n",
    "    if original_norm > max_norm:\n",
    "        print(f\"Step {step}: original norm = {original_norm:.2f} → CLIPPED to {max_norm:.2f}\")\n",
    "        print(f\"         gradient after clipping = {grad.tolist()}\")\n",
    "    else:\n",
    "        print(f\"Step {step}: norm = {original_norm:.2f} (no clipping needed)\")\n",
    "        print(f\"         gradient = {grad.tolist()}\")\n",
    "    \n",
    "    grad = grad * 2  # Try to double (but will be clipped next iteration)\n",
    "\n",
    "print(\"\\\\n✅ Clipping prevents explosion! Training remains stable.\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"\\\\nCLIPPING BY VALUE vs BY NORM:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Show difference between two clipping strategies\n",
    "grad_example = torch.tensor([3.0, 4.0])  # Norm = 5.0\n",
    "print(f\"Original gradient: {grad_example.tolist()}, norm = {compute_grad_norm([grad_example]):.2f}\")\n",
    "\n",
    "# Clip by norm (rescale entire gradient)\n",
    "grad_norm_clip = grad_example.clone()\n",
    "clip_by_norm([grad_norm_clip], max_norm=2.5)\n",
    "print(f\"\\\\nClip by NORM (max_norm=2.5): {grad_norm_clip.tolist()}\")\n",
    "print(f\"  → Preserves direction, scales magnitude to 2.5\")\n",
    "\n",
    "# Clip by value (clip each element independently)\n",
    "grad_value_clip = torch.clamp(grad_example, min=-2.5, max=2.5)\n",
    "print(f\"\\\\nClip by VALUE ([-2.5, 2.5]): {grad_value_clip.tolist()}\")\n",
    "print(f\"  → Changes direction! Each element clipped independently\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"\\\\n🎯 Best Practice: Use NORM clipping for training (preserves gradient direction)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Optional\n",
    "\n",
    "class ClippingOptimizer:\n",
    "    def __init__(self, params: List[torch.Tensor], lr: float = 0.01,\n",
    "                 clip_norm: Optional[float] = None, clip_value: Optional[float] = None,\n",
    "                 kl_penalty: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize optimizer with gradient clipping.\n",
    "        \n",
    "        Args:\n",
    "            params: List of parameters to optimize\n",
    "            lr: Learning rate\n",
    "            clip_norm: Maximum gradient norm (None = no clipping)\n",
    "            clip_value: Maximum absolute value for each gradient (None = no clipping)\n",
    "            kl_penalty: KL divergence penalty coefficient for trust region\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_value = clip_value\n",
    "        self.kl_penalty = kl_penalty\n",
    "        self.grad_norms = []  # Track gradient norms for statistics\n",
    "    \n",
    "    def compute_grad_norm(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute the global L2 norm of all gradients.\n",
    "        \n",
    "        Returns:\n",
    "            Total gradient norm\n",
    "        \"\"\"\n",
    "        # TODO: Implement gradient norm computation\n",
    "        # Formula: sqrt(sum of squared gradients across all parameters)\n",
    "        # Hint: Use torch.sum(param.grad ** 2) for each parameter\n",
    "        pass\n",
    "    \n",
    "    def clip_by_norm(self):\n",
    "        \"\"\"\n",
    "        Clip gradients by their global norm.\n",
    "        If total_norm > clip_norm, scale all gradients by clip_norm / total_norm.\n",
    "        \"\"\"\n",
    "        # TODO: Implement gradient clipping by norm\n",
    "        # 1. Compute total gradient norm\n",
    "        # 2. If total_norm > self.clip_norm:\n",
    "        #    - Compute scale = self.clip_norm / total_norm\n",
    "        #    - Multiply each gradient by scale\n",
    "        pass\n",
    "    \n",
    "    def clip_by_value(self):\n",
    "        \"\"\"\n",
    "        Clip each gradient element to [-clip_value, clip_value].\n",
    "        \"\"\"\n",
    "        # TODO: Implement gradient clipping by value\n",
    "        # Hint: Use torch.clamp(param.grad, -self.clip_value, self.clip_value)\n",
    "        pass\n",
    "    \n",
    "    def step_with_constraint(self, reference_params: Optional[List[torch.Tensor]] = None):\n",
    "        \"\"\"\n",
    "        Perform optimization step with optional KL constraint penalty.\n",
    "        \n",
    "        Args:\n",
    "            reference_params: Reference parameters for KL penalty (trust region)\n",
    "        \"\"\"\n",
    "        # TODO: Implement constrained optimization step\n",
    "        # 1. Apply gradient clipping (if enabled)\n",
    "        # 2. If kl_penalty > 0 and reference_params provided:\n",
    "        #    - Add penalty to gradient: grad += kl_penalty * (param - reference_param)\n",
    "        # 3. Update parameters: param = param - lr * grad\n",
    "        pass\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Perform a standard optimization step with clipping.\n",
    "        \"\"\"\n",
    "        # Record gradient norm before clipping\n",
    "        if self.clip_norm is not None or self.clip_value is not None:\n",
    "            original_norm = self.compute_grad_norm()\n",
    "            self.grad_norms.append(original_norm)\n",
    "        \n",
    "        # Apply clipping\n",
    "        if self.clip_norm is not None:\n",
    "            self.clip_by_norm()\n",
    "        if self.clip_value is not None:\n",
    "            self.clip_by_value()\n",
    "        \n",
    "        # Update parameters\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.data -= self.lr * param.grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out the gradients of all parameters.\"\"\"\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "\n",
    "def test_clipping_optimizer():\n",
    "    \"\"\"Test the clipping optimizer.\"\"\"\n",
    "    print(\"Testing Gradient Clipping Optimizer...\\\\n\")\n",
    "    \n",
    "    # Test 1: Gradient norm computation\n",
    "    print(\"Test 1: Gradient Norm Computation\")\n",
    "    x = torch.tensor([3.0, 4.0], requires_grad=True)\n",
    "    loss = (x ** 2).sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer = ClippingOptimizer([x], lr=0.1)\n",
    "    norm = optimizer.compute_grad_norm()\n",
    "    expected_norm = 10.0  # sqrt(6^2 + 8^2) = sqrt(100) = 10\n",
    "    assert abs(norm - expected_norm) < 1e-5, f\"Test 1 Failed: Expected norm {expected_norm}, got {norm}\"\n",
    "    print(f\"✓ Gradient norm: {norm:.2f} (gradients: {x.grad.tolist()})\\\\n\")\n",
    "    \n",
    "    # Test 2: Clipping triggers when norm exceeds threshold\n",
    "    print(\"Test 2: Norm Clipping Triggers\")\n",
    "    x2 = torch.tensor([3.0, 4.0], requires_grad=True)\n",
    "    loss2 = (x2 ** 2).sum()\n",
    "    loss2.backward()\n",
    "    \n",
    "    optimizer2 = ClippingOptimizer([x2], lr=0.1, clip_norm=5.0)\n",
    "    original_grad = x2.grad.clone()\n",
    "    optimizer2.clip_by_norm()\n",
    "    \n",
    "    clipped_norm = optimizer2.compute_grad_norm()\n",
    "    assert abs(clipped_norm - 5.0) < 1e-5, f\"Test 2 Failed: Clipped norm should be 5.0, got {clipped_norm}\"\n",
    "    print(f\"✓ Clipping: {original_grad.tolist()} (norm=10.0) → {x2.grad.tolist()} (norm=5.0)\\\\n\")\n",
    "    \n",
    "    # Test 3: Value clipping\n",
    "    print(\"Test 3: Value Clipping\")\n",
    "    x3 = torch.tensor([10.0, -8.0, 2.0], requires_grad=True)\n",
    "    loss3 = (x3 ** 2).sum()\n",
    "    loss3.backward()\n",
    "    \n",
    "    optimizer3 = ClippingOptimizer([x3], lr=0.1, clip_value=5.0)\n",
    "    optimizer3.clip_by_value()\n",
    "    \n",
    "    expected = torch.tensor([5.0, -5.0, 4.0])  # Clip to [-5, 5]\n",
    "    assert torch.allclose(x3.grad, expected, atol=1e-5), f\"Test 3 Failed: Expected {expected}, got {x3.grad}\"\n",
    "    print(f\"✓ Value clipping to [-5, 5]: {[20.0, -16.0, 4.0]} → {x3.grad.tolist()}\\\\n\")\n",
    "    \n",
    "    print(\"✓ All clipping optimizer tests passed!\")\n",
    "\n",
    "test_clipping_optimizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Hint</b> (click to expand)</summary>\n",
    "\n",
    "**Computing Gradient Norm:**\n",
    "```python\n",
    "total_norm = 0.0\n",
    "for param in self.params:\n",
    "    if param.grad is not None:\n",
    "        total_norm += torch.sum(param.grad ** 2).item()\n",
    "return total_norm ** 0.5\n",
    "```\n",
    "\n",
    "**Clipping by Norm:**\n",
    "```python\n",
    "total_norm = self.compute_grad_norm()\n",
    "if total_norm > self.clip_norm:\n",
    "    scale = self.clip_norm / total_norm\n",
    "    for param in self.params:\n",
    "        if param.grad is not None:\n",
    "            param.grad.mul_(scale)  # In-place scaling\n",
    "```\n",
    "\n",
    "**Key Insights:**\n",
    "- Norm clipping preserves gradient direction (scales uniformly)\n",
    "- Value clipping can change gradient direction (clips independently)\n",
    "- For RNNs and transformers, norm clipping is standard (typically max_norm=1.0)\n",
    "- Track gradient norms over time to detect instabilities early\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Congratulations! You've now implemented the core optimization algorithms that power modern machine learning. Let's recap what we've covered:\n",
    "\n",
    "1. **Gradient Descent Variants**: From vanilla SGD to momentum and Nesterov, understanding how velocity accumulation leads to faster convergence.\n",
    "\n",
    "2. **Adam Optimizer**: The workhorse of deep learning, combining adaptive learning rates with momentum for robust training across diverse parameter scales.\n",
    "\n",
    "3. **Learning Rate Scheduling**: Essential techniques for stable training, especially warmup and cosine decay used in state-of-the-art LLM training.\n",
    "\n",
    "4. **Projected Gradient Descent**: Handling constraints through soft thresholding and projections, crucial for sparse models and constrained optimization.\n",
    "\n",
    "5. **Gradient Clipping**: A practical necessity for preventing training instabilities in RNNs, transformers, and other deep networks.\n",
    "\n",
    "These aren't just theoretical concepts—they're the tools you'll use every day as an ML engineer. Whether you're debugging a training run that won't converge, choosing hyperparameters for a new model, or implementing custom optimization logic, the deep understanding you've gained here will serve you well.\n",
    "\n",
    "In the next chapter, we'll build on this foundation to explore training efficiency and memory optimization—because understanding optimization is just the first step toward training models efficiently at scale.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
